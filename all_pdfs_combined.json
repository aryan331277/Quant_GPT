[
  {
    "filename": "The economic value of depth.pdf",
    "text": "\n\n--- Page 1 ---\narXiv:2506.15354v1  [econ.GN]  18 Jun 2025\nThe Economic Value of Depth\nPedro Afonso Fernandes\u2217\nUniversidade Cat\u00b4olica Portuguesa\nCat\u00b4olica Lisbon School of Business & Economics\nCat\u00b4olica Lisbon Research Unit in Business & Economics (CUBE)\nPortugal\nJune 19, 2025\nAbstract\nThe main goal of this article is to introduce an economic perspective in the social logic\nof space. Firstly, we describe the economic model of a linear city to show how depth can\ngenerate value by creating local monopolies in less integrated spaces. Then, a new syntactic\nmeasure, the d-value, is proposed to capture the relation between the depth of some space\nfrom outside and the mean depth of all spaces from outside. An application to a public\nhousing estate suggests that economic activities and services may be located in spaces with\na d-value close to one. The article is complemented by a Prolog programme with a special\npredicate to compute the d-value.\nKeywords: urban economics, space syntax, logic programming.\n\u2217ORCID: https://orcid.org/0000-0001-5762-5157.\nCorrespondence: Universidade Cat\u00b4olica Portuguesa,\nCat\u00b4olica Lisbon School of Business & Economics, Palma de Cima, Building 5, 4th floor, Room 5430, 1649-023\nLisboa, Portugal. Email: paf@ucp.pt. URL: https://paf.com.pt.\n1\n\n\n--- Page 2 ---\n1\nIntroduction\nSpace syntax is a set of techniques for analysing urban settlements or buildings and theories\nlinking space and society founded on architecture, engineering, mathematics, sociology, anthro-\npology, ethnography, linguistics, psychology, biology, and computer science. It was developed\noriginally by Bill Hillier, Julienne Hanson, and colleagues at the Bartlett School of Architecture\nand Planning, University College of London (UCL) since the 1970s. Their innovative approach\nwas condensed into three landmark books: The Social Logic of Space (Hillier and Hanson, 1984),\nDecoding Homes and Houses (Hanson, 1998), and Space is the machine: a configurational theory\nof architecture (Hillier, 2007).\nSpace syntax is mainly concerned with the study of the relations between convex spaces.\nConvexity exists when straight lines can be drawn from any point in a space to any other\npoint in it without leaving the space itself (Hillier and Hanson, 1984). In fact, convex spaces\nrather than concave spaces stimulate social interaction in the sense that everyone sees everyone\nwithin that kind of space. Thus, the starting point of a syntactic analysis is typically a map\nof the fattest convex and open (or permeable) spaces that cover the settlement (or building) in\nquestion. Then, several syntactic measures can be computed to explore the connectivity and\nasymmetry between convex spaces or the minimum set of axial lines that covered them.\nIn this scope, the notion of depth is concerned with the number of intervening spaces through\nwhich it is necessary to go to get from one space to another (Hillier and Hanson, 1984). The\ndeepest or asymmetric spaces are typically located at the end or limit of a complex because\nwe may have to take several steps to reach the other spaces. In contrast, integrated spaces are\ntypically located in the \u2019middle\u2019 of the settlement. Thus, depth has an economic meaning in the\nsense that it represents the cost of moving between spaces. In fact, the most valuable spaces\ntypically have a small mean depth, that is, we do not need to take too much steps from them\nto get the other spaces in the complex.\nThe economic meaning of integration can be found in the high rents typically paid in the\ncentral business district (CBD) of most towns, or in neighbourhoods well connected with high-\nways or railways. However, depth itself can have an economic value. In addition to the value\nof peace and quiet, suburban consumers may be willing to pay higher prices for goods sold near\ntheir homes to avoid freight or commuting costs or because they like local stores. This kind of\nvalue is not captured by typical syntactic measures.\nIn this article, we describe the economic model of a linear city to show how depth can\ngenerate value by creating local monopolies in less integrated spaces. Then, a new syntactic\nmeasure is proposed, the d-value, to capture the economic value of depth. An application to a\npublic housing estate located in Odivelas, Portugal, used Prolog, a logic programming language\ndeveloped in the 1970s (Colmerauer and Roussel, 1993), to illustrate how the d-value can be\neasily computed.\nProlog applies logic to represent knowledge and uses deduction to solve problems by deriving\nlogical consequences (Kowalski, 1988). A corollary of using logic to represent knowledge is that\nsuch knowledge can be understood declaratively. This kind of reasoning is embodied in space\nsyntax when it represents spatial arrangements as a field of knowables, that is, as a system of\npossibilities governed by a simple and underlying system of concepts (Hillier and Hanson, 1984).\nA comprehensive description of the application of logic programming to space syntax can be\nfound in Fernandes (2023).\nThe material in this article is complemented by an open source computer programme stored\nin SWISH (https://swish.swi-prolog.org/p/gulbenkian.pl, accessed on 12 June 2025).\nSWISH is the online version for the sharing of SWI-Prolog, a free and versatile implementa-\ntion of the Prolog language developed by Jan Wielemaker and colleagues at the University of\nAmsterdam, The Netherlands, since 1987 (Wielemaker et al., 2012).\n2\n\n\n--- Page 3 ---\n2\nBasic Concepts\nBased on graph theory and computer-aided simulations, space syntax aims to find and explain\nthe relation between spatial configurations and social activities. Configuration is a concept that\naddresses the whole of a complex (settlement or building) rather than its parts and captures\nhow the relations between two convex spaces, say A and B, might be affected by a third space C\n(Hillier, 2007). For example, if A and B are adjacent or permeable, then they have a symmetric\nconfiguration in the sense that, if A is the neighbour of B, then B is the neighbour of A, as\nillustrated on the left-hand side of Figure 1.\nFigure 1: Basic configurations.\nHowever, if only A is connected with a third space C, as in the right-hand side of the same\nfigure, A and B become asymmetrical in relation to C because we have to pass through A to\nget to B from C, but we do not have to pass through B to get to A from C. Thus, asymmetry\nrelates to depth, that is, with the number of spaces (or steps) necessary to go from a certain\nspace, say C, to another space, A (1 step), B (2 steps), and so on.\nC \u2192A \u2192B \u2192. . .\nIf we count the number of steps necessary to go from a certain space to every other space in\na complex, we can obtain a measure of its total depth (TD), or mean depth (MD) by dividing\nthat total by the number of spaces in the complex minus one, the original space (Hillier and\nHanson, 1984) (Hanson, 1998). In the previous example, the total depth of C is 3, which is the\nsum of steps to reach A and B (1 + 2) from that origin. This is also the case for B, but the total\ndepth of A is 2, noting that this (central) space is directly connected to the spaces B or C (1 +\n1). Thus, the mean depth is 1 (2/2) for A and 1.5 (3/2) for B or C, noting that the number of\nspaces minus one in this simple complex is 2 (3 \u22121).\nThis simple illustration suggests that syntactic measures can be computed for every convex\nspace in a settlement (or building). Then, we may eventually find that some spaces have a lower\ndepth than all other spaces (A in the same example), and others have a greater depth (B and\nC). The former are the most integrated spaces, where social life may be concentrated in cities\n(Heitor and Pinelo Silva, 2015). The latter are typically the most segregated, quiet, or remote\nspaces in a complex. Thus, integration is inversely related to depth. It is a global measure in\nthe sense that it considers the configuration of a certain space in relation to all other spaces. In\naddition, local measures such as control are based on the relations between each space and only\nthe spaces directly connected to it (Turner, 2004).\nTraditionally, high spatial integration of the street network is a sufficient condition for eco-\nnomic centrality (van Nes and Yamu, 2022).\nThus, the places where trade, shopping, and\nfinances take place may be the most integrated streets on a local or global scale. Accessibility,\nvisibility, permeability, and connectivity to direct side streets are also necessary conditions for\nthe concentration of stores. However, as the following model suggests, depth can protect local\nmarkets, especially with high transportation costs. Thus, asymmetry can favour profits.\n3\n\n\n--- Page 4 ---\n3\nModel\nThe linear city economic model was originally developed by Harold Hotelling (1929) to deal with\nthe instability of the duopoly model, where a small change in the price of either company takes\naway all the opponent\u2019s business. In Hotelling\u2019s Main Street, the buyers of a homogeneous good\nare uniformly distributed along a line of length l. At distances a and b respectively from the\ntwo ends of this line are stores A and B (Figure 2).\na\nA\nw\nx\ny\nB\nwb\nFigure 2: Hotelling\u2019s linear city.\nLet us denote p1 the price of A, p2 the price of B, q1 the quantity sold by A and q2 the quantity\nsold by B. Each consumer is indifferent between buying from A or B except in terms of price\nplus transportation cost between the store A or B and her home at a cost c per unit of distance.\nIf store A wants to sell something, its price p1 should be below p2 plus the transportation cost\nfrom store B, that is, p1 < p2 + c(l \u2212a \u2212b). In this case, store A sells the quantity a plus the\nquantity x which depends on the difference between prices p1 and p2. Similarly, store B sells b\nplus y if p2 < p1 + c(l \u2212a \u2212b). Once again, y is a function of the difference between prices.\nThus, stores are monopolists at the respective end of the line: A gets the demand a, and\nB the demand b. Between locations A and B, stores have to split the market based on their\n(optimal) prices. The point of division between the regions served by A and B, respectively x\nand y, is determined by the condition of indifference to buy from one store or another, that is,\np1 + cx = p2 + cy.\n(1)\nNoting that a + x + y + b = l, Hotelling (1929) found from (1) that\nx = 1\n2\n\u0012\nl \u2212a \u2212b + p2 \u2212p1\nc\n\u0013\n, and\n(2)\ny = 1\n2\n\u0012\nl \u2212a \u2212b + p1 \u2212p2\nc\n\u0013\n.\n(3)\nThus, the quantity x sold by store A increases with the price of the other store p2 or when\nits own price p1 decreases. Similarly, store B sold more when p1 increased or p2 decreased.\nObviously, the quantity to be split x + y decreases with length a + b. Having some end of the\nline minimises the competition in the middle of the line.\nThe optimal price for each store can be found by equating the first derivative of its own\nprofit to zero, the so-called first-order condition. Noting that the profits of both firms are\n\u03c01 = p1q1 = p1(a + x) = 1\n2 (l + a \u2212b) p1 \u2212p12\n2c + p1p2\n2c , and\n(4)\n\u03c02 = p2q2 = p2(b + y) = 1\n2 (l \u2212a + b) p2 \u2212p22\n2c + p1p2\n2c ,\n(5)\nfrom the first-order conditions described by Hotelling (1929), we obtain the optimal prices\np1 = c\n\u0012\nl + a \u2212b\n3\n\u0013\n, and\n(6)\np2 = c\n\u0012\nl + b \u2212a\n3\n\u0013\n.\n(7)\n4\n\n\n--- Page 5 ---\nThus, if a store is located far away from the end of the city and if it does not face local\ncompetition until that end, which requires some depth from the middle of the line, it should\nadopt a higher price. In fact, the optimal price of store A (p1) increases with length a and the\nsame applies to store B, that is, p2 increases with b. Similarly, store A or B can achieve higher\nsales and profits from length a or b, respectively, noting that optimal quantities are\nq1 = a + x = 1\n2\n\u0012\nl + a \u2212b\n3\n\u0013\n, and\n(8)\nq2 = b + y = 1\n2\n\u0012\nl + b \u2212a\n3\n\u0013\n.\n(9)\nA small example can illustrate why a > b implies higher prices, quantities, and profits for\nstore A. Following Hotelling (1929), let l = 35, a = 4 and b = 1 as in figure 2, with c = 1. Then,\nfrom equations (6) and (7), we get p1 = 35 + (4 \u22121)/3 = 36 and p2 = 35 + (1 \u22124)/3 = 34.\nSimilarly, from (8) and (9), we get q1 = (35 + 3/3)/2 = 18 and q2 = (35 \u22123/3)/2 = 17. Thus,\nthe profit of store A will be 36 \u00d7 18 = 648 euros, while store B will get 34 \u00d7 17 = 578 euros.\nIf the location of B is fixed, A has an economic incentive to make a as large as possible. This\nmeans that it will come as close to B. However, if A and B are too close, the system becomes\nunstable like the classic duopoly: each store can get the full market by slightly decreasing its\nown price. Thus, A and B should maintain some distance between them. As Hotelling (1929)\nsaid: \u201dThe intermediate segment of the market acts as a cushion as well as a bone of contention;\nwhen it disappears we have Cournot\u2019s case, and Bertrand\u2019s objection applies\u201d.\n4\nImplications\nWhat are the implications of Hotelling\u2019s model on space syntax? Despite its simplicity and\ntheoretic nature, Hotelling\u2019s model suggests that depth can matter in the sense that a less\nintegrated space can provide local demand and less competition for firms located there. Central\nor integrated spaces are typically very tough: In addition to high rents, companies may have\nto lower their prices to sell their goods and services. In contrast, peripheral locations can offer\nhigher profits. However, companies located in very deep locations may only get a small portion\nof the total market, such as firm B in Figure 2 which gets b << a. Thus, a good location may be\nan intermediate point between the middle and end of the settlement, keeping some intermediate\nspace between stores.\nThis evidence suggests a new syntactic measure, d-value, that captures the relation between\nthe depth of some space from outside (Do) and the mean depth of all spaces from outside (MDo),\nthat is,\nd-value =\nDo\nMDo\n.\n(10)\nIn this framework, a d-value close to 0 means that the space is too close to the end of the\nsettlement, where the market dimension may be tiny. In contrast, a d-value greater than 1\nmeans that the space is located close to the centre of the settlement, where competition may\nbe hard. Ideally, the space should have a d-value close to 1, which means that its depth from\noutside is roughly equal to the mean depth from outside.\n5\nApplication\nBairro Gulbenkian (figure 3) is a public housing estate built in the late 1960s in Odivelas, near\nLisbon, Portugal.\nIt occupies a plot of 28,500 square metres and is the largest estate built\nwith the financial and technical support of the Calouste Gulbenkian Foundation in a special\n5\n\n\n--- Page 6 ---\nresettlement plan to deal with the homeless of the great flood of November 25-26, 1967, in the\nGreat Lisbon, which killed about 700 people (Malheiros et al., 2018). Composed of 15 isolated\nblocks with three storeys and a total of 160 dwellings (about 600 inhabitants), Bairro Gulbenkian\nis a good case for testing and developing new syntactic methods.\nFigure 3: Bairro Gulbenkian: aerial photograph.\nThe axial map of Bairro Gulbenkian (figure 4) reveals that the maximum depth from the\noutside is 2, that is, the lines that cover its convex spaces are one or two steps away from the\nend of the settlement.\nThe mean depth from the outside (MDo) is 1.32. It can be easily computed with the Prolog\nprogramme available at https://swish.swi-prolog.org/p/gulbenkian.pl (accessed on 12\nJune 2025) by asking the following query:\n?-meandepth(outside,Y,D,MD).\nThis computation is possible because we declared all the lines connected with \u2019outside\u2019 at the\nbeginning of the programme, namely,\nconnected(outside,hotelcar,1).\nThus, axial lines with depth 1 from the outside, such as \u2019pavilhao\u2019 or \u2019hotelcar\u2019, have a d-\nvalue of 0.76 = 1/1.32. Similarly, axial lines with depth 2, such as \u2019talude\u2019 or \u2019pcta grao vasco\u2019,\nhave a d-value of 1.52 = 2/1.32. These figures can be computed with the following Prolog query:\n?-dvalue(X,Y,Z,D,Q).\nthat uses the predicate dvalue defined in the main programme as\ndvalue(X,Y,Z,D,Q):-depth(outside,X,D), meandepth(outside,Y,Z,MD), Q is D/MD.\n6\n\n\n--- Page 7 ---\nFigure 4: Bairro Gulbenkian: axial map (dotted line: outside).\nInterestingly, most stores and services are located in spaces with depth 1, that is, with d-\nvalue close to one. This happens with the sports hall, nursery, elderly centre, and coffee shop,\nall located at \u2019pavilhao\u2019, the mall, located at \u2019cc girassol\u2019, and the garage, located at \u2019hotelar\u2019.\n6\nConclusion\nThe Hotelling model is founded on several unrealistic assumptions, namely, that the market is\na finite line where consumers are uniformly distributed and take decisions only on the basis of\nprice plus transportation costs linear to distance, there are no production costs or there is only\none identical good sold on the market by two firms, an assumption that can be easily relaxed\n(Economides, 1993). However, this model describes the essence of economic decision within a\nspatial framework: the distance between consumers and producers matters because it imposes\na cost, protecting local markets. Thus, a store located at some depth may have some market\npower, noting that optimal profits increase with the unit cost of transportation c:\n\u03c01 = p1q1 = c\n2\n\u0012\nl + a \u2212b\n3\n\u00132\n, and\n(11)\n\u03c02 = p2q2 = c\n2\n\u0012\nl + b \u2212a\n3\n\u00132\n.\n(12)\nThus, firms can be favoured locally when transport is difficult for consumers (Hotelling,\n1929). Because profits also depend on the distance to the end of the line, a or b, stores should\nbe located in an intermediate point between that end and the centre of the city. The proposed\nd-value captures this idea by comparing the depth from outside with the average depth of\nall possible locations. With this simple syntactic measure, we can introduce some economic\nreasoning to the social logic of space.\n7\n\n\n--- Page 8 ---\nAcknowledgments\nThis work was financed by Funda\u00b8c\u02dcao para a Ci\u02c6encia e a Tecnologia (FCT) under a doctor-\nate auxiliary researcher grant from Universidade Cat\u00b4olica Portuguesa (UCP) - Cat\u00b4olica Lis-\nbon Research Unit in Business & Economics (CUBE) with the digital object identifier (DOI):\n10.54499/CEECINST/00070/2021/CP1778/CT0008.\nReferences\nA. Colmerauer and P. Roussel. The birth of Prolog. ACM SIGPLAN Notices, 28(3):37\u201352, 3\n1993. ISSN 0362-1340. doi: 10.1145/155360.155362.\nN. Economides. Hotelling\u2019s \u201dMain Street\u201d with more than Two Competitors. Journal of Regional\nScience, 33(3):303\u2013319, 1993.\nP. A. Fernandes. Space Syntax with Logic Programming: An Application to a Modern Estate.\nUrban Science, 7(3):78, 2023. doi: 10.3390/urbansci7030078.\nJ. Hanson. Decoding Homes and Houses. Cambridge University Press, Cambridge, UK, 1998.\nT. Heitor and J. Pinelo Silva. A Sintaxe Espacial e o Ambiente Constru\u00b4\u0131do - An\u00b4alise Morfol\u00b4ogica.\nIn V. Oliveira, T. Marat-Mendes, and P. Pinho, editors, O Estudo da Forma Urbana em\nPortugal, pages 147\u2013189. Universidade do Porto, Porto, Portugal, 2015. ISBN 978-989-746-\n064-7.\nB. Hillier. Space is the machine: a configurational theory of architecture. Space Syntax, London,\nUK, 2007. URL https://spaceisthemachine.com/.\nB. Hillier and J. Hanson. The Social Logic of Space. Cambridge University Press, Cambridge,\nUK, 1984.\nH. Hotelling. Stability in Competition. The Economic Journal, 39(153):41\u201357, 3 1929.\nR. A. Kowalski. The early years of Logic Programming. Communications of the Association for\nComputing Machinery, 31(1):38\u201343, 1988. doi: 10.1145/35043.35046.\nJ. Malheiros, J. L. Z\u02c6ezere, A. Ludovici, S. Pereira, S. Oliveira, and M. Malheiros. Um s\u00b4eculo\nde respostas habitacionais p\u00b4ublicas a cat\u00b4astrofes - Experi\u02c6encias passadas e reflex\u02dcoes para o\nfuturo. In Habita\u00b8c\u02dcao, cem anos de pol\u00b4\u0131ticas p\u00b4ublicas em Portugal, 1918-2018, pages 365\u2013405.\nInstituto da Habita\u00b8c\u02dcao e da Reabilita\u00b8c\u02dcao Urbana, Lisboa, Portugal, 2018.\nA. Turner. Depthmap 4 - A Researcher\u2019s Handbook. Bartlett School of Graduate Studies, Uni-\nversity College of London, London, 2004. URL https://discovery.ucl.ac.uk/id/eprint/\n2651/.\nA. van Nes and C. Yamu. Introduction to Space Syntax in Urban Studies. Springer Nature, 2022.\nISBN 978-3-030-59139-7. doi: 10.1007/978-3-030-59140-3. URL https://link.springer.\ncom/book/10.1007/978-3-030-59140-3.\nJ. Wielemaker, T. Schrijvers, M. Triska, and T. Lager. SWI-Prolog. Theory and Practice of\nLogic Programming, 12(1-2):67\u201396, 2012. ISSN 1471-0684. doi: 10.1017/S1471068411000494.\n8\n",
    "pages": 8
  },
  {
    "filename": "GARCH_research.pdf",
    "text": "\n\n--- Page 1 ---\nReview of Integrative Business and Economics Research, Vol. 9, Issue 2 \n138 \n \nCopyright \uf6d9 2020 GMP Press and Printing \nISSN: 2304-1013 (Online); 2304-1269 (CDROM); 2414-6722 (Print) \n \nModeling Stock Market Volatility Using GARCH \nModels Case Study of Dar es Salaam Stock \nExchange (DSE) \n \nMutaju Marobhe \nTanzania Institute of Accountancy \n \nDickson Pastory* \nCollege of Business Education \n \n \nABSTRACT \nThis study was carried out to model volatility of stock returns at Dar es Salaam Stock \nExchange (DSE) using daily closing stock price indices from 2nd January 2012 to 22nd \nNovember 2018. Modeling was done using both symmetrical and asymmetrical \ngeneralized auto regressive Heteroskedastic model (GARCH) models; these were \nGARCH (1,1), E-GARCH (1,1) and P-GARCH (1,1). The findings showed that all three \n(3) models were significant to forecast stock returns volatility at DSE. GARCH (1,1) and \nP-GARCH (1,1) both revealed that the magnitude of shocks in volatility is higher with \ngood news as opposed to bad news. E-GARCH model (1,1) showed the evidence of \nleverage effect associated with the stock returns which can be detrimental to the trading \ncompanies\u2019 capital structures. P-GARCH (1,1) was found to be more accurate to in \npredicting stock returns based on both the Root Mean Squares Error (RMSE) and Theil \nInequality Coefficient (TIC).   \nKeywords: Volatility, Dar es Salaam Stock Exchange (DSE), GARCH. \nReceived 28 November 2018 | Revised 22 February 2019 | Accepted 13 May 2019. \n \n1. INTRODUCTION \nVolatility refers to the amount of risk or uncertainty pertaining to the variations in a \nsecurity\u2019s value. Some securities are highly volatile which implies that their values \nfluctuate over a larger range of values while others are less volatile which means that \ntheir values can be spread out over s smaller range of values. Fama (1965) depict that this \nvariation/deviation of securities\u2019 returns are not directly observable hence it\u2019s the duty of \ntraders, institutional investors and other participants to have an understanding of the \nnature of the returns between return and volatility.  \n \nThe Global growth of stock markets has aroused interest among researchers and \npractitioners about modeling volatility of stock returns. Modeling volatility forms a vital \npart of designing investment plans to reduce risk and improve stock returns and it is also \nvery useful in securities and options pricing. However, its importance is not only \nconfined to investors and other market participants, but also to the overall economy as \nwell. High levels of volatility tend to distort stability of capital markets, destabilize \ncurrency value and hinder international trade (Bhowmik, 2013).  \n \n\n\n--- Page 2 ---\nReview of Integrative Business and Economics Research, Vol. 9, Issue 2 \n139 \n \nCopyright \uf6d9 2020 GMP Press and Printing \nISSN: 2304-1013 (Online); 2304-1269 (CDROM); 2414-6722 (Print) \n \nOver the years numerous models have been devised by researchers seeking to model \nvolatility in stock returns, these have been grouped into symmetrical and non-\nsymmetrical models. Engle (1982) is considered to be the pioneer of volatility modeling \ndesigned Auto regressive Conditional Heteroskedastic (ARCH) model to forecast time \nseries data volatility. After a few years (Bollerslev, 1986) developed a model known as \nGeneralized-ARCH (GARCH) model. The other models include GARCH in Mean Model \n(GARCH-M) by (Engle et al, 1987); Exponential GARCH (EGARCH) model by \n(Nelson, 1991) and Threshold-GARCH (TGARCH) by (Zakoian, 1994).  \n \nStock market volatility has been widely researched in developed countries; unfortunately \nthe case is different for Sub Saharan Africa as only a few studies have been carried out \nover the years to investigate the matter. Studies such as those by (Wagala et al; 2012) on \nNairobi Stock Exchange, (Ogege; 2016) on Nigerian Stock Exchange are among a few of \nthese studies that forecast stock market volatility. So this paper aims to add knowledge \nabout stock market volatility in Africa by modeling this phenomenon at Dar es Salaam \nStock Exchange (DSE) using daily closing price indices in the period from 2nd January \n2012 to 22nd November 2018.  \n \n1.1 Objectives of the Study \nThe main objective of this study is to model stock market volatility at Dar es Salaam \nStock Exchange (DSE). The specific objectives are as follows; \na) To forecast stock returns volatility using both symmetrical and asymmetrical \nmodels. \nb) To examine the accuracy of forecasting models in predicting volatility of stock \nreturns. \n \n1.2 Significance of the Study \nThis study is vital as it adds knowledge to the existing contrast between theories and \nempirical studies on the topic in Tanzania perspective. The financial analysts, investors \nand other key players in the Dar es Salaam Stock Exchange (DSE) will be able to get \nsome insights on how stock returns behave so that they can be in a position to predict \nfuture behavior. This will help these market players to improve stock returns using \nscientific means rather than just predicting stock price behavior on individual intuition or \ngut feeling. \n1.3 Overview of Dar Salaam Stock Exchange (DSE) \nThe Dar es Salaam Stock Exchange (DSE) is a stock exchange located Dar es Salaam \nCity, Tanzania. DSE was established by the capital markets and security authority \n(CMSA) under the Capital Markets and Securities (CMS) Act of 1994. It was \nincorporated in September 1996 but commenced trading in April 1998. DSE is a member \nof the African Stock Exchanges Association.  \nTrading is conducted five (5) days a week; from Monday through Friday from 10.00 am \nto 14.00 pm. DSE operations are monitored and supervised by the Capital Markets and \nSecurities Authority (CMSA). Trading at DSE is carried out through an Automated \nTrading System (ATS). ATS is an automated electronic system that matches bids and \noffers by making use of electronic matching engine. The ATS is fully integrated with the \n\n\n--- Page 3 ---\nReview of Integrative Business and Economics Research, Vol. 9, Issue 2 \n140 \n \nCopyright \uf6d9 2020 GMP Press and Printing \nISSN: 2304-1013 (Online); 2304-1269 (CDROM); 2414-6722 (Print) \n \nCDS to assist automated validation of securities holdings and straight through processing \nof securities transactions. \nInitially DSE was incorporated as a private company limited by guarantee and not having \na share capital under the Companies Ordinate, however in June 2015; DSE re-registered \nand became a public limited company. DSE changed its name from Dar Es Salaam Stock \nExchange Limited to Dar Es Salaam Stock Exchange Public Limited Company. \nDSE offers several benefits to issuers of financial instruments including reduced \ncorporate tax from 30% to 25% for three (3) successive years subsequent to listing of a \ncompany that have issued at least 25% of its shares to the public together with tax \ndeductibility of all Initial Public Offering (IPO) costs for the purposes of income tax \ndetermination.  \nThe investors at DSE enjoy zero capital gain tax as opposed to 10% for unlisted \ncompanies, zero stamp duty on transactions executed at the DSE compared to 6% for \nunlisted companies, 5% withholding tax on dividend income as opposed to 10% for \nunlisted companies and zero withholding tax on interest income from listed bonds whose \nmaturities are three years and above. As of November 2018 a total of 28 companies were \nlisted at DSE with a total market capitalization of TZS. 19.903071 Trillion. \n \n2. LITERATURE REVIEW \nA study by (Eryilmaz, 2015) modeled and examined stock market volatility of Istanbul \nStock Exchange using BIST-100 index for the period 1997-2015.The research employed \nARCH, GARCH, EGARCH and TARCH models and found out that the EGARCH best \nmodels volatility for BIST-100 and bad news that impact the market were observed to \naccelerate volatility at Istanbul Stock Exchange. \nSrinivasan (2011) conducted a study forecasting stock market volatility of S&P 500 index \nreturns of New York Stock Exchange (NYSE). The study made use of daily data from 1st \nJanuary 1996 to 29th January 2010 and employed GARCH (1,1), E-GARCH (1,1) and T-\nGARCH (1,1) models. The results revealed that the symmetric GARCH model is more \nefficient in forecasting conditional variance as opposed to asymmetric GARCH models \ninspite of leverage effect.  \n \nTamilselvan & Vali (2016) forecasted stock market volatility using four (4) indices from \nMuscat security market in the period 2001-2015. The study made use of GARCH, \nEGARCH and TGARCH  models and results revealed a  positive relationship between \nrisk and return. The findings further showed that GARCH models generated significant \nevidence of asymmetrical relationship between return shocks and volatility adjustments \nin all four (4) indices. \n \nWagala et al (2012) examined stock volatility at Nairobi Stock Exchange (NSE) by \nemploying the ARCH and GARCH models. The study used the Shwartz Bayesian \nCriteria (SBC), Akaike Information Criteria (AIC) and the Mean Squared Error (MSE) to \nevaluate the ARCH and GARCH models. The results revealed that the AR-Integrated \nGARCH (IGARCH) models are the most efficient models for forecasting volatility at this \nstock market.  \n\n\n--- Page 4 ---\nReview of Integrative Business and Economics Research, Vol. 9, Issue 2 \n141 \n \nCopyright \uf6d9 2020 GMP Press and Printing \nISSN: 2304-1013 (Online); 2304-1269 (CDROM); 2414-6722 (Print) \n \n \nIn another study, (Dima and Haim, 2008) modeled volatility of stock returns in using \nstock indices from Tel Aviv Stock Exchange (TASE) by employing GARCH and \nEGARCH models. The findings show that asymmetric GARCH model together with \nEGARCH model is more efficient in modeling stock indices volatility at TASE.  \n \nThe other study by (Ogege, 2016) assessed the nature of stock returns at Nigerian Stock \nExchange (NSE) employing monthly stock indices in the period January 2003-December \n2014. The research used GARCH (1.1) model to analyze stock returns and the results \nprovided strong evidence of volatility clustering in the NSE return series and volatility \npersistence for the Nigeria stock returns data.  \n \nBanumathy and Azhagaiah (2012) also modeled stock market volatility on Indian stock \nmarket using daily closing prices of S&P CNX Nifty Index for the period 2003 - 2012. \nBoth symmetric and asymmetric models of GARCH were used to analyze volatility and \nthe results found GARCH (1,1) and TGARCH (1,1) models to be the most appropriate \nmodels to forecast symmetric and asymmetric stock volatility respectively.  \n \nAhmed and Suliman (2011) analyzed volatility of daily stock returns at Khartoum Stock \nExchange (KSE) in the period (January 2006 - November 2010). The study employed \nboth symmetric and asymmetric GARCH models and found out that asymmetric models \nfar better estimations of volatility as compared to symmetric models the fact which shows \nthe evidence of leverage effect. The findings indicate high levels of volatility in stock \nreturns at KSE.   \n \n3. METHODOLOGY \n3.1 Research Design \nThis study employs a quantitative research design; modeling stock market volatility \ninvolves statistical analysis using quantitative stock market index data.  \n3.2 Types of Data \nThe study uses time series data from DSE daily closing price index, these statistics were \nobtained from DSE website which is the commonly used source for providing stock \nmarket information of listed companies together with the market indices in real time. \n \n3.3 Study Period \nThe study covers a period of from 2nd January 2012 to 22nd November 2018 which is \ndeemed to be sufficient period of time to generate appropriate conclusions due to \nsubstantial number of data sets/observations.  \n \n3.4 Data Analysis and Model Specifications \nData analysis tools were applied with respect to the specific objectives of the study and \nthis was done using STATA 14 software.  \n \n \n3.4.1 Normality Diagnostics \nBefore commencing stock return volatility modeling it is vital to examine whether the \ndaily time series data are normally distributed as a prerequisite. The basic descriptive \n\n\n--- Page 5 ---\nReview of Integrative Business and Economics Research, Vol. 9, Issue 2 \n142 \n \nCopyright \uf6d9 2020 GMP Press and Printing \nISSN: 2304-1013 (Online); 2304-1269 (CDROM); 2414-6722 (Print) \n \nstatistics were carried out namely; Mean, Standard deviation, Variance, Skewness and \nKurtosis.  \n \nLastly, the study conducted the Shapiro normality test as proposed by (Shapiro and \nFrancia, 1972) to statistically test whether the daily stock return data used for modeling \nare normally distributed. The following hypothesis was developed and tested for this test; \n \nHo = The time series data are not normally distributed \nH1 = The time series data are normally distributed \n \n3.4.2 Unit Root Test  \nModeling stock market returns requires time series data to be stationary i.e. must not have \na unit root. To test for unit root or stationarity of daily stock returns the Augmented Dick \nFuller Test (ADF) developed by (Dickey and Fuller, 1979). The ADF is presented as \nshown in the equation below; \n    \u0394yt = \u03b1yt-1 + \nas\u0394yt-s + vt \nWhereby Yt = Variable Y at current time \u201ct\u201d \n              Yt-1 = Variable Y at previous time \u201ct-1\u201d  \n \nThe following hypothesis was developed and tested; \nHo = Time series data does not contain a unit root \nH1 = Time series does contain a unit root \n \n3.4.3 Heteroskedasticity Diagnosis \nHeteroskedasticity is a condition whereby the variability/standard deviation of a variable \nis not constant over a period of time. Stock returns can sometimes exhibit this behavior \nand so before applying the forecasting models it was vital to test for presence of \nautoregressive conditional heteroskedasticity effects This was done by employing \nLangrage Multiplier for autoregressive conditional heteroskedasticity. The following \nhypothesis was developed; \nH0: There are autoregressive conditional Heteroskedastic effects in the time series data. \nH1: There are no autoregressive conditional Heteroskedastic effects in the time series \ndata. \n \n3.4.4 The autoregressive model \nTo model volatility of stock returns using ARCH and GARCH models it is vital to first \ndevelop an autoregressive equation which is as follows \n              SRt = \u03b21SRt-1  + \u03b2o + e \nWhereby; \n  SRt = Stock return at the current time \u201ct\u201d            SRt-1  = Stock return at the previous \ntime \u201ct-1\u201d \n  \u03b21   = Coefficient of stock market return at time (t-1)        \u03b2o = The intercept \n  e = A stochastic error term \nThe important variable in this model is the stock market return which refers to the \ngains/losses an investor realizes from the changes in stock\u2019s price. \n            Stock return (SR) = log (SRt/SRt-1 ) \n\n\n--- Page 6 ---\nReview of Integrative Business and Economics Research, Vol. 9, Issue 2 \n143 \n \nCopyright \uf6d9 2020 GMP Press and Printing \nISSN: 2304-1013 (Online); 2304-1269 (CDROM); 2414-6722 (Print) \n \nThe autoregressive model presented above is an indication of the fact that the stock return \nof the current period is dependent upon two (2) factors; firstly, the stock returns from \nprevious period and stochastic error term. This model must first be developed and tested \nbefore going into detail to model stock returns volatility. \n \n3.4.5 Symmetrical Volatility Models \nThis study aims to forecast volatility using both symmetrical and non symmetrical \nforecasting models. The first part of modeling employed symmetrical models, the models \nand their descriptions are as follows; \nGeneralized Auto Regressive Conditional Heteroskedastic (GARCH) Model \nGARCH model was introduced by (Bollerslev, 1986) is an improvement to the previous \nARCH model which includes a moving average aspect in modeling time series data \nvolatility in addition to autoregressive aspect. But the problem with this model is that it \ndoes not capture asymmetrical volatility of returns. This is presented as follows; \nMean equation; rt = \u00b5 +\u03b5 t \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.(i)     \nVariance equation; \u03c3t2 = \u03b10 + \u2211qi=1 \u03b1i \u03b52t-1 +\u2211p j=1\u03b2j \u03b12 t-j\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026(ii)   \nWhere; \u03c3t2 = Conditional variance;  \u00b5 = Average return;  \u03b5t= residual returns \nAssume \u03b10 > 0; \u03b1i \u2265 0, i = 1, q; \u03b2j \u2265 0, j = 1, p; \u2211qi-1 \u03b1i +\u2211p j-1\u03b2j < 1 for ensuring {\u03c32t } as \nweak stationary. \n3.4.6 Asymmetrical Volatility Models \na) Exponential \nGeneralized \nAuto \nRegressive \nConditional \nHeteroskedastic                          \n(E-GARCH) Model \nE-GARCH was put forward by (Nelson, 1991) to model volatility of time series data \nbased on the asymmetrical effect of positive and negative error terms on volatility. The \nmodel forecasts volatility of a time series variable by using conditional variance as a \nmultiplicative function rather than addictive functions of lagged innovations. It \nincorporates both the symmetrical and asymmetrical volatility of returns. This is \npresented as follows; \n \nLog(\u03c3t2) =c + \u2211qi=1 \u03b1i ((|\u0190t-1 /\u03b1t-1| - E(|\u0190t-1 /\u03b1t-1|))+\u2211p j=1\u03b2j log\u03b12 t-j  +  \u2211pi=1  \u03b3i  (\u0190t-1 /\u03b1t-1) \nWhereby; \u03b1 = The symmetric effect; \n                \u03b2 = measures the lagged conditional variance and \u03b3 reflects the asymmetric \nperformance. \nb) Power Generalized Auto Regressive Conditional Heteroskedastic (P-GARCH) \nModel \nPGARCH was developed by (Ding et al, 1993) and the model took a different approach \ncompared to the preceding models by using conditional standard deviation rather than \nconditional variance as a measure of volatility. It does not impose power parameter as in \n\n\n--- Page 7 ---\nReview of Integrative Business and Economics Research, Vol. 9, Issue 2 \n144 \n \nCopyright \uf6d9 2020 GMP Press and Printing \nISSN: 2304-1013 (Online); 2304-1269 (CDROM); 2414-6722 (Print) \n \nthe E-GARCH but it generated its own power based on the nature of volatility. This is \npresented as follows; \n    \u03c3t\u03c3 = \u019c  + \u03b21 \u03b1\u03c3t-1  + (|\u0190t-1| -  \u03b31\u0190t-1) \u03c3 \n \nWhereby; \u03b11 and \u03b21 = standard ARCH and GARCH parameters;    \u03b31= The leverage \nparameter  \n                              \u03c3 = \u03a4he parameter for the power term. \n3.4.7 Forecasting accuracy \nThe forecasting accuracy of each forecasting model tested was measured by the following \ntools; \n3.4.7.1 Root Mean Squares Error (RMSE) \nThis test estimates the differences between the observed values and the forecasted \ndependent variables by summing them up together and dividing the total by degrees of \nfreedom to obtain the mean error sum of squares. Then the square root of the mean \nerror sum of squares is the RMSE. The forecasting model accuracy is measured by the \nmagnitude of RMSE and usually a smaller value means less errors. \n3.4.7.2 Theil Inequality Coefficient (TIC) \n TIC as proposed by (Theil, 1958) is an index that measures forecasting accuracy using \nthe ratio of the Mean Square Error (MSE) of the predicted values and Mean Square \nError (MSE) of the observed actual values. The coefficient ranges from 0 to 1 with the \nvalues near to zero (0) indicating less errors and more accurate forecast.   \n4. RESEARCH FINDINGS AND RESULTS  \n4.1 Descriptive Statistics \nThe descriptive statistics for weekly DSE returns for the study period are presented in \ntable 1 below; \nTable 1: Results from descriptive statistics for daily DSE returns  \n \nSource: Field data (2018) \nThe results presented in table 1 show the mean stock returns of 0.011%, which indicates \npositive average returns to the stock investors at DSE. The stock returns are skewed to \nthe positive side with the skewness value of 0.2965 which indicates that the time series \ndata of stock returns is asymmetrical i.e. skewed to the right. Kurtosis value is high which \nis an indication that normal distribution curve has fatter and longer tails which makes it \nleptokurtic.  \n \n4.2 Shapiro-Francia Normality Test Results for DSE daily Returns \nThe results for Shapiro-Francia normality test for DSE daily stock returns are presented \nin table 2 below: \n \n \n \n \n \n99%       1.4425        14.2504       Kurtosis       121.6332\n95%        .7152         7.1094       Skewness       .2965913\n90%       .47235         6.7253       Variance       .6687197\n75%        .1695         4.8252\n                        Largest       Std. Dev.      .8177528\n50%       .00235                      Mean           .0114493\n\n\n--- Page 8 ---\nReview of Integrative Business and Economics Research, Vol. 9, Issue 2 \n145 \n \nCopyright \uf6d9 2020 GMP Press and Printing \nISSN: 2304-1013 (Online); 2304-1269 (CDROM); 2414-6722 (Print) \n \nTable 2: Results from Shapiro-Francia normality test  \n \nSource: Field data (2018) \n \nThe results from table 2 indicate that the p value is very small i.e. far less than 0.05 level \nof confidence hence the null hypothesis is rejected. This shows that DSE returns used in \nthis study are not normally distributed which is a common phenomenon in financial time \nseries data.  \n \n4.3 DSE Daily Stock Returns Trend  \nThis study models volatility of DSE daily stock returns from 2nd January 2012 to 22nd \nNovember 2018. The trend of these returns is presented in figure 1 below; \n \nFigure 1: DSE stock returns in the period 2nd January 2012 to 22nd November 2018) \n \nSource: Field data (2018) \n \nThe graphical presentation of stock returns shows how they behavior over time. It can be \nobserved that variations in returns have increased over time from 2014. Volatility \nclustering has increased from this year to date as compared to the period before 2014 \nwhich indicates the increase in the magnitude of volatility at DSE. Understanding how \nreturns behave is vital for forecasting how volatile they and the trend shows that volatility \nhas increased over time which can cause investors to be skeptical in making stock \ninvestment. \n \n \n4.4 Augmented Dickey-Fuller (ADF) Unit Root Test Results \n \nstockreturnt        1,690    0.50727    501.313    16.117    0.00000\n                                                                    \n    Variable          Obs       W           V         z       Prob>z\n-20.0000\n-15.0000\n-10.0000\n-5.0000\n0.0000\n5.0000\n10.0000\n15.0000\n20.0000\n01/02/12\n01/02/13\n01/02/14\n01/02/15\n01/02/16\n01/02/17\n% RETURNS\nPeriod\n\n\n--- Page 9 ---\nReview of Integrative Business and Economics Research, Vol. 9, Issue 2 \n146 \n \nCopyright \uf6d9 2020 GMP Press and Printing \nISSN: 2304-1013 (Online); 2304-1269 (CDROM); 2414-6722 (Print) \n \nThe ADF results for unit root of DSE stock returns are presented in table 3 below;  \nTable 3: Results from Augmented Dickey Fuller test for unit root \n \nSource: Field data (2018) \n \nThe ADF test results presented in table 3 above show that the test statistic value of -3.725 \nis less than the 1% critical value of -3.960, so the null hypothesis cannot be rejected at \n1% confidence interval. This indicates that the DSE stock returns are stationary i.e. do not \ncontain a unit root which makes them appropriate for volatility modeling. \n \n4.5 Langrage Multiplier (LM) Heteroskedasticity Test Results \nThe results for this crucial test required before applying GARCH models to forecast stock \nreturns are presented in table 4 below; \nTable 4: Results from Langrage Multiplier test for auto regressive conditional \nHeteroskedastic effects \n \nSource: Field data (2018) \n \nThe LM test results presented in table 4 shows that the p-value to be less than the 0.05 \nconfidence interval so the null hypothesis is rejected which means that DSE daily returns \nhave ARCH effects. This is a common feature of stock returns because the variance of \nreturns over time changes which is a condition known as heteroskedasticity.   \n \n4.6 Symmetrical Volatility Modeling Results \n4.6.1 Generalized Auto Regressive Conditional Heteroskedastic (GARCH) Modeling \nResults \nThe study modeled DSE stock returns by first employing GARCH model which is the \nmost appropriate tool in the family of symmetrical models. The results for this model are \npresented in table 5 below; \n \nThe results presented in table 5 indicate that GARCH model is significant to explain \nvolatility of DSE daily stock returns. This can be explained by the fact that the p-value is \nvery small and far less than the 0.05 confidence interval. The GARCH coefficient of \n0.6539662 is greater than zero (0) i.e. positive, so the argument can be made that positive \nor good news have a greater impact on stock returns volatility as opposed to negative or \nbad news.  \n \n \n \n \nMacKinnon approximate p-value for Z(t) = 0.0208\n                                                                              \n Z(t)             -3.725            -3.960            -3.410            -3.120\n                                                                              \n               Statistic           Value             Value             Value\n                  Test         1% Critical       5% Critical      10% Critical\n                                          Interpolated Dickey-Fuller          \n                                                                           \n       1              183.133               1                   0.0000\n                                                                           \n    lags(p)             chi2               df                 Prob > chi2\n                                                                           \n\n\n--- Page 10 ---\nReview of Integrative Business and Economics Research, Vol. 9, Issue 2 \n147 \n \nCopyright \uf6d9 2020 GMP Press and Printing \nISSN: 2304-1013 (Online); 2304-1269 (CDROM); 2414-6722 (Print) \n \nTable 5: Results from generalized auto regressive conditional Heteroskedastic \n(ARCH) modeling  \n \nSource: Field data (2018) \n \n4.7 Asymmetrical Volatility Models \n4.7.1Exponential \nGeneralized \nAuto \nregressive \nConditional \nHeteroskedastic                          \n(E-GARCH) Modeling \nE-GARCH model results for stock returns volatility are presented in table 6; \n \nTable 6: Results from exponential generalized auto regressive conditional \nHeteroskedastic  (ARCH) modeling  \n     \n \nSource: Field data (2018) \n \nThe EGARCH modeling results presented in table 6 indicate that the model is significant \nto explain daily stock returns volatility at DSE as shown by the p-value which is far less \nthan 0.05 confidence interval. The model has a coefficient of -0.4317501 which is less \nthan zero (0) i.e. negative which indicates the fact that shocks in stock returns caused by \nbad or negative news are exceed those shocks caused by positive news. This is an \nindication of leverage effect on companies\u2019 capital structure which can increase risks \ncaused by increasing proportion of debts. \n \n                                                                              \n       _cons     .0004225   .0000608     6.95   0.000     .0003034    .0005417\n              \n         L1.     .6539662   .0039723   164.63   0.000     .6461806    .6617518\n       garch  \n              \n         L1.     1.427424   .0282911    50.45   0.000     1.371975    1.482874\n        arch  \nARCH          \n                                                                              \n       _cons      .007442    .002148     3.46   0.001     .0032319     .011652\ndse           \n                                                                              \n         dse        Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n                               OPG\n                                                                              \n       _cons     -1.26079   .0275875   -45.70   0.000    -1.314861    -1.20672\n              \n         L1.     .1355331   .0055158    24.57   0.000     .1247223     .146344\n        arch  \n              \n         L1.    -.4317501   .0275647   -15.66   0.000     -.485776   -.3777242\n      egarch  \nARCH          \n                                                                              \n       _cons     .0092916   .0152567     0.61   0.543    -.0206111    .0391943\ndse           \n                                                                              \n         dse        Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n                               OPG\n                                                                              \n\n\n--- Page 11 ---\nReview of Integrative Business and Economics Research, Vol. 9, Issue 2 \n148 \n \nCopyright \uf6d9 2020 GMP Press and Printing \nISSN: 2304-1013 (Online); 2304-1269 (CDROM); 2414-6722 (Print) \n \n4.7.2 Power Generalized Auto Regressive Conditional Heteroskedastic (GARCH) \nModeling Results \nThe results for PGARCH modeling of DSE daily stock returns are presented in table 7 \nbelow: \n \nTable \n7: \nResults \nfrom \npower \ngeneralized \nauto \nregressive \nconditional \nHeteroskedastic (PGARCH) modeling  \n \nSource: Field data (2018) \n \nThe results from PGARCH model for DSE daily stock returns show a very small p-value \nthat is far less than 0.05 confidence interval which implies that this particular model is \nsignificant to forecast DSE daily stock returns. The model has a coefficient of 0.6739781 \nwhich is greater than zero (0), so the case can be made that based on PGARCH, positive \nor good news have a tremendous impact on stock returns volatility as opposed to negative \nor bad news. \n4.8 Forecasting Accuracy \nIt has been observed that GARCH, EGARCH and PGARCH models are all significant in \nforecasting DSE daily stock returns. So after this, the study forecasted stock returns based \non each of these significant models for the period 2nd January 2012 to 22nd November \n2018. The forecasted figures were compared with the actual observed to test for \nforecasting accuracy.  Two (2) tools namely; Root Mean Squared Error (RMSE) and \nTheil Inequality Coefficient (TIC) were used to assess forecasting accuracy and the \nresults are presented in table 8; \nTable 8: Root Mean Squared Error and Theil Inequality Coefficient Results for \nforecasting accuracy \nNo. Model \nRoot Mean Squared Error \n(RMSE) \nTheil Inequality Coefficient \n(TIC) \n1. \nGARCH (1,1) \n17.969 \n0.5876 \n2. \nE-GARCH (1,1) \n29.875 \n0.6101 \n3. \nP-GARCH (1,1) \n2.6814 \n0.4724 \nSource: Field data (2018) \n                                                                              \n       _cons    -.0000672   .0000122    -5.52   0.000     -.000091   -.0000433\n              \n         L1.     .7586091   .0196685    38.57   0.000     .7200596    .7971586\n        arch  \n              \n         L1.     .6739781   .0043132   156.26   0.000     .6655244    .6824318\n      pgarch  \nARCH          \n                                                                              \n       _cons     .0096765   .0008598    11.25   0.000     .0079913    .0113616\ndse           \n                                                                              \n         dse        Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n                               OPG\n                                                                              \n\n\n--- Page 12 ---\nReview of Integrative Business and Economics Research, Vol. 9, Issue 2 \n149 \n \nCopyright \uf6d9 2020 GMP Press and Printing \nISSN: 2304-1013 (Online); 2304-1269 (CDROM); 2414-6722 (Print) \n \nThe results from table 8 indicate that P-GARCH (1,1) forecasting model has the lowest \nRMSE compared to the other forecasting model which makes it more accurate in \nforecasting DSE stock returns volatility. On the other hand, P-GARCH (1,1) has also the \nlowest TIC compared to the other two (2) forecasting models. TIC ranges from 0 to 1 and \nthe smaller it is, the smaller is the difference between observed values and forecasted \nvalues hence more accuracy. So in this case the argument can be made that P-GARCH \n(1,1) is more accurate in forecasting stock returns volatility at DSE. \n4.9 Conclusions \nStock markets play an important role by enabling companies to raise extra capital from \nthe public which enables them to expand their operations, increasing national income by \npaying taxes from profits and employ more people. Benefits from stock markets are not \nonly confined to listed companies but also investors, brokers and the economy as well. So \na well-functioning stock market is crucial for economic development especially of \ndeveloping countries such as Tanzania.  \nOne of the key issues that concerns market participants is that of volatility of stock \nreturns. Highly volatile markets lower investors\u2019 confidence hence affecting the total \nmarket capitalization due to fear of losses due to the unpredictability of the markets. \nStock markets that are less volatile are considered to be stable and create investors\u2019 \nconfidence which increases their propensity to invest their funds. So the crucial aspect \namong experts is to understand the behavior or volatility of stock returns by forecasting \nor modeling them so that proper decisions can be made based on strong grounds. For \ninstance options can be correctly priced if volatility is well forecasted which can help \ndealers and investors improve their profits.  \nThis study has modeled volatility of stock returns at DSE using both symmetrical and \nasymmetrical models so as to ensure that the most efficient forecasting model is \nidentified and put into use in this case the P-GARCH (1,1) was found to be more accurate \nas opposed to GARCH (1,1) and P-GARCH (1,1). So DSE participants are urged to apply \nthis model in their efforts to forecast stock returns volatility and reduce uncertainties \nassociated with these returns. \nACKNOWLEDGEMENT \nI would like to extend my sincere gratitude to my employer, Tanzania Institute of \nAccountancy for the financial support that facilitated presentation and eventual \npublication this paper.  \nREFERENCES \n[1] Ahmed, E. J & Suliman, S. D. (2011), \u201cModeling Stock Market Volatility Using \nGARCH Models Evidence from Sudan, International Journal of Business and Social \nScience, 2(23), 43-54.  \n[2] Banumathy, K. E & Azhagaiah, R. J. (2012), \u201cModeling Stock Market Volatility: \nEvidence from India\u201d, Managing Global Transitions, 13(1), 28\u201344.                                                       \n[3] Bhowmik, D. M. (2013), \u201cStock Market Volatility: An Evaluation\u201d, International \nJournal of Scientific and Research Publications, 3(10), 112-123.                                                      \n\n\n--- Page 13 ---\nReview of Integrative Business and Economics Research, Vol. 9, Issue 2 \n150 \n \nCopyright \uf6d9 2020 GMP Press and Printing \nISSN: 2304-1013 (Online); 2304-1269 (CDROM); 2414-6722 (Print) \n \n[4] Bollerslev, \nT. \nA. \n(1986), \n\u201cGeneralized \nAutoregressive \nConditional \nHeteroskedasticity, Journal of Econometrics, 31(3), 308\u2013330.                                                                \n[5] Dickey, D. S & Fuller, W. H. (1979), \u201cDistribution of the Estimates For \nAutoregressive Time Series with a Unit Root\u201d, Journal of American Statistical \nAssociation, 74(6), 434-441.                            \n[6] Dima, A. N & Haim, S. K. (2008), \u201cEstimating Stock Market Volatility Using \nAsymmetric GARCH Models\u201d, Applied Financial Economics, 18(2), 1203\u20131212.                                 \n[7] Ding, Z. A., Granger, C. D & Engle, R. F. (1993), A Long Memory Property of \nStock Market Returns and A New Model, Journal of Empirical Finance, 1(1), 17-24.                           \n[8] Engle, R. H. (1982), \u201cAutoregressive Conditional Heteroskedasticity with Estimates \nof the variance of UK Inflation\u201d, Econometrica, 50(4), 1003\u20131007.                                                        \n[9] Engle, R. H., Lilien, D. E & Robins, R. G. (1987), \u201cEstimating Time Varying Risk \nPremia In The Term Structure: The ARCH-M Model\u201d, Econometrica, 55(2), 398\u2013\n412.                                           \n[10] Eryilmaz, F. M. (2015), \u201cModeling Stock Market Volatility: The Case of BIST-100\u201d, \nInternational Journal of Commerce and Finance, 2(1), 37-53.                                                                \n[11] Fama, E. K. (1965), \u201cThe Behavior of Stock Market Prices\u201d, Journal of Business, \n64(1), 34-105.  \n[12] Nelson, D. O. (1991), \u201cConditional Heteroscedasticity in Asset Returns: A new \napproach\u201d, Econometrica, 59(2), 345\u2013380.                                                                                               \n[13] Ogege, S. M. (2016), \u201cGauging The Volatility Level of Stock Returns in The \nNigerian Stock Market\u201d, The Pacific Journal of Science And Technology, 17(1), 118-\n120.                                                \n[14] Shapiro, S. N & Francia, R. J. (1972). \u201cAn Approximate Analysis of Variance Test \nfor Normality, Journal of American Statistics, 67(8), 217-222.                                                                \n[15] Srinivasan, P. M. (2011), \u201cModeling and forecasting the stock market volatility of \nS&P 500 index using GARCH Models\u201d, The IUP Journal of Behavioral Finance, \n8(1), 55-75.                            \n[16] Tamilselvan, M. I & Vali, S. B. (2016), \u201cForecasting Stock Market Volatility- \nEvidence from Muscat Security Market Using GARCH Models\u201d, International \nJournal of commerce, 2(1), 76-83.  \n[17] Theil, H. A. (1958), \u201cEconomic Forecasts and Policy\u201d, Amsterdam, North Holland.  \n[18] Wagala, A. C., Nassiuma, D. K., Islam, A. V & Mwangi, J. N. (2012), \u201cVolatility \nmodeling of the Nairobi Securities Exchange weekly returns using the ARCH-type \nmodels\u201d, International Journal of Applied Science and Technology, 2(3), 167-172.                               \n[19] Zakoian, J. S. (1994) Threshold heteroscedasticity models, Journal of Economic \nDynamics and Control, 18(5), 935\u2013 965. \n",
    "pages": 13
  },
  {
    "filename": "Mutivariate Quant volatility.pdf",
    "text": "\n\n--- Page 1 ---\nMultivariate Rough Volatility\nRanieri Dugo\u2217\nGiacomo Giorgio\u2020\nPaolo Pigato\u2021\nDecember 20, 2024\nAbstract\nMotivated by empirical evidence from the joint behavior of realized volatility time series, we propose to\nmodel the joint dynamics of log-volatilities using a multivariate fractional Ornstein-Uhlenbeck process.\nThis model is a multivariate version of the Rough Fractional Stochastic Volatility model proposed in\nGatheral, Jaisson, and Rosenbaum, Quant. Finance, 2018. It allows for different Hurst exponents in the\ndifferent marginal components and non trivial interdependencies.\nWe discuss the main features of the model and propose an estimator that jointly identifies its parameters.\nWe derive the asymptotic theory of the estimator and perform a simulation study that confirms the\nasymptotic theory in finite sample.\nWe carry out an extensive empirical investigation on all realized volatility time series covering the\nentire span of about two decades in the Oxford-Man realized library. Our analysis shows that these time\nseries are strongly correlated and can exhibit asymmetries in their cross-covariance structure, accurately\ncaptured by our model. These asymmetries lead to spillover effects that we analyse theoretically within\nthe model and then using our empirical estimates. Moreover, in accordance with the existing literature,\nwe observe behaviors close to non-stationarity and rough trajectories.\nKeywords: stochastic volatility, rough volatility, realized volatility, multivariate time series, volatility\nspillovers, mean reversion.\nJEL Classification: C32, C51, C58, G17.\n\u2217Department of Economics and Finance, University of Rome Tor Vergata, ranieri.dugo@students.uniroma2.eu\n\u2020Department of Mathematics, University of Rome Tor Vergata, giorgio@mat.uniroma2.it\n\u2021Department of Economics and Finance, University of Rome Tor Vergata, paolo.pigato@uniroma2.it\nAcknowledgements: We are grateful to Tommaso Proietti, Stefano De Marco, Davide Pirino, to the participants to the 4th\nMeeting in Probability in Rome, the QFFE24 in Marseille, and the VI Aarhus Workshop in Econometrics for early feedback.\nWe thank J.-F. Coeurjolly for sharing the code for simulating the multivariate fractional Brownian Motion. Funding: PP was\nsupported by the project PRICE, financed by the Italian Ministry of University and Research under the program PRIN 2022,\nProt. 2022C799SX.\n1\narXiv:2412.14353v1  [q-fin.ST]  18 Dec 2024\n\n\n--- Page 2 ---\n1\nIntroduction\nWe consider the log-normal, fractional model proposed in Comte and Renault (1998) and Gatheral et al.\n(2018) for volatility time series and introduce a multivariate extension, where the dynamics of the multivariate\nlog-volatility is specified by an Ornstein-Uhlenbeck process driven by the multivariate fractional Brownian\nmotion (mfBm) in the sense of Amblard et al. (2010), a process that we have analysed in Dugo et al. (2024).\nThe marginal components are one dimensional fractional Ornstein-Uhlenbeck processes, consistently with the\nestablished literature on fractional and rough volatility, with Hurst regularity parameter that can be different\nin each component. The cross-correlation between each two components is ruled by one parameter related\nto the contemporaneous correlation, and another parameter related to the time reversibility of the process.\nBoth are inherited from the driving mfBm.\nWe estimate the parameters of the model using a Minimum Distance Estimation procedure (Tieslau et al.\n1996), which aims at matching model-implied and empirical cross-covariances. First, we derive the asymptotic\ntheory of the estimator, proving speed of convergence and asymptotic normality, and perform an extensive\nsimulation study that confirms these results in finite sample. Then, we estimate the model on 20 years of 22\nrealized volatility time series data from the Oxford-Man library, finding a remarkably good fit, particularly\nwith respect to cross-correlations and their possibly asymmetric decay as a function of the time lag, which\nseems to be related to the Hurst exponents of the components as predicted by our model. These asymmetries\nsuggest the presence of spillover effects between components, which we analyse theoretically within the model,\nin the framework of Diebold and Yilmaz (2012), and then empirically using the parameter estimates obtained\nin sample. Our analysis also suggests, consistently with the known literature, roughness in the trajectories\nand behaviors close to non-stationarity.\nBackground: A mean-reverting, fractional, Gaussian volatility model, able to reproduce the long-memory\nbehavior of realized volatility, and based on the fractional Ornstein-Uhlenbeck process (see Cheridito et al.\n2003), was first introduced in Comte and Renault (1998). Estimations of this model on empirical data,\non short time lags, have subsequently hinted to extremely small Hurst parameters for realized volatility\ntrajectories, supporting the \u201drough volatility\u201d specification by Gatheral et al. (2018). The empirical rough\nbehavior of the trajectories of this model has been confirmed by a number of econometrical studies (Bolko\net al. 2023; Wang et al. 2023; Eumenius-Schulz 2020; Bianchi et al. 2023), and the estimation of similar\nmodels also points to rough behavior of the realized volatility trajectories (Bennedsen et al. 2021; Chong et al.\n2024b; Chong et al. 2024a; Fukasawa et al. 2022; Livieri et al. 2018). The fact that empirical log-volatility is\nmean-reverting, Gaussian, and exhibits fractional features is well established (Fouque et al. 2000; Andersen\net al. 2001; Ding and Granger 1996). The rough volatility specification, moreover, is supported by the option\npricing literature with particular attention to the implied volatility skew (Bayer et al. 2016; Bayer et al. 2019;\nFriz et al. 2022; Livieri et al. 2018; Delemotte et al. 2023, see also Guyon and El Amrani 2023).\nConcerning the modelling of multivariate time series and their cross memory structure, let us mention\nPodobnik et al. (2010), Podobnik et al. (2007), Wang et al. (2011), and Podobnik et al. (2008), where\napplications to economics, but also to physics, physiology, and genomics, are considered. Time-reversibility\nhas also been widely considered in the financial literature, at least in the unidimensional setting (see e.g.\nZumbach 2009; Cordi et al. 2021).\nOutline: Section 2 introduces and describes the model. Section 3 develops the estimation procedure and\nits asymptotic properties. Section 4 evaluates the estimator finite-sample performance through simulations.\nSection 5 details the empirical estimation results. Section 6 examines spillover effects. Section 7 concludes.\nProofs and supplementary results are provided in the Appendix, with additional empirical analyses and\nreproducible code available online.\n2\nThe Model\nThe model we propose for the logarithm of realized volatility is a multivariate version of the Rough Fractional\nStochastic Volatility model by Gatheral et al. (2018). It is the solution to a mean-reverting stochastic\ndifferential equation (SDE) driven by the mfBm by Amblard et al. (2010). We introduced this continuous-time\nprocess, that we call multivariate fractional Ornstein-Uhlenbeck (mfOU) process, in Dugo et al. (2024).\nLet (W H\nt )t = (W H1\nt\n, . . . , W HN\nt\n)t, be a mfBm, i.e. a vector-valued Gaussian process in dimension N\n2\n\n\n--- Page 3 ---\nwith parameters H \u2208(0, 1)N, \u03c1 \u2208[\u22121, 1]N\u00d7N, and \u03b7 \u2208RN\u00d7N. In this context, H = (H1, . . . , HN) is the\nvector of Hurst exponents of the components, the matrix \u03c1 represents the contemporaneous correlation of the\nmfBm at each point in time, and the matrix \u03b7 is related to the asymmetry in time of the cross-covariance\nfunction and the time reversibility of the process. These matrices are such that \u03c1i,i = 1, \u03c1i,j = \u03c1j,i, \u03b7i,i = 0,\nand \u03b7i,j = \u2212\u03b7j,i for i, j = 1, . . . , N. In addition, the parameters Hi, \u03c1i,j, and \u03b7i,j need to satisfy pairwise\ncoherency constraints, c(Hi, Hj, \u03c1i,j, \u03b7i,j) \u22641, \u2200i, j = 1, . . . , N, in order to have a well-defined covariance\nfunction (see Dugo et al. 2024).\nThe mfOU process, (Yt)t = (Y 1\nt , . . . , Y N\nt )t, is the vector process whose components solve pathwise\n(Cheridito et al. 2003) equations\ndY i\nt = \u03b1i(\u00b5i \u2212Y i\nt )dt + \u03bdidW Hi\nt\n,\ni = 1, . . . , N,\n(1)\nwhere \u00b5i \u2208R is the long-term mean, \u03bdi > 0 is the diffusion coefficient, and \u03b1i > 0 is the speed of mean\nreversion. Let us write \u03bd = (\u03bd1, . . . , \u03bdN) and \u03b1 = (\u03b11, . . . , \u03b1N). Equation (1) has a stationary solution given\nby\nY i\nt = \u00b5i + \u03bdi\nZ t\n\u2212\u221e\ne\u2212\u03b1i(t\u2212s)dW Hi\ns ,\ni = 1, . . . , N.\n(2)\nThis is an autoregressive process, as one can see writing\nY i\nt = e\u2212\u03b1i\u2206Y i\nt\u2212\u2206+\n\u00001 \u2212e\u2212\u03b1i\u2206\u0001\n\u00b5i + \u03bdi\nZ t\nt\u2212\u2206\ne\u2212\u03b1i(t\u2212s)dW Hi\ns ,\ni = 1, . . . , N.\nIn this work, we assume to observe the log-volatility process at the stationary regime. Its expectation is\nE [Yt] = \u00b5 = (\u00b51, . . . , \u00b5N).\nLet us denote \u03b3i,j(k) := Cov(Y i\nt+k, Y j\nt ), k \u2208R, the cross-covariance function. We assume here Hi,j =\nHi + Hj \u0338= 1, in which case\n\u03b3i,j(k) = e\u2212\u03b1jk\u03b3i,j(0) + \u03bdi\u03bdje\u2212\u03b1jkHi,j(Hi,j \u22121)\u03c1i,j + \u03b7i,j\n2\nZ k\n0\ne\u03b1jv\n\u0012Z 0\n\u2212\u221e\ne\u03b1iu(v \u2212u)Hi,j\u22122du\n\u0013\ndv,\n(3)\nwhere the covariance, \u03b3i,j(0), is\n\u03b3i,j(0) = \u0393(Hi,j + 1)\u03bdi\u03bdj\n2(\u03b1i + \u03b1j)\n\u0010\u0010\n\u03b11\u2212Hi,j\ni\n+ \u03b11\u2212Hi,j\nj\n\u0011\n\u03c1i,j +\n\u0010\n\u03b11\u2212Hi,j\nj\n\u2212\u03b11\u2212Hi,j\ni\n\u0011\n\u03b7i,j\n\u0011\n.\nIn Dugo et al. (2024) we showed that the cross-covariance decays as a power law with exponent Hi,j \u22122,\nas k \u2192\u221e, therefore allowing for long-range interdependence (meaning that the cross-covariance is not\nintegrable) when Hi,j > 1, in analogy to long-memory in the univariate case. Note that a different expression\nholds for the covariance function when Hi,j = 1, which corresponds to a discontinuity of the cross-covariance\nas a function of the Hurst exponents.\nIn line with the univariate case treated by Gatheral et al. (2018), if the mean reversion coefficients are\nsmall, the process behaves locally as a mfBm.\nProposition 1. Let\n\u0000W H\nt\n\u0001\nt be a mfBm, (Yt)t be the mfOU process in (2). Then, as \u03b1 \u21920,\nE\n\"\nsup\nt\u2208[0,T ]\n\f\f\f\fYt \u2212Y0 \u2212\u03bd \u2299W H\nt\n\f\f\f\f\n#\n\u21920,\nwhere || \u00b7 || represent the L2 norm and \u2299indicates the Hadamard product.\nProof. Follows from the univariate result in Gatheral et al. (2018).\nNote that \u03b3i,j(k) and \u03b3i,j(0) depend on \u03b1i and \u03b1j. In the regime of slow mean reversion, the cross-\ncovariance function of mfOU is approximately linear in kHi,j.\n3\n\n\n--- Page 4 ---\nProposition 2. For Hi,j < 1 and fixed k > 0, as (\u03b1i, \u03b1j) \u2192(0, 0),\n\u03b3i,j(k) = \u03b3i,j(0) \u2212\u03c1i,j + \u03b7i,j\n2\n\u03bdi\u03bdjkHi,j + o(1).\nProof. Follows by taking the limit in (3) and standard computations.\nAn analogous approximation result holds taking k \u21920 instead of (\u03b1i, \u03b1j) \u2192(0, 0) (see Dugo et al. 2024).\n3\nEstimation method\nIn order to jointly estimate all the parameters in our model, we consider a suitable loss function. The\nparameters count to p = N(N + 2), including N \u00d7 3 parameters governing the marginal distributions,\nspecifically \u03b1i, \u03bdi, and Hi for i = 1, . . . , N, and N(N \u22121)/2 parameters in each of the matrices \u03c1 and \u03b7,\ndetermining the multivariate dynamics. We subtract the sample mean from the observations of the process in\nthe beginning, so we do not deal with \u00b5i.\nWe define the parameter vector \u03b8 = (\u03b1i, \u03bdi, Hi, \u03c1i,j, \u03b7i,j, i = 1, . . . , N, i < j < N) \u2208\u0398 \u2282RN\n+ \u00d7 RN\n+ \u00d7\n(0, 1)N \u00d7 [\u22121, 1]N(N\u22121)/2 \u00d7 RN(N\u22121)/2, which represents the full set of parameters to be estimated. While\nmaximizing the likelihood function would be the most efficient approach, the non-Markovian nature of the\nprocess makes this computationally infeasible. In Amblard and Coeurjolly (2011), discrete filtering techniques\nare used in this context to estimate the mfBm. Here, we adopt a Minimum Distance Estimator (MDE) in the\nspirit of Tieslau et al. (1996), which can be seen as a specific case of the Generalized Method of Moments\n(Hansen 1982).\nIn the MDE approach, we consider an overdetermined system of equations in the parameters to be\nestimated, which we call moment conditions. The estimator is defined as the parameter value that is closest,\nin a mean-square sense, to solving the system. Let (Yi\u2206)n\ni=0 be a set of n \u2208N discrete observations of the\nprocess over the interval [0, T], observed at time intervals \u2206= T/n. Our goal is for the model-implied\ncross-covariances, calculated with the estimated parameters \u03b3k\ni,j(\u02c6\u03b8), to be as close as possible to sample\ncross-covariances\n\u02c6\u03b3k\ni,j =\n1\nn \u2212k\nn\u2212k\nX\nl=1\n\u0000Y i\nl+k \u2212\u02c6\u00b5i\n\u0001 \u0010\nY j\nl \u2212\u02c6\u00b5j\n\u0011\n,\nwhere \u02c6\u00b5i = 1\nn\nPn\nl=1 Y i\nl , i = 1, . . . , N are computed in our framework before the optimization takes place.\nWe define the vectors that contain, respectively, model and sample cross-covariances, ordered consistently,\nin the obvious manner:\n\u03b3(\u03b8) =\n\u0010\u0000\u03b3k\nij(\u03b8)\n\u0001\nk\u2208L, i,j=1,...,N\n\u0011T\n\u2208RN(L+(N\u22121)(L\u22121/2)),\n\u02c6\u03b3n =\n\u0010\u0000\u02c6\u03b3k\nij\n\u0001\nk\u2208L, i,j=1,...,N\n\u0011T\n\u2208RN(L+(N\u22121)(L\u22121/2)),\nwhere L is the cardinality of the set L \u2282N \u222a{0}, appropriately chosen (cf. Andersen and S\u00f8rensen 1996).\nWe define the MDE estimator \u02c6\u03b8 as the value of \u03b8 that minimizes the loss function\nT (\u03b8) = (\u02c6\u03b3n \u2212\u03b3(\u03b8))T W (\u02c6\u03b3n \u2212\u03b3(\u03b8)) ,\ni.e.\n\u02c6\u03b8n = arg min\n\u03b8\nT (\u03b8) ,\n(4)\nwhere W is a symmetric, positive definite matrix of order N(L+(N \u22121)(L\u22121/2)). Stationarity and ergodicity\nof the mfOU process, together with the regularity of the cross-covariance function, imply the following.\nProposition 3. Let \u03b80 \u2208\u0398 be the true value of the parameter in the population distributed as the mfOU\nprocess. If Hi,j \u0338= 1, \u2200i, j = 1, . . . , N:\nI. the MDE estimator is consistent, i.e.\n\u02c6\u03b8n\np\u2192\u03b80\nas n \u2192\u221e,\n4\n\n\n--- Page 5 ---\nII. when max (Hi, i = 1, . . . , N) < 3\n4, the MDE estimator is asymptotically normal, i.e.\n\u221an\n\u0010\n\u02c6\u03b8n \u2212\u03b80\n\u0011\nd\u2192N(0, \u03a3)\nas n \u2192\u221e,\nwhere \u03a3 =\n\u0000JT\n\u03b3 WJ\u03b3\n\u0001\u22121 JT\n\u03b3 W\u0393WJ\u03b3\n\u0000JT\n\u03b3 WJ\u03b3\n\u0001\u22121, J\u03b3 := J\u03b3(\u03b80) = \u2202\u03b3(\u03b8)/\u2202\u03b8|\u03b80 is the Jacobian matrix\nof \u03b3 (\u03b8) at \u03b80, and \u0393 is the asymptotic covariance matrix of \u02c6\u03b3n.\nThe proof of the above results is postponed to Appendix A. We expect, but do not prove here, a\nnon-Gaussian result to hold if max (Hi, i = 1, . . . , N) > 3/4, similarly to Dugo et al. 2024.\nRemark 1. The inequality\n(JT\n\u03b3 WJ\u03b3)\u22121JT\n\u03b3 W\u0393WD(JT\n\u03b3 WJ\u03b3)\u22121 \u2212(JT\n\u03b3 \u0393\u22121J\u03b3)\u22121 \u22650,\nsuggests that higher efficiency for \u02c6\u03b8 could be achieved by choosing W = \u0393\u22121, which delivers the lowest\nasymptotic variance \u03a3\u2217= (JT\n\u03b3 \u0393\u22121J\u03b3)\u22121. This is the the optimal weighting used in generalized linear models\n(see Hayashi 2011 for details). However, at present \u0393 appears intractable for the mfOU process (see Appendix\nA), so we proceed with the identity matrix W := I. We leave developing a computable expression for \u0393 and/or\nexploring its estimation as a possible future improvement of the method.\nWe solve the optimization in (4) numerically, using initial conditions provided by a 2-step estimator\nthat combines the univariate estimators from Wang et al. (2023) with those from Dugo et al. (2024) for the\ncorrelation parameters. We perform the numerical optimization with the L-BFGS-B algorithm within the\noptim() function in R 4.3.3.\n4\nMonte Carlo study\nIn this section, we evaluate the accuracy of the MDE procedure on synthetic data.\n4.1\nSimulation method\nWe simulate the mfOU process via the Euler-Maruyama scheme in combination with a circulant embedding\nscheme for the driving fractional Gaussian noise, which is an exact simulation method for stationary Gaussian\nprocesses initially introduced by Wood and Chan (1994) and later adapted by Amblard et al. (2010) to the\nmfBm.\nWe want a discrete trajectory on the uniform partition with mesh \u2206of the interval [0, T]. Given a set of\nparameters (\u03b1i, Hi, \u03bdi, \u00b5i, i = 1, . . . , N), we produce M time series of length n = T/\u2206repeating the following\nprocedure.\n1) We start with a longer horizon T \u22c6> T and a finer partition \u03b4 < \u2206. The preliminary period T \u22c6\u2212T is\nnecessary to obtain realizations from the stationary distribution while the finer partition \u03b4 < \u2206is used\nto reduce the discretization error induced by the Euler-Maruyama scheme.\n2) We produce a sequence of n\u22c6= T \u22c6/\u03b4 multivariate fractional Gaussian noises (mfGn) of Hurst exponents\nH1, . . . , HN given by the increments of the mfBm over the intervals of size \u03b4,\n\u0010\nW H1\nj\u03b4 \u2212W H1\n(j\u22121)\u03b4, . . . , W HN\nj\u03b4\n\u2212W HN\n(j\u22121)\u03b4\n\u0011n\u22c6\nj=1 ,\nwith the circulant embedding scheme proposed in Amblard et al. 2010.\n3) Starting from Y i\n0 , i = 1, . . . , N drawn from the stationary distribution, and the mfGn obtained at the\nprevious step, we calculate the Euler-Maruyama scheme for the mfOU process on the finer grid with\nmesh \u03b4,\nY i\nj\u03b4 = Y i\n(j\u22121)\u03b4 + \u03b1i\n\u0010\n\u00b5i \u2212Y i\n(j\u22121)\u03b4\n\u0011\n\u03b4 + \u03bdi\n\u0010\nW Hi\nj\u03b4 \u2212W Hi\n(j\u22121)\u03b4\n\u0011\n,\nfor j = 1, . . . , n\u22c6, i = 1, . . . , N.\n5\n\n\n--- Page 6 ---\n4) We discard the first (T \u22c6\u2212T) /\u03b4 observations so as to obtain T/\u03b4 observations of the process at the\nstationary regime and retain a subset (Yj\u2206)n\nj=0 of observations at intervals \u2206, for which the discretization\nerror is smaller, from the finer discretized trajectory (Yj\u03b4)T/\u03b4\nj=0.\nFor each set of parameters considered in the following, the previous methodology is implemented with\nN = 2, \u00b5i = 0, i = 1, 2, \u03b4 = 1/(252 \u00d7 210), \u2206= 1/252, T \u22c6= 28, T = 20, and M = 104. The values for \u2206\nand T are similar to the daily observations over roughly 20 years in the dataset employed in the empirical\nanalysis.\n4.2\nFinite sample performance\nWe conduct Monte Carlo (MC) simulations across seven different parameter sets to examine how the estimator\nbehaves with varying values of \u03b1i, Hi, and \u03b7i,j. We focus on a bivariate model where \u03b1i and Hi vary jointly\nfor i = 1, 2. Following Bolko et al. (2023), we use the set of lags L = (0, 1, 2, 3, 4, 5, 20, 50), which yields\n31 moment conditions with 23 overidentifying restrictions. See Andersen and S\u00f8rensen (1996) for a discussion\non the choice of L.\nTable 1 presents the results. Each panel, labeled A through G, reports the true parameter values (True),\nthe average of point estimates across MC samples (Avg), the MC sample standard deviation (Std Err), and\nthe bias (Bias = Avg - True).\nPanel A represents the baseline scenario, reflecting estimates obtained from the logarithm of realized volatility\ntime series for the pair FCHI - FTSE in the empirical analysis of Section 5. Here, we find reliable estimates\nfor all parameters except for \u03b11 and \u03b12, which display slight upward biases - consistently with prior findings\nin the literature (Wang et al. 2023). Standard errors remain low and the biases are null for the remaining\nparameters. When \u03b1i, i = 1, 2 decrease (Panel B and C), their upward biases grow. These biases, which do\nnot affect other parameters\u2019 biases or standard errors, reflect biases in the covariances, which appear when\n\u03b1i, i = 1, 2 shrink. These are transferred to the estimates of \u03b1i, i = 1, 2 but not to the other parameters.\nThe MDE estimator shows lower biases in \u03b1i, i = 1, 2 than the 2-step one used for the initial conditions.\nLet us now focus on varying values of Hi, i = 1, 2 in Panel D and E. When Hi = 0.45, i = 1, 2, we observe\nslightly higher standard errors and biases in the parameter estimates, due to the vicinity to the region\nHi,j = 1, where the cross-covariance function, \u03b3k\ni,j (\u03b8), is discontinuous. For Hi = 0.7, i = 1, 2, standard\nerrors and biases also increase for some parameters but less than in the previous case, now due to the fact\nthat we are getting closer to the region Hi > 3/4, where the sample average estimator of the cross-covariance\ndoes not admit a Gaussian central limit theorem.\nFinally, in Panels F and G, we increase \u03b71,2 to 0.1 and 0.2, respectively, without observing notable deviations\nfrom the baseline behavior. More extreme values for \u03b71,2 are not allowed by the coherency constraint (cf.\nSection 2).\nIn general, the MDE estimator shows lower standard errors compared to the initial 2-step estimator, especially\nfor \u03b1i, i = 1, 2, \u03c11,2, and \u03b71,2.\nTable 1: Finite sample performance of the MDE estimator, \u02c6\u03b8, on simulated mfOU processes. Baseline scenario\n(True) corresponding to empirical estimates for the pair FCHI - FTSE from Section 5. Point estimates\n(Avg), standard errors (Std Err), and biases (Bias) are computed as MC averages. Simulation parameters:\nN = 2, M = 104, \u2206= 1/252, T = 20, \u00b5i = 0, i = 1, 2.\n\u03b11\n\u03b12\n\u03bd1\n\u03bd2\nH1\nH2\n\u03c11,2\n\u03b71,2\nPanel A\nTrue\n1.32\n1.45\n0.78\n0.79\n0.19\n0.21\n0.94\n0.00\nAvg\n1.48\n1.61\n0.79\n0.79\n0.20\n0.21\n0.94\n0.00\nStd Err\n0.56\n0.58\n0.05\n0.05\n0.02\n0.02\n0.01\n0.02\nBias\n0.16\n0.16\n0.00\n0.01\n0.00\n0.00\n0.00\n0.00\nPanel B\nTrue\n0.50\n0.50\n0.78\n0.79\n0.19\n0.21\n0.94\n0.00\nContinued on next page\n6\n\n\n--- Page 7 ---\n\u03b11\n\u03b12\n\u03bd1\n\u03bd2\nH1\nH2\n\u03c11,2\n\u03b71,2\nAvg\n0.68\n0.68\n0.79\n0.80\n0.20\n0.21\n0.94\n0.00\nStd Err\n0.38\n0.38\n0.05\n0.05\n0.02\n0.02\n0.01\n0.01\nBias\n0.18\n0.18\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nPanel C\nTrue\n0.05\n0.05\n0.78\n0.79\n0.19\n0.21\n0.94\n0.00\nAvg\n0.29\n0.29\n0.79\n0.80\n0.20\n0.21\n0.95\n0.00\nStd Err\n0.28\n0.27\n0.06\n0.06\n0.02\n0.02\n0.04\n0.03\nBias\n0.24\n0.24\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nPanel D\nTrue\n1.32\n1.45\n0.78\n0.79\n0.45\n0.45\n0.94\n0.00\nAvg\n1.70\n1.85\n0.83\n0.85\n0.47\n0.47\n0.94\n-0.01\nStd Err\n0.72\n0.76\n0.14\n0.14\n0.05\n0.05\n0.02\n0.43\nBias\n0.39\n0.41\n0.05\n0.05\n0.02\n0.02\n0.00\n-0.01\nPanel E\nTrue\n1.32\n1.45\n0.78\n0.79\n0.70\n0.70\n0.94\n0.00\nAvg\n1.73\n1.73\n0.79\n0.80\n0.69\n0.69\n0.94\n0.01\nStd Err\n0.55\n0.57\n0.09\n0.09\n0.05\n0.05\n0.03\n0.11\nBias\n0.41\n0.42\n0.01\n0.01\n-0.01\n-0.01\n0.00\n0.01\nPanel F\nTrue\n1.32\n1.45\n0.78\n0.79\n0.19\n0.21\n0.94\n0.10\nAvg\n1.49\n1.61\n0.79\n0.80\n0.20\n0.21\n0.94\n0.10\nStd Err\n0.18\n0.16\n0.01\n0.01\n0.00\n0.00\n0.00\n0.00\nBias\n0.18\n0.16\n0.01\n0.01\n0.00\n0.00\n0.00\n0.00\nPanel G\nTrue\n1.32\n1.45\n0.78\n0.79\n0.19\n0.21\n0.94\n0.20\nAvg\n1.49\n1.62\n0.79\n0.80\n0.20\n0.21\n0.94\n0.20\nStd Err\n0.58\n0.59\n0.06\n0.05\n0.02\n0.02\n0.01\n0.05\nBias\n0.18\n0.17\n0.01\n0.01\n0.00\n0.00\n0.00\n0.00\nAnother perspective on the results of the MC analysis is presented in Figure 1, which shows the estimation\nerror densities. The figure focuses on one parameter at a time, comparing the Nadaraya-Watson density of\nthe standardized estimation errors for each experimental setting (dashed lines) with the standard Gaussian\ndistribution (shaded area). The colors of the dashed lines correspond to the different panels in Table 1.\nFrom the results, we can conclude that a trajectory length of n = 20 \u00d7 252 = 5040 is generally sufficient\nto approximate the normal distribution predicted by the asymptotic theory (Proposition 3). However,\nnotable exceptions are observed in the distributions of the errors for \u03b1i, i = 1, 2, which exhibit skewness,\nespecially for lower parameter values. Under the same circumstances, a slight skewness is also present in the\nerror distribution for \u03c11,2. Under the specifications in Panels D and G, where the baseline is modified to\nHi = 0.45, i = 1, 2 and \u03b71,2 = 0.2, respectively, the error distribution for \u03b71,2 appears excessively peaked.\nThis is due to the discontinuity of \u03b3k\ni,j (\u03b8) at Hi + Hj = 1 in the first case, and to the fact that the coherency\nconstraint is satisfied by a very small margin in the second case.\n4.3\nSlow mean reversion\nMotivated by the asymptotic expression of the cross-covariance function for \u03b1 \u21920 given in Proposition 2,\nand the lower-quality results obtained for small \u03b1i, i = 1, 2 in simulation in Section 4.2, we also attempt\nminimum distance estimation using the small-alpha cross-covariance approximation in the moment conditions.\nWe include lag-0 variances and covariances (V1, V2, C1,2), which appear in the asymptotic formula, as\nparameters to be estimated in the optimization process. The results are presented in Table 2 and Figure 2.\nThe results for \u03bdi, Hi, i = 1, 2, \u03c11,2, and \u03b71,2 are satisfactory. We only observe slight biases in most\nparameters (Panel A), which vanish as \u03b1i, i = 1, 2 decrease (Panels B and C). The standard errors are very\nsimilar to those obtained using the exact cross-covariance function (3) in Table 1.\n7\n\n\n--- Page 8 ---\nFigure 1: Kernel estimates of the densities of the elements in (\u02c6\u03b8 \u2212\u03b80)/d\ns.e.(\u02c6\u03b8), where d\ns.e.(\u02c6\u03b8) is the MC\nstandard error of the MDE estimator \u02c6\u03b8.\nParameter settings are in Table 1.\nSimulation parameters:\nN = 2, M = 104, \u2206= 1/252, T = 20, \u00b5i = 0, i = 1, 2. The bold letters emphasize the parameter being\nvaried within each panel.\n8\n\n\n--- Page 9 ---\nHowever, the same cannot be said for V1, V2, C1,2, which exhibit a bias that originates similarly to the one\npreviously associated with \u03b1i, i = 1, 2. Motivated by this finding, we estimate the cross-covariance using\na sample average estimator and observe similar evidence of biases across several lags when \u03b1i, i = 1, 2 are\nsmall. This finding may explain the nature of the bias in the estimator of the mean reversion coefficient in\nWang et al. (2023), which relies on the sample variance.\nTable 2: Finite sample performance of the MDE estimator based on asymptotic cross-covariance conditions\non simulated mfOU process. Baseline scenario (True) corresponding to empirical estimates for the pair FCHI\n- FTSE from Section 5. Point estimates (Avg), standard errors (Std Err), and biases (Bias) are computed as\nMC averages. Simulation parameters: N = 2, M = 104, \u2206= 1/252, T = 20, \u00b5i = 0, i = 1, 2.\n\u03bd1\n\u03bd2\nH1\nH2\n\u03c11,2\n\u03b71,2\nV1\nV2\nC1,2\nPanel A\nTrue\n0.78\n0.79\n0.19\n0.21\n0.94\n0.00\n0.24\n0.24\n0.23\nAvg\n0.76\n0.77\n0.19\n0.20\n0.94\n-0.01\n0.24\n0.24\n0.23\nStd Err\n0.05\n0.04\n0.01\n0.01\n0.01\n0.02\n0.03\n0.03\n0.03\nBias\n-0.02\n-0.02\n-0.01\n-0.01\n0.00\n-0.01\n0.00\n0.00\n0.00\nPanel B\nTrue\n0.78\n0.79\n0.19\n0.21\n0.94\n0.00\n0.35\n0.37\n0.34\nAvg\n0.78\n0.79\n0.19\n0.21\n0.94\n0.00\n0.34\n0.36\n0.33\nStd Err\n0.04\n0.05\n0.01\n0.02\n0.01\n0.02\n0.07\n0.08\n0.07\nBias\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n-0.01\n-0.02\n-0.01\nPanel C\nTrue\n0.78\n0.79\n0.19\n0.21\n0.94\n0.00\n0.87\n0.97\n0.87\nAvg\n0.78\n0.80\n0.19\n0.21\n0.94\n0.00\n0.57\n0.61\n0.56\nStd Err\n0.05\n0.06\n0.02\n0.02\n0.01\n0.03\n0.24\n0.27\n0.25\nBias\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n-0.30\n-0.36\n-0.31\nSimilar conclusions can be drawn from the estimation error densities shown in Figure 2. They are\nslightly skewed in the baseline scenario (darkest line) but quickly become centered and resemble a Gaussian\ndistribution as the mean reversion coefficients decrease. In contrast, the variances (V1, V2) and the covariance\n(C1,2) grow increasingly skewed and non-normal.\n5\nEmpirical analysis\nOur empirical study utilizes realized volatility time series based on 5-minute price increments from the\nRealized Library of the Oxford-Man Institute.1 The dataset we use spans over 20 years of daily observations,\nstarting from January 3, 2000, to June 28, 2022. It includes realized volatilities for 31 indices associated\nwith stock exchanges worldwide. We retain only the series covering the entire sample period and discard the\nones that have sequences of missing values at the beginning, middle, or end of the sample. The final sample\nis composed of 22 time series, each averaging 5616 observations. We treat this collection as a multivariate\nsystem after removing observations with a realized volatility equal to zero, scaling to annual percentage\npoints, and applying the logarithm transformation. A detailed description of the dataset can be found in\nAppendix B.\n5.1\nEstimates\nTable 3, 4, and 5 present the estimates obtained with the procedure described in Section 3 on the Oxford-\nMan dataset. We employ the MDE estimation procedure with the same lags as for the MC study, i.e.\nL = (0, 1, 2, 3, 4, 5, 20, 50), and \u2206= 1/252. Since L = #L = 8 and N = 22, we now have 528 parameters\nand 3641 moment conditions.\n1https://oxford-man.ox.ac.uk/research/realized-library (no longer accessible)\n9\n\n\n--- Page 10 ---\nFigure 2: kernel estimates of the densities of the elements in (\u02c6\u03b8\u2212\u03b80)/d\ns.e.(\u02c6\u03b8), where \u02c6\u03b8 denotes the MDE estimator\nthat uses asymptotic cross-covariance conditions, and d\ns.e.(\u02c6\u03b8) is the MC standard error of \u02c6\u03b8. Parameter\nsettings are in Table 2. Simulation parameters: N = 2, M = 104, \u2206= 1/252, T = 20, \u00b5i = 0, i = 1, 2. The\nbold letters emphasize the parameter being varied within each panel.\n10\n\n\n--- Page 11 ---\nTable 3: Estimates of the univariate marginal parameters on log\n\u0000100\n\u221a\nRV \u00d7 252\n\u0001\n, where RV is the realized\nvariance from Oxford-Man (rv5). The MDE estimates are shown above and the initial values from the 2-step\nprocedure are shown below among parentheses. The MDE procedure uses L = {0, 1, 2, 3, 4, 5, 20, 50}\nand \u2206= 1/252. The 2-step procedure corresponds to the estimator in Wang et al. (2023). Symbol is based\non the provider convention.\nSymbol\n\u00b5\n\u03b1\nH\n\u03bd\nS5E\n2.679\n1.355\n0.157\n0.833\n(0.698)\n(0.119)\n(0.748)\nSSMI\n2.395\n1.837\n0.244\n0.785\n(0.625)\n(0.162)\n(0.621)\nIBEX\n2.698\n1.736\n0.207\n0.808\n(1.294)\n(0.171)\n(0.737)\nGDAXI\n2.680\n1.471\n0.213\n0.856\n(0.230)\n(0.130)\n(0.640)\nFTSE\n2.538\n1.446\n0.208\n0.795\n(0.053)\n(0.086)\n(0.574)\nFCHI\n2.627\n1.316\n0.194\n0.781\n(0.478)\n(0.143)\n(0.664)\nBFX\n2.463\n1.739\n0.223\n0.778\n(0.417)\n(0.138)\n(0.615)\nAEX\n2.529\n1.332\n0.194\n0.806\n(0.602)\n(0.151)\n(0.702)\nSSEC\n2.669\n2.280\n0.357\n0.989\n(2.469)\n(0.196)\n(0.941)\nNSEI\n2.532\n0.136\n0.118\n0.595\n(0.942)\n(0.144)\n(0.768)\nN225\n2.517\n1.768\n0.206\n0.767\n(1.028)\n(0.145)\n(0.698)\nSymbol\n\u00b5\n\u03b1\nH\n\u03bd\nKSE\n2.468\n4.263\n0.258\n1.045\n(6.190)\n(0.189)\n(1.093)\nKS11\n2.540\n2.004\n0.393\n0.913\n(0.015)\n(0.104)\n(0.486)\nHSI\n2.536\n1.852\n0.259\n0.694\n(0.000)\n(0.041)\n(0.371)\nBSESN\n2.640\n1.409\n0.188\n0.763\n(2.504)\n(0.188)\n(0.872)\nAORD\n2.143\n2.523\n0.283\n0.861\n(0.000)\n(0.026)\n(0.446)\nSPX\n2.425\n1.224\n0.185\n0.898\n(0.199)\n(0.127)\n(0.694)\nRUT\n2.351\n1.559\n0.152\n0.833\n(0.886)\n(0.127)\n(0.738)\nMXX\n2.403\n2.413\n0.164\n0.758\n(0.006)\n(0.052)\n(0.504)\nIXIC\n2.537\n1.162\n0.171\n0.860\n(2.025)\n(0.206)\n(0.953)\nDJI\n2.442\n1.293\n0.183\n0.867\n(0.014)\n(0.083)\n(0.573)\nBVSP\n2.756\n2.654\n0.164\n0.729\n(6.245)\n(0.188)\n(0.877)\nTable 3 presents the parameters related to the univariate marginal components: the MDE estimates as\nmain figures and the 2-step estimates, which are the starting values in the numerical optimization, provided\nin parentheses below. The average volatility ranges from 2.143 for AORD to 2.756 for BVSP, with all of\nthem being around 2.5. The mean reversion parameters, \u03b1i, are mostly estimated around 1.6, with some\nexceptions, most notably NSEI (0.136) and KSE (4.263). The effect of the MDE estimator over the starting\nvalues is mainly to increase the estimates, although it sometimes also mitigates very large initial values. The\nestimates for the Hurst exponent Hi, which rules the regularity of the trajectories and the memory of the\nprocess, consistently fall below 1/2. This suggests that the trajectories are rough, with the process exhibiting\nabsence of long memory at the univariate level and short-range interdependence at the multivariate level\n(cf. Section 2). The MDE routine mostly reduces roughness by increasing the initial values for Hi, obtained\nwith the method in Wang et al. (2023) and Lang and Roueff (2001). Additionally, the values for \u03bdi mostly\nincrease slightly due to the MDE routine, and a positive empirical correlation of 0.5 is observed between\nthe estimates for Hi and \u03bdi. Table 4 and 5 present the estimates of the matrices \u03c1 and \u03b7. Starting values\ngiven by the 2-step procedure are provided below among parenthesis. Table 4 exhibits the results for \u03c1, the\ncorrelation coefficient of the underlying mfBm (\u03c1i,j = \u03c1j,i). The primary finding is that most values in \u03c1 are\nhigh and positive, with fewer than half of the pairs showing values below 0.5. The strongest correlations are\nobserved between the log-volatilities of S5E and FCHI (0.996), as well as SPX and DJI (0.990). Several pairs\nwithin Europe or North America display \u03c1i,j > 0.9, indicating that their volatilities tend to fluctuate together\nclosely. In contrast, volatilities in Asia and Oceania (columns between SSEC and AORD) exhibit more\n11\n\n\n--- Page 12 ---\nmoderate \u03c1i,j values, with only one pair, NSEI and BSESN, exceeding 0.8 (0.893). The lowest correlations\nare observed between the volatilities for KSE and several others, or less notably between SSEC and others.\nSimilar conclusions emerge from the graphical representation of the estimates of \u03c1 in Figure 3. The figure\ndisplays an undirected graph, where nodes correspond to indices, and edges are inversely proportional to the\nestimates of \u03c1i,j. This visualization shows dense clustering among the volatilities of North American and\nEuropean indices, in contrast to the more dispersed patterns observed among Asian indices.\nIn Table 5 the estimated values of \u03b7 are presented. Recall that \u03b7i,j = \u2212\u03b7j,i. Together with \u03b1i and \u03b1j,\n\u03b7i,j determines the degree of asymmetry in the cross-covariance function, which is directly related to the\ntime reversibility of the process. More specifically, when Hi,j < 1 (short-range interdependence) both the\nconditions \u03b1i > \u03b1j and \u03b7i,j > 0 result in a faster decrease of \u03b3i,j(k) compared to \u03b3j,i(k) as k increases. The\nvalues in |\u03b7| range from 4 \u00d7 10\u22124, for the pair SPX and KSE to 0.285 for KS11 and N225. Two-thirds of\nthe 231 pairwise relationships are characterized by |\u03b7i,j| < 0.05, and 90% of them by |\u03b7i,j| < 0.1. High\nabsolute values in \u03b7 are estimated for pairs in which the Asian indices SSEC, N225, or AORD are involved.\nHowever, HSI, despite being in Asia, is characterized by small absolute \u03b7i,js. Among the lowest absolute\nvalues in \u03b7, we observe 5 \u00d7 10\u22124 for GDAXI and BSESN and 0.002 between FCHI and FTSE. These pairs\nare also characterized by similar \u03b1i and \u03b1j in Table 3, suggesting symmetric cross-covariances. Generally,\ninterconnected exchanges, such as European and North American ones, show smaller absolute values in \u03b7.\nOverall, the optimization significantly increased the absolute values of the initial estimates obtained with the\n2-step procedure, for both \u03b7 and \u03c1.\nThe coherency constraint ensuring the positive semidefiniteness of the covariance matrix (cf. Section 2) was\nnot directly included in the optimization routine. However, it was satisfied for 230 out of 231 pairs, with the\nexception of the pair FCHI-S5E, where the constraint was slightly exceeded, c(Hi, Hj, \u03c1i,j, \u03b7i,j) = 1.04 > 1.\n12\n\n\n--- Page 13 ---\nS5E\nSSMI\nIBEX\nGDAXI\nFTSE\nFCHI\nBFX\nAEX\nSSEC\nNSEI\nN225\nKSE\nKS11\nHSI\nBSESN\nAORD\nSPX\nRUT\nMXX\nIXIC\nDJI\nBVSP\nS5E\n1\nSSMI\n0.923\n1\n(0.508)\nIBEX\n0.868\n0.745\n1\n(0.579)\n(0.555)\nGDAXI\n0.972\n0.889\n0.760\n1\n(0.719)\n(0.627)\n(0.608)\nFTSE\n0.926\n0.927\n0.829\n0.875\n1\n(0.603)\n(0.420)\n(0.401)\n(0.480)\nFCHI\n0.996\n0.918\n0.882\n0.955\n0.942\n1\n(0.691)\n(0.642)\n(0.724)\n(0.716)\n(0.499)\nBFX\n0.920\n0.891\n0.859\n0.869\n0.937\n0.936\n1\n(0.565)\n(0.551)\n(0.608)\n(0.594)\n(0.425)\n(0.721)\nAEX\n0.978\n0.948\n0.804\n0.956\n0.941\n0.969\n0.934\n1\n(0.647)\n(0.632)\n(0.682)\n(0.688)\n(0.491)\n(0.843)\n(0.699)\nSSEC\n0.245\n0.339\n0.167\n0.209\n0.325\n0.252\n0.243\n0.263\n1\n(0.076)\n(0.045)\n(0.041)\n(0.058)\n(0.072)\n(0.071)\n(0.70)\n(0.078)\nNSEI\n0.434\n0.544\n0.242\n0.417\n0.498\n0.417\n0.439\n0.460\n0.455\n1\n(0.115)\n(0.103)\n(0.041)\n(0.58)\n(0.72)\n(0.71)\n(0.70)\n(0.78)\n(0.)\nN225\n0.654\n0.682\n0.507\n0.663\n0.648\n0.637\n0.544\n0.647\n0.183\n0.433\n1\n(0.173)\n(0.155)\n(0.135)\n(0.154)\n(0.168)\n(0.160)\n(0.141)\n(0.161)\n(0.164)\n(0.102)\nKSE\n0.019\n0.043\n-0.124\n0.070\n0.067\n0.024\n0.014\n0.037\n0.104\n0.227\n0.040\n1\n(0.000)\n(0.001)\n(-0.001)\n(-0.031)\n(0.044)\n(-0.025)\n(0.013)\n(-0.010)\n(-0.006)\n(-0.005)\n(-0.019)\nKS11\n0.514\n0.549\n0.282\n0.632\n0.578\n0.551\n0.493\n0.570\n0.170\n0.446\n0.571\n0.256\n1\n(0.095)\n(0.127)\n(0.109)\n(0.107)\n(0.083)\n(0.125)\n(0.118)\n(0.116)\n(0.117)\n(0.082)\n(0.256)\n(0.015)\nHSI\n0.580\n0.615\n0.485\n0.635\n0.680\n0.630\n0.618\n0.645\n0.355\n0.512\n0.560\n0.016\n0.680\n1\n(0.141)\n(0.121)\n(0.122)\n(0.133)\n(0.130)\n(0.156)\n(0.142)\n(0.169)\n(0.262)\n(0.122)\n(0.236)\n(0.037)\n(0.263)\nBSESN\n0.433\n0.464\n0.258\n0.505\n0.501\n0.467\n0.409\n0.472\n0.297\n0.894\n0.569\n0.299\n0.699\n0.636\n1\n(0.116)\n(0.116)\n(0.136)\n(0.110)\n(0.091)\n(0.133)\n(0.143)\n(0.154)\n(0.079)\n(0.797)\n(0.094)\n(0.015)\n(0.087)\n(0.154)\nAORD\n0.636\n0.692\n0.683\n0.538\n0.772\n0.686\n0.742\n0.671\n0.391\n0.394\n0.450\n-0.019\n0.284\n0.541\n0.331\n1\n(0.139)\n(0.096)\n(0.090)\n(0.105)\n(0.177)\n(0.096)\n(0.102)\n(0.101)\n(0.071)\n(0.058)\n(0.231)\n(0.028)\n(0.068)\n(0.203)\n(0.068)\nSPX\n0.875\n0.849\n0.714\n0.866\n0.889\n0.883\n0.835\n0.888\n0.269\n0.514\n0.689\n0.113\n0.661\n0.708\n0.593\n0.648\n1\n(0.353)\n(0.307)\n(0.316)\n(0.364)\n(0.342)\n(0.370)\n(0.314)\n(0.367)\n(0.025)\n(0.029)\n(0.081)\n(0.014)\n(0.086)\n(0.089)\n(0.039)\n(-0.012)\nRUT\n0.627\n0.633\n0.596\n0.487\n0.714\n0.627\n0.671\n0.631\n0.325\n0.585\n0.401\n0.009\n0.289\n0.521\n0.350\n0.743\n0.717\n1\n(0.194)\n(0.180)\n(0.193)\n(0.215)\n(0.207)\n(0.224)\n(0.195)\n(0.230)\n(0.034)\n(0.014)\n(0.050)\n(-0.001)\n(0.032)\n(0.066)\n(0.028)\n(0.002)\n(0.627)\nMXX\n0.517\n0.539\n0.517\n0.457\n0.689\n0.570\n0.590\n0.537\n0.279\n0.486\n0.445\n0.052\n0.408\n0.608\n0.474\n0.719\n0.648\n0.710\n1\n(0.274)\n(0.192)\n(0.195)\n(0.235)\n(0.269)\n(0.229)\n(0.180)\n(0.220)\n(0.012)\n(0.017)\n(0.073)\n(0.014)\n(0.064)\n(0.060)\n(0.011)\n(0.003)\n(0.427)\n(0.249)\nIXIC\n0.759\n0.718\n0.592\n0.802\n0.782\n0.779\n0.726\n0.791\n0.108\n0.352\n0.655\n0.160\n0.700\n0.724\n0.617\n0.511\n0.913\n0.573\n0.586\n1\n(0.316)\n(0.302)\n(0.308)\n(0.341)\n(0.257)\n(0.350)\n(0.288)\n(0.364)\n(0.026)\n(0.060)\n(0.003)\n(0.051)\n(0.047)\n(0.025)\n(0.057)\n(0.717)\n(0.663)\n(0.242)\n(0.)\nDJI\n0.873\n0.862\n0.700\n0.869\n0.892\n0.878\n0.835\n0.894\n0.280\n0.507\n0.688\n0.101\n0.670\n0.702\n0.573\n0.645\n0.989\n0.696\n0.629\n0.896\n1\n(0.339)\n(0.294)\n(0.298)\n(0.349)\n(0.339)\n(0.350)\n(0.300)\n(0.334)\n(0.019)\n(0.022)\n(0.077)\n(0.013)\n(0.087)\n(0.089)\n(0.026)\n(-0.033)\n(0.925)\n(0.564)\n(0.443)\n(0.592)\nBVSP\n0.639\n0.672\n0.466\n0.634\n0.685\n0.640\n0.591\n0.672\n0.311\n0.701\n0.628\n-0.046\n0.609\n0.628\n0.606\n0.490\n0.712\n0.578\n0.501\n0.633\n0.714\n1\n(0.188)\n(0.180)\n(0.168)\n(0.204)\n(0.144)\n(0.196)\n(0.154)\n(0.199)\n(0.020)\n(0.023)\n(0.039)\n(-0.014)\n(0.012)\n(0.012)\n(0.014)\n(0.012)\n(0.309)\n(0.272)\n(0.239)\n(0.350)\n(0.271)\nTable 4: Estimates of \u03c1i,j on log\n\u0000100\n\u221a\nRV \u00d7 252\n\u0001\n, where RV is the realized variance from Oxford-Man (rv5). The MDE estimates are shown above\nand the initial values from the 2-step procedure are shown below among parentheses. The MDE procedure uses L = {0, 1, 2, 3, 4, 5, 20, 50} and\n\u2206= 1/252. The 2-step procedure relies on the Low frequency estimator from Dugo et al. (2024) with time-lag equal to 1 and univariate parameters\nfrom Table 3. Symbol is based on the provider convention.\n13\n\n\n--- Page 14 ---\nS5E\nSSMI\nIBEX\nGDAXI\nFTSE\nFCHI\nBFX\nAEX\nSSEC\nNSEI\nN225\nKSE\nKS11\nHSI\nBSESN\nAORD\nSPX\nRUT\nMXX\nIXIC\nDJI\nBVSP\nS5E\n0\nSSMI\n-0.027\n0\n(0.021)\nIBEX\n0.008\n0.025\n0\n(0.007)\n(-0.022)\nGDAXI\n0.008\n0.044\n0.011\n0\n(0.017)\n(-0.016)\n(0.007)\nFTSE\n0.010\n0.055\n0.034\n0.003\n0\n(-0.006)\n(-0.017)\n(0.000)\n(-0.014)\nFCHI\n0.019\n0.050\n0.027\n0.009\n0.002\n0\n(0.015)\n(-0.022)\n(0.004)\n(0.000)\n(0.012)\nBFX\n-0.014\n-0.018\n-0.010\n-0.028\n-0.049\n-0.044\n0\n(0.010)\n(-0.008)\n(0.011)\n(0.005)\n(0.008)\n(0.003)\nAEX\n0.004\n0.030\n0.012\n-0.004\n-0.020\n-0.017\n0.027\n0\n(0.020)\n(-0.014)\n(0.006)\n(0.006)\n(0.016)\n(0.007)\n(0.003)\nSSEC\n0.073\n0.203\n-0.117\n-0.072\n-0.092\n-0.097\n-0.138\n-0.096\n0\n(-0.011)\n(-0.008)\n(0.004)\n(0.014)\n(0.011)\n(0.016)\n(0.007)\n(0.012)\nNSEI\n-0.027\n-0.003\n0.085\n0.063\n0.023\n0.031\n0.028\n0.019\n0.210\n0\n(-0.051)\n(-0.061)\n(0.041)\n(0.053)\n(0.037)\n(0.049)\n(0.043)\n(0.061)\n(-0.013)\nN225\n-0.090\n-0.041\n0.094\n0.104\n0.099\n0.098\n0.064\n0.081\n-0.088\n-0.074\n0\n(-0.074)\n(-0.083)\n(0.065)\n(0.064)\n(0.057)\n(0.073)\n(0.052)\n(0.074)\n(-0.023)\n(-0.004)\nKSE\n0.052\n0.028\n-0.117\n0.002\n-0.040\n-0.049\n-0.076\n-0.047\n0.123\n-0.087\n0.060\n0\n(0.030)\n(0.015)\n(-0.050)\n(-0.028)\n(0.007)\n(-0.041)\n(-0.027)\n(-0.037)\n(-0.011)\n(-0.004)\n(0.001)\nKS11\n0.001\n0.131\n-0.045\n0.023\n-0.031\n-0.016\n-0.039\n-0.031\n0.050\n-0.050\n0.285\n0.184\n0\n(-0.016)\n(-0.027)\n(0.023)\n(0.026)\n(0.009)\n(0.023)\n(0.023)\n(0.021)\n(-0.031)\n(0.009)\n(0.037)\n(0.007)\nHSI\n-0.012\n0.039\n-0.003\n0.027\n0.041\n0.020\n0.002\n-0.011\n0.084\n0.009\n0.115\n0.037\n-0.001\n0\n(-0.017)\n(-0.020)\n(-0.015)\n(0.016)\n(0.027)\n(0.025)\n(0.021)\n(0.025)\n(0.032)\n(0.021)\n(0.047)\n(0.004)\n(0.011)\nBSESN\n0.029\n0.024\n0.031\n-0.001\n0.006\n0.028\n-0.022\n-0.024\n0.032\n-0.050\n0.150\n0.093\n0.095\n0.018\n0\n(-0.058)\n(-0.071)\n(-0.051)\n(-0.061)\n(-0.047)\n(-0.060)\n(0.054)\n(0.075)\n(-0.006)\n(-0.013)\n(0.002)\n(0.009)\n(-0.012)\n(-0.024)\nAORD\n-0.079\n-0.080\n-0.074\n-0.088\n-0.138\n-0.112\n-0.090\n0.093\n-0.208\n-0.093\n0.006\n-0.049\n-0.160\n-0.101\n-0.104\n0\n(-0.074)\n(-0.064)\n(-0.042)\n(-0.066)\n(-0.075)\n(-0.052)\n(-0.037)\n(0.061)\n(-0.019)\n(-0.020)\n(-0.004)\n(-0.024)\n(-0.027)\n(-0.132)\n(-0.021)\nSPX\n0.015\n0.050\n-0.030\n-0.009\n-0.027\n-0.002\n-0.045\n-0.015\n-0.093\n0.002\n-0.116\n0.000\n- 0.031\n-0.034\n0.023\n-0.134\n0\n(0.094)\n(0.067)\n(-0.086)\n(-0.066)\n(-0.102)\n(-0.075)\n(-0.086)\n(-0.087)\n(0.007)\n(-0.084)\n(-0.124)\n(-0.004)\n(-0.038)\n(-0.054)\n(-0.097)\n(-0.124)\nRUT\n-0.017\n-0.040\n0.033\n0.039\n0.029\n0.035\n0.027\n0.027\n-0.132\n0.067\n-0.003\n0.077\n0.093\n0.012\n0.042\n-0.092\n-0.035\n0\n(0.099)\n(0.065)\n(-0.071)\n(-0.068)\n(-0.098)\n(-0.076)\n(-0.086)\n(-0.091)\n(0.006)\n(-0.079)\n(-0.095)\n(-0.019)\n(-0.061)\n(-0.057)\n(-0.091)\n(-0.105)\n(0.032)\nMXX\n0.020\n0.037\n-0.031\n-0.017\n-0.008\n-0.017\n-0.016\n-0.024\n-0.098\n-0.046\n0.094\n-0.004\n0.081\n-0.026\n0.059\n-0.099\n-0.005\n0.018\n0\n(0.033)\n(0.038)\n(-0.040)\n(-0.029)\n(-0.031)\n(-0.034)\n(-0.036)\n(-0.040)\n(-0.002)\n(0.063)\n(0.052)\n(0.013)\n(-0.024)\n(-0.030)\n(-0.061)\n(-0.089)\n(-0.035)\n(-0.046)\nIXIC\n0.035\n0.058\n-0.038\n-0.025\n-0.030\n-0.018\n-0.040\n-0.031\n-0.147\n0.048\n0.111\n-0.001\n0.049\n-0.030\n0.016\n-0.105\n0.026\n0.042\n-0.003\n0\n(0.110)\n(0.086)\n(-0.090)\n(-0.081)\n(-0.114)\n(-0.091)\n(-0.095)\n(-0.105)\n(0.018)\n(0.105)\n(0.129)\n(0.008)\n(0.060)\n(-0.051)\n(-0.124)\n(-0.099)\n(0.020)\n(-0.013)\n(0.034)\nDJI\n0.004\n0.039\n0.017\n-0.004\n0.014\n-0.008\n-0.033\n-0.004\n-0.082\n-0.005\n0.110\n-0.010\n0.021\n0.030\n0.028\n-0.122\n-0.012\n0.032\n-0.004\n-0.035\n0\n(0.075)\n(0.049)\n(0.072)\n(0.048)\n(0.084)\n(0.056)\n(-0.070)\n(-0.068)\n(0.011)\n(0.077)\n(0.102)\n(0.003)\n(0.024)\n(0.049)\n(-0.085)\n(-0.126)\n(-0.012)\n(-0.035)\n(0.033)\n(-0.031)\nBVSP\n-0.038\n-0.049\n-0.022\n-0.079\n-0.052\n-0.054\n0.032\n0.038\n0.055\n-0.076\n0.074\n0.064\n-0.016\n-0.046\n0.050\n-0.070\n-0.027\n0.037\n-0.025\n-0.017\n-0.020\n0\n(0.036)\n(0.005)\n(0.030)\n(0.016)\n(0.024)\n(0.021)\n(-0.029)\n(-0.023)\n(-0.002)\n(0.040)\n(0.043)\n(-0.031)\n(-0.002)\n(0.001)\n(-0.047)\n(0.031)\n(-0.053)\n(-0.054)\n(-0.019)\n(-0.070)\n(-0.045)\nTable 5: Estimates of \u03b7i,j on log\n\u0000100\n\u221a\nRV \u00d7 252\n\u0001\n, where RV is the realized variance from Oxford-Man (rv5). The MDE estimates are shown above\nand the initial values from the 2-step procedure are shown below among parentheses. The MDE procedure uses L = {0, 1, 2, 3, 4, 5, 20, 50} and\n\u2206= 1/252. The 2-step procedure relies on the Low frequency estimator from Dugo et al. (2024) with time-lag equal to 1 and univariate parameters\nfrom Table 3. Symbol is based on the provider convention.\n14\n\n\n--- Page 15 ---\nFigure 3: Undirected graph representation of the estimates of \u03c1i,j, i, j = 1, . . . , N, i \u0338= j. The nodes\ncorrespond to indices and the edges among pairs of indices have length inversely proportional to the MDE\nestimates of \u03c1i,j reported in Table 4.\nTo demonstrate the goodness of fit of our model and illustrate the degrees of asymmetry in the empirical\ncross-covariances of log-volatility, in Figure 4 we present plots for the pairs FCHI-FTSE and KS11-N225, two\nextreme cases, identified based on the values of |\u03b7i,j|: when the value of the former is among the lowest ones\nand when it is the highest. Note that the asymmetry depends also on \u03b1i and \u03b1j.\nIn the FCHI - FTSE case, we have \u03b7i,j = 0.002, \u03b1i = 1.316, and \u03b1j = 1.446. The strong symmetry\nsuggested by the close values of \u03b1i and \u03b1j and the small \u03b7i,j is supported empirically. In the KS11 - N225\ncase, where \u03b7i,j = 0.285, \u03b1i = 2.004, and \u03b1j = 1.768, we expect and indeed observe asymmetry. In this\ninstance, since Hi,j < 1, both \u03b7i,j > 0 and \u03b1i > \u03b1j accelerate the right-hand side of the cross-covariance\ntowards zero.2 In the former case, \u03b7i,j > 0 and \u03b1i < \u03b1j compensate each other.\nOur model fits well both these different behaviors. Moreover, as shown in Appendix C, it effectively fits\nthe autocovariance of the marginal components. The fit is also good for auto and cross-covariances of all the\n22 components in the system, which overall exhibit characteristics closer to the symmetric case (see Online\nAppendix for further empirical evidence).\n5.2\nSlow mean reversion\nFollowing an approach similar to Gatheral et al. (2018), we examine the empirical cross-covariance against\nkHi+Hj, where Hi and Hj are proxied by the estimated values in Table 3. According to the cross-covariance\nfunction of the mfOU process in the small \u03b1 (or small k) regime, this relationship should be approximately\nlinear. Figure 5 shows that this linear behavior holds for lags up to 50, particularly well in the symmtric case\nof FCHI-FTSE. Evidence for autocovariances is provided in Appendix C, with similar findings observed across\n2The opposite holds when \u03b7i,j < 0 and/or \u03b1i < \u03b1j\n15\n\n\n--- Page 16 ---\nFigure 4: Empirical cross-covariances of log-volatilities as bars, alongside the theoretical cross-covariances\nfrom our model, indicated by the red curves and based on the estimated parameters from Tables 3, 4, and 5.\nThe upper panel shows the pair FCHI-FTSE, while the lower panel shows the pair KS11-N225.\n16\n\n\n--- Page 17 ---\nthe entire system available in the Online Appendix. Motivated by these findings, we attempt estimation of the\nFigure 5: Empirical cross-covariances of log-volatilities as bars, plotted against a suitable power of the lag\n(given by the sum of the Hurst exponents), alongside the best linear fits, indicated by the red lines. The\nupper panel shows the pair FCHI-FTSE, while the lower panel shows the pair KS11-N225.\nsystem using the asymptotic cross-covariance function given in Proposition 2 in the moment conditions of the\nMDE estimator. The resulting fit is strong, as demonstrated in Figure 6. Additional evidence on univariate\nmarginals is provided in Appendix C, with findings on the remaining series available in the Online Appendix.\nParameter variations were minor for the FCHI-FTSE pair but more significant for N225-KS11. Overall,\ncross-covariances between the two estimation procedures are consistent, a direct result of the MDE design,\nthough the estimated parameters can vary (cf. Online Appendix). This can be interpreted as additional\nevidence for the \u201dnear non-stationary regime\u201d of realized volatility found by Gatheral et al. (2018).\n6\nSpillovers\nPrompted by the asymmetries in the cross-covariances, we study the effects of the lead-lag relationship in the\nframework of spillovers (Diebold and Yilmaz 2009; Diebold and Yilmaz 2012). Spillovers are defined in a\nmultivariate system of time series as the shares of the forecast error variance in one variable due to the effect\nof the innovations in another variable. Let \u03c8i,j(h), be the share of the variance in the error of predicting\nY i\nt+h due to innovations in the variable\n\u0000Y j\ns\n\u0001\ns\u2208[t,t+h], for i, j = 1, . . . , N. In Pesaran (1997) and Pesaran and\nShin (1998), in the discrete time setting, this is defined as\n\u03c8i,j(h) =\nE\nh\u0000Y i\nt+h \u2212E\n\u0002\nY i\nt+h|Ft\u22121\n\u0003\u00012i\n\u2212E\n\"\u0012\nY i\nt+h \u2212E\n\u0014\nY i\nt+h|Ft\u22121,\n\u0010\n\u03b5j\nt+k\n\u0011h\nk=0\n\u0015\u00132#\nE\nh\u0000Y i\nt+h \u2212E\n\u0002\nY i\nt+h|Ft\u22121\n\u0003\u00012i\n,\n17\n\n\n--- Page 18 ---\nFigure 6: Empirical cross-covariances of log-volatilities as bars, alongside the approximate theoretical cross-\ncovariances from our model for \u03b1 \u21920, indicated by the red curves and based on the estimated parameters\n(available in the Online Appendix). The upper panel shows the pair FCHI-FTSE, while the lower panel shows\nthe pair KS11-N225.\n18\n\n\n--- Page 19 ---\nwhere \u03b5t is the N-variate white noise innovation in Yt at time t, \u03b5j\nt is its j-th component, and Ft\u22121 is the\ninformation set at time t \u22121, i.e. the filtration generated by the underlying \u03b5t\u2212i, i = 1, . . .. In order to\nuse the information in the variance decomposition matrix \u03a8, with entries [\u03a8(h)]i,j = \u03c8i,j(h), to construct\nspillover indices in the presence of correlated white noises, Diebold and Yilmaz (2012) consider the normalized\nquantities\ne\u03c8i,j(h) =\n\u03c8i,j(h)\nPN\nj=1 \u03c8i,j(h)\n.\n(5)\nThe mfOU process admits an integral moving average representation where the innovations are white noise\n(cf. Dugo et al. 2024). In order to give a meaningful intepretation to time, and spillovers, we need to restrict\nour attention to the causal version of the mfOU process, that is, when the process depends only on past\nvalues of the driving white noise (see Amblard et al. 2010 for details). In this setting, we lose one degree of\nfreedom in the choice of the parameters, which we can see as fixing \u03b7i,j = f(Hi, Hj, \u03c1i,j) for a function f\ngiven in Amblard et al. (2010). In this setting, \u03b7i,j = 0 \u21d0\u21d2Hi = Hj.\nProposition 4. In the adapted time discretization of the causal mfOU process (see (15)), assuming t > h,\nwe have\ne\u03c8i,j(h) =\nG2\ni,j/\np\nGj,j\nPN\nk=1 G2\ni,k/\np\nGk,k\n,\nwhere\nGi,j =\ns\nB(Hi + 1\n2, Hi + 1\n2)B(Hj + 1\n2, Hj + 1\n2)\nsin (\u03c0Hi) sin (\u03c0Hj)\n1\nB(Hi + 1\n2, Hj + 1\n2)\nsin (\u03c0(Hi + Hj))\n(cos (\u03c0Hi) + cos (\u03c0Hj))\u03c1i,j,\nand B(x, y), x, y > 0, denotes the Beta function.\nThe proof is given in Appendix E.\nRemark 2. In the adapted discretization of the moving average representation of the causal mfOU process,\nthe forecast error variance shares, e\u03c8i,j, and the resulting spillover indices are independent of the forecasting\nhorizon, h, the history of the process, Ft\u22121, and the discretization step.\nWe proceed to estimate the causal model following the same methodology detailed in Section 3 with the\nadditional constraint \u03b7i,j = f (Hi, Hj, \u03c1i,j), calculate e\u03c8i,j with the estimated parameters, and construct the\nspillover indices defined in Table 6.3 We obtain the following results over the whole period from January\n2000 to June 2022.\n\u2022 Total spillovers amount to 85% of the normalized forecast error variance decomposition of log-volatilities.\n\u2022 Directional spillovers are illustrated in Figure 7. The top panel shows that the amount of received\nspillovers is quite uniform among European and North American volatilities, with greater differences\namong other components. Their level of variability is a direct consequence of the normalization step in\n(5). Greater variability is preserved in the spillovers transmitted to others, in the middle panel, resulting\nin clearly different net quantities. In fact, as shown in the bottom panel, European and North American\nindices generally transmit at least as much volatility spillover as they receive, with FTSE, DJI, and\nSPX playing major roles, while Asian indices consistently exhibit negative net volatility spillovers. An\nexception in Europe is IBEX, which shows negative net volatility spillovers. The KSE index appears to\nbe the most isolated in our sample in terms of volatility, with relatively low levels of both transmitted\nand received spillovers.\n\u2022 Net pairwise spillovers, presented in Figure 8, offer a more detailed perspective. Although this measure\ncannot be interpreted in terms of shares of the normalized forecast error variance, net pairwise spillovers\nprovide valuable qualitative insights. Notably, FTSE is the only index with all positive net pairwise\nvolatility spillovers, followed by SPX, which only exhibits negative net volatility spillovers against\nFTSE. Other primary net transmitters include DJI and AEX. The main net receivers appear to be\n3Estimates obtained in the causal setting are available in the Online Appendix.\n19\n\n\n--- Page 20 ---\nTable 6: Definitions of Spillover Indices from Diebold and Yilmaz (2012), where e\u03c8i,j(h) is defined in (5).\nSpillover Index\nDescription\nDefinition\nTotal\nAggregation over all components in the\nsystem of the normalized forecast error\nvariances due to shocks to other compo-\nnents.\nS(h) =\nPN\ni,j=1,i\u0338=j e\n\u03c8ij(h)\nN\n\u00b7 100\nReceived\nShare of total spillovers received by a\nparticular component of the system from\nthe other components.\nSi,\u00b7(h) =\nPN\nj=1,j\u0338=i e\n\u03c8ij(h)\nN\n\u00b7 100\nTransmitted\nShare of total spillovers transmitted by\na particular component of the system to\nthe other components.\nS\u00b7,i(h) =\nPN\nj=1,j\u0338=i e\n\u03c8ji(h)\nN\n\u00b7 100\nNet\nDifference between the spillovers trans-\nmitted to and those received from all\nother components of the system.\nSi(h) = S\u00b7,i(h) \u2212Si,\u00b7(h)\nNet Pairwise\nDifference between the spillovers trans-\nmitted from component i to component\nj and those transmitted from j to i.\nSi,j(h) =\n\u0012\ne\n\u03c8ji(h)\u2212e\n\u03c8ij(h)\nN\n\u0013\n\u00b7 100\nSSEC and KSE, with the latter being relatively neutral and primarily influenced by the geographically\nneighbouring BSESN and NSEI indices.\nThe presence of substantial spillovers suggests that forecasting volatility time series as a multivariate\nsystem could provide benefits. In the context of the mfOU process, this task could be easily achieved using\nrules for conditional expectations of Gaussian vectors (as, for example, in Bennedsen et al. 2021).\n7\nConclusion\nMotivated by recent advances in volatility modeling, we introduce a multivariate model for rough volatility.\nOur objective is to extend the widely-used RFSV model of Gatheral et al. (2018) to a multivariate framework\nand empirically study a system of historical volatilities. This extension retains consistency with the findings\nin the univariate case, namely that volatility is rough and mean reverts slowly, while broadening our\nunderstanding of volatility dynamics through the analysis of cross-covariances and spillover effects, which are\ndescriptive of how volatility evolves across markets. These results can have implications in risk management\nand forecasting techniques in finance.\nThe results in this paper could be further developed considering the distinction between spot and realized\nvolatility, for example leveraging the moment structure of integrated variance (as in Bolko et al. 2023) or\ndirectly estimating spot volatility (Jacod 2000). Another potential direction is to extend the empirical study\nwith rolling window parameter estimation, which could then be used for dynamic forecasting and spillover\nanalyses.\nIn conclusion, by introducing the multivariate fractional Ornstein-Uhlenbeck process, we confirm the\nincreasing evidence that rough processes effectively model volatility dynamics and extend our understanding\nof these dynamics to a multidimensional context, with potential for further applications.\n20\n\n\n--- Page 21 ---\nFigure 7: Estimates of directional volatility spillovers over the whole period (2000-2022) for each index. The\nfigures in the first and second rows represent aggregated shares of the forecast error variance decomposition.\nSee definitions in Table 6.\n21\n\n\n--- Page 22 ---\nFigure 8: Estimates of net pairwise volatility spillovers over the whole period (2000-2022) for each combination\nof indices i,j. It is the difference between the volatility shocks transmitted from index i to index j and those\ntransmitted from j to i. See definition in Table 6.\n22\n\n\n--- Page 23 ---\nA\nAsymptotic theory for the MDE estimator\nProof of Proposition 3 - (I): By definition of the MDE estimator, \u02c6\u03b8 satisfies the first order condition\n\u2207T (\u03b8) |\u03b8=\u02c6\u03b8 = 0,\nwhere \u2207T (\u03b8) denotes the gradient of T (\u03b8).Using a first-order Taylor expansion around \u03b80, the true value of\nthe parameter, with Lagrange form of the remainder, we have\n\u2207T (\u03b8)\u03b8=\u02c6\u03b8 = \u2207T (\u03b8) |\u03b8=\u03b80 + \u22072T (\u03b8) |\u03b8=\u03b8\u22c6\n\u0010\n\u02c6\u03b8 \u2212\u03b80\n\u0011\n= 0,\nwhere \u03b8\u22c6lies between \u03b80 and \u02c6\u03b8, and \u22072T (\u03b8) indicates the Hessian of T (\u03b8). Solving for \u02c6\u03b8 \u2212\u03b80, we get\n\u02c6\u03b8 \u2212\u03b80 = \u2212\n\u0000\u22072T (\u03b8)\n\u0001\u22121 |\u03b8=\u03b8\u22c6\u2207T (\u03b8) |\u03b8=\u03b80.\n(6)\nGiven the differentiability of \u03b3k\ni,j(\u03b8) when Hi,j \u0338= 1, we can explicitly calculate the terms in (6) as\n\u2207T (\u03b8) = \u22122JT\n\u03b3 Wn (\u02c6\u03b3n \u2212\u03b3(\u03b8)) ,\n(7)\nand\n\u22072T (\u03b8) = 2JT\n\u03b3 WnJ\u03b3 + o (\u02c6\u03b3n \u2212\u03b3(\u03b8)) ,\n(8)\nwhere J\u03b3 is the Jacobian matrix of \u03b3 (\u03b8) and o (\u02c6\u03b3n \u2212\u03b3(\u03b8)) represents a term containing the second derivatives\nof \u03b3(\u03b8), which are bounded in a neighborhood of \u02c6\u03b8, times \u02c6\u03b3n \u2212\u03b3(\u03b8). Dugo et al. (2024) show that Yt is\nergodic and consequently that\n\u02c6\u03b3k\ni,j\np\u2192\u03b3i,j(k),\nas n \u2192\u221e,\nwhich implies that\n\u02c6\u03b3n\np\u2192\u03b3(\u03b8)\nas n \u2192\u221e.\nFrom this we can conclude that\n\u2207T (\u03b8)\np\u21920\nand\n\u22072T (\u03b8)\np\u21922JT\n\u03b3 WJ\u03b3.\n(9)\nThese two convergences imply together that \u03b8\u22c6\np\u2192\u03b80 and \u02c6\u03b8\np\u2192\u03b80.\nProof of Proposition 3 - (II): In order to establish the central limit theorem for \u02c6\u03b8, we need to understand the\nlimit distribution of the term in (7). It is possible to prove that when Hi + Hj < 3\n2, i, j = 1, . . . , N, \u2200k\n\u221an\n \n1\nn\nn\u2212k\nX\nt=1\nY i\nt+kY j\nt \u2212\u03b3i,j(k)\n!\nd\u2192N(0, \u03c32\nij(k)),\n(10)\nwhere\n\u03c32\nij(k) = Var\n\u0010\nY i\nt+kY j\nt\n\u0011\n+ 2\n\u221e\nX\ns=0\nCov\n\u0010\nY i\nt+kY j\nt , Y i\nt+k+sY j\nt+k\n\u0011\n.\nThe previous result can be extended to the vector \u02c6\u03b3n \u2212\u03b3(\u03b8), as\n\u221an (\u02c6\u03b3n \u2212\u03b3(\u03b8))\nd\u2192N(0, \u0393),\n(11)\nwhere the diagonal elements of \u0393 would coincide with \u03c32\nij(k) above, i, j = 1, . . . , N, k \u2208L, and the additional\noff-diagonal covariance terms would be in a similar form. The results in (10) and (11) are deduced by Theorem\n3.6 in Dugo et al. (2024). See also Theorem 4 in Arcones (1994). Therefore, it is possible to conclude that\n\u221an\u2207T (\u03b8) = \u2212\u221an2JT\n\u03b3 Wn (\u02c6\u03b3n \u2212\u03b3(\u03b8))\nd\u2192N(0, 4JT\n\u03b3 W\u0393WJ\u03b3)\nwhich together with (6), (9), and Slutsky\u2019s Theorem establishes (II).\n23\n\n\n--- Page 24 ---\nTable 7: Description of the dataset utilized in the empirical analysis obtained from the Oxford-Man realized\nlibrary. Log-volatilities are computed as log\n\u0000100\n\u221a\nRV \u00d7 252\n\u0001\n. The columns Missing and Zeros report the\ncount of days with missing values and zero volatility, respectively. Descriptive statistics include the in-sample\naverage (Mean), standard deviation (SD), minimum (Min), median (Med), and maximum (Max) values.\nIncluded specifies wether the time series is included in the multivariate system under analysis or not.\nSymbol\nCountry\nCount\nDescriptive Statistics\nIncluded\nMissing\nZeros\nMean\nSD\nMin\nMedian\nMax\nAEX\nNetherlands\n159\n0\n2.53\n0.51\n0.69\n2.48\n4.63\nYes\nAORD\nAustralia\n218\n0\n2.14\n0.48\n0.51\n2.09\n4.70\nYes\nBFX\nBelgium\n161\n0\n2.46\n0.47\n1.16\n2.42\n4.57\nYes\nBSESN\nIndia\n314\n9\n2.64\n0.49\n1.12\n2.59\n5.27\nYes\nBVLG\nPeru\n3495\n1\n2.27\n0.38\n1.17\n2.26\n4.14\nNo\nBVSP\nBrazil\n363\n0\n2.76\n0.41\n1.28\n2.73\n4.80\nYes\nDJI\nUSA\n264\n0\n2.44\n0.56\n0.79\n2.41\n4.99\nYes\nFCHI\nFrance\n157\n0\n2.63\n0.50\n0.97\n2.61\n4.73\nYes\nFTMIB\nItaly\n2579\n0\n2.60\n0.45\n0.33\n2.59\n4.42\nNo\nFTSE\nUK\n226\n0\n2.54\n0.50\n0.61\n2.48\n5.10\nYes\nGDAXI\nGermany\n196\n0\n2.68\n0.52\n1.17\n2.64\n4.80\nYes\nGSPTSE\nCanada\n857\n0\n2.21\n0.55\n0.67\n2.14\n5.61\nNo\nHSI\nHong Kong\n389\n0\n2.54\n0.42\n1.20\n2.49\n4.65\nYes\nIBEX\nSpain\n194\n0\n2.70\n0.47\n1.17\n2.71\n4.77\nYes\nIXIC\nUSA\n257\n0\n2.54\n0.55\n0.95\n2.48\n4.81\nYes\nKS11\nSouth Korea\n362\n0\n2.54\n0.51\n0.61\n2.48\n4.81\nYes\nKSE\nPakistan\n416\n0\n2.47\n0.52\n-1.52\n2.43\n4.57\nYes\nMXX\nMexico\n257\n0\n2.40\n0.45\n1.10\n2.35\n4.74\nYes\nN225\nJapan\n434\n0\n2.52\n0.47\n0.83\n2.50\n4.59\nYes\nNSEI\nIndia\n318\n12\n2.53\n0.52\n0.53\n2.49\n5.32\nYes\nOMXC20\nDenmark\n1740\n0\n2.59\n0.44\n1.43\n2.52\n5.17\nNo\nOMXHPI\nFinland\n1699\n0\n2.45\n0.50\n1.16\n2.37\n5.47\nNo\nOMXSPI\nSweden\n1699\n0\n2.41\n0.51\n0.90\n2.33\n5.09\nNo\nOSEAX\nNorway\n707\n0\n2.59\n0.48\n1.41\n2.53\n5.41\nNo\nRUT\nUSA\n260\n0\n2.35\n0.51\n-3.32\n2.30\n4.53\nYes\nSMSI\nSwitzerland\n1565\n0\n2.62\n0.48\n0.38\n2.61\n4.82\nNo\nSPX\nUSA\n259\n0\n2.43\n0.57\n0.56\n2.39\n4.94\nYes\nSSEC\nChina\n467\n0\n2.67\n0.53\n1.09\n2.61\n4.64\nYes\nSSMI\nSwitzerland\n259\n0\n2.40\n0.45\n1.36\n2.31\n4.77\nYes\nSTI\nSingapore\n2203\n1\n2.33\n0.34\n1.39\n2.30\n4.22\nNo\nS5E\nEurope\n173\n1\n2.68\n0.53\n-1.88\n2.66\n5.11\nYes\nB\nDataset\nTable 7 and Figure 9 provide a description of the dataset, see the captions for details.\nC\nEmpirics of marginal components\nThe Figures 10, 11, and 12 provide additional results to the analysis presented in Section 5, focusing on the\nunivariate marginal components. They display the same results shown in Figures 4, 5, and 6, but in terms\nof the autocovariances of each component. The key takeaway is consistent with the one derived from the\ncross-covariances.\n24\n\n\n--- Page 25 ---\nFigure 9: Time series of log\n\u0000100\n\u221a\nRV \u00d7 252\n\u0001\nfor the entire sample, where RV is the realized variance from\n5-minute price increments provided in the Oxford-Man realized library. Notice how the symbols not included\nin the empirical analysis (see Table 7) display prolonged periods of missing values.\n25\n\n\n--- Page 26 ---\nFigure 10: Empirical autocovariances of log-volatilities as bars, alongside the theoretical autocovariances from\nour model, indicated by the red curves and based on the estimated parameters from Table 3. The panels,\narranged from left to right and top to bottom, correspond to the indices FCHI, FTSE, N225, and KS11.\n26\n\n\n--- Page 27 ---\nFigure 11: Empirical autocovariances of log-volatilities plotted against a suitable power of the lag (given\nby twice the Hurst exponent) as bars, alongside the best linear fits, indicated by the red lines. The panels,\narranged from left to right and top to bottom, correspond to the indices FCHI, FTSE, N225, and KS11.\n27\n\n\n--- Page 28 ---\nFigure 12: Empirical autocovariances of log-volatilities as bars, alongside the theoretical autocovariances from\nour model when \u03b1i \u21920, indicated by the red curves and based on the estimated parameters (available in the\nOnline Appendix). The panels, arranged from left to right and top to bottom, correspond to the indices\nFCHI, FTSE, N225, and KS11.\n28\n\n\n--- Page 29 ---\nD\nSpillovers methodology\nStarting from a discrete-time moving average representation of a discrete time process Yt \u2208RN,\nYt =\n\u221e\nX\nk=0\nAi\u03b5t\u2212k,\n(12)\nwhere \u03b5t\niid\n\u223cN(0, \u03a3) and Ai \u2208RN\u00d7N such that the representation (12) is well-defined, Pesaran (1997) and\nPesaran and Shin (1998) show that\n\u03c8i,j(h) =\nE\nh\u0000Y i\nt+h \u2212E\n\u0002\nY i\nt+h|Ft\u22121\n\u0003\u00012i\n\u2212E\n\"\u0012\nY i\nt+h \u2212E\n\u0014\nY i\nt+h|Ft\u22121,\n\u0010\n\u03b5j\nt+i\n\u0011h\ni=0\n\u0015\u00132#\nE\nh\u0000Y i\nt+h \u2212E\n\u0002\nY i\nt+h|Ft\u22121\n\u0003\u00012i\n=\np\n\u03a3i,i\n\u22121 Ph\nl=0\n\u0000eT\ni Al\u03a3ej\n\u00012\nPn\nl=0 eT\ni Ai\u03a3AT\ni ei\n,\n(13)\nwhere Ft\u22121 contains information regarding all the past innovations \u03b5t\u2212i, i = 1, 2, . . .. The definition for\n\u03c8i,j(h) is used in Diebold and Yilmaz (2012) to construct spillover indices and analyse them. Therefore, we\nframe the mfOU process in the same setting as above in order to conduct a similar analysis. Consider the\nrepresentation of the causal mfOU (Dugo et al. 2024):\nYt =\nZ t\n\u2212\u221e\nK(t, s)MdWs,\n(14)\nwhere the kernel K(t, s) : R2 \u2192RN is diagonal with components\nKi(t, s) = \u03bdi\n\u0012\n(t \u2212s)\nHi\u22121\n2\n+\n\u2212(\u2212s)\nHi\u22121\n2\n+\n\u2212\u03b1i\nZ t\ns\ne\u2212\u03b1i(t\u2212u) \u0010\n(u \u2212s)\nHi\u22121\n2\n+\n\u2212(\u2212s)\nHi\u22121\n2\n+\n\u0011\ndu\n\u0013\n,\ni = 1, . . . , N, M is an N \u00d7 N matrix such that MM T = P (from Amblard et al. 2010), with\nPi,j =\nsin (\u03c0(Hi + Hj))\nB\n\u0000Hi + 1\n2, Hj + 1\n2\n\u0001\n(cos (\u03c0Hi) + cos (\u03c0Hj))\u03c1i,j,\nwhere B(x, y), x, y > 0, denotes the Beta function, and Wt is a standard N-dimensional Brownian motion.\nNote that the representation in (14) introduces a modification compared to that in Dugo et al. (2024), as it\nfactors M out of the kernel. This factorization enables the separation of the dependence on time from that\nbetween components, aligning the model with the framework in (12). Equivalently,\nY i\nt =\nZ t\n\u2212\u221e\nKi(t, s)\nN\nX\nj=1\nMi,jdW j\ns\n=\ns\nsin(\u03c0Hi)\nB\n\u0000Hi + 1\n2, Hi + 1\n2\n\u0001\nZ t\n\u2212\u221e\nKi(t, s)\nN\nX\nj=1\ns\nB\n\u0000Hi + 1\n2, Hi + 1\n2\n\u0001\nsin(\u03c0Hi)\nMi,jdW j\ns\n=\ns\nsin(\u03c0Hi)\nB\n\u0000Hi + 1\n2, Hi + 1\n2\n\u0001\nZ t\n\u2212\u221e\nKi(t, s)\nN\nX\nj=1\ndW\nj\ns\nwhere W is a Brownian motion with covariance matrix G, where\nGi,j =\ns\nB(Hi + 1\n2, Hi + 1\n2)B(Hj + 1\n2, Hj + 1\n2)\nsin (\u03c0Hi) sin (\u03c0Hj)\n1\nB(Hi + 1\n2, Hj + 1\n2)\nsin (\u03c0(Hi + Hj))\n(cos (\u03c0Hi) + cos (\u03c0Hj))\u03c1i,j.\n29\n\n\n--- Page 30 ---\nNote that W\nj\n\u00b7 is a Brownian motion with unit variance, for any j. We can rewrite\nYt =\nZ t\n0\nK(t \u2212s)dW s +\nZ 0\n\u2212\u221e\nK\u2217(t, s)dW s,\nwhere K\u2217(t, s) = K(t, s)\np\nsin(\u03c0Hi)/B(Hi + 1/2, Hi + 1/2) and\nK(t \u2212s)i,j = \u03bdi\ns\nsin(\u03c0Hi)\nB\n\u0000Hi + 1\n2, Hi + 1\n2\n\u0001\n\u0012\n(t \u2212s)Hi\u22121\n2 \u2212\u03b1i\nZ t\ns\ne\u2212\u03b1i(t\u2212u) \u0010\n(u \u2212s)Hi\u22121\n2\n\u0011\ndu\n\u0013\n.\nOne can check that this dependens only on t \u2212s with simple change of variables. At this point we can\napproximate the process using a left point discretization scheme over a grid with uniform mesh \u2206, introducing\n\u03b5l := \u2206lW = W l\u2206\u2212W (l\u22121)\u2206, as\nYt \u2248\nt\n\u2206\nX\nl=1\nK (t \u2212(l \u22121)\u2206) \u2206lW +\n0\nX\nl=\u2212\u221e\nK\u2217(t, (l \u22121)\u2206) \u2206lW\n=\nt\n\u2206\u22121\nX\nk=0\nK ((k + 1)\u2206) \u03b5 t\n\u2206\u2212k +\n+\u221e\nX\nk= t\n\u2206\nK\u2217(t, t \u2212(k + 1)\u2206)\u03b5 t\n\u2206\u2212k\n=\nt\u22121\nX\nk=0\nAk\u03b5t\u2212k +\n\u221e\nX\nk=t\nBk,t\u03b5t\u2212k,\n(15)\nwhere we fix t :=\nt\n\u2206, k + l = t, Ak = K ((k + 1)\u2206) , and Bk,t = K\u2217(t, t \u2212(k + 1)\u2206). When t \u2265h > 0, the\ncalculation in (13) delivers the same result for the discretized mfOU process in (15) as for the process in (12),\ndue to the common convolution term Pt\u22121\nk=0 Ak\u03b5t\u2212k, and therefore\n\u03c8i,j(h) = \u2206\np\n\u03a3j,j\n\u22121 Ph\u22121\nl=0\n\u0000eT\ni K ((l + 1)\u2206) Gej\n\u00012\nPh\u22121\nl=0\n\u0010\neT\ni K ((l + 1)\u2206) GK ((l + 1)\u2206)T ei\n\u0011\n=\n\u2206Ph\u22121\nl=0 K\n2\ni,j ((l + 1)\u2206) G2\ni,j\np\n\u03a3j,j\nPh\u22121\nl=0 K\n2\ni,j ((l + 1)\u2206) Gi,i\n=\n\u2206G2\ni,j\np\nGj,jGi,i\n,\n(16)\nand\ne\u03c8i,j =\n\u03c8i,j\nPN\nj=1 \u03c8i,j\n=\nG2\ni,j/\np\nGj,j\nPN\nm=1 G2\ni,m/\np\nGm,m\n.\n(17)\nRemark 3. The result in (17) is obtained under the assumption that \u03b5j\nt+i, i = 0, . . . , n, j = 1, . . . , N follows a\nwhite noise process in (12). A similar spillover analysis could be performed by relaxing the i.i.d. assumption and,\ninstead of using the moving average representation, simply conditioning on \u03b5j\nt+i, i = 1, . . . , n, j = 1, . . . , N\nbeing the fractional Gaussian noise innovations in (13). Preliminary results on this variant suggest qualitatively\nsimilar outcomes, though with dependency on the forecast horizon and larger computational burden.\nE\nAdditional material\nThe Online Appendix, available at https://ranieridugo.github.io/mfou, provides additional empirical results\nfor the components of the system not reported here, as well as results related to the small-alpha (slow mean\nreversion) and causal regimes. The working code to reproduce the empirical findings of this paper and\nefficiently simulate a bivariate version of the mfOU process is accessible at the same GitHub repository.\n30\n\n\n--- Page 31 ---\nReferences\nAmblard, Pierre-Olivier and Jean-Fran\u00e7ois Coeurjolly (2011). \u201cIdentification of the multivariate fractional\nBrownian motion\u201d. In: IEEE Transactions on Signal Processing 59.11, pp. 5152\u20135168.\nAmblard, Pierre-Olivier, Jean-Fran\u00e7ois Coeurjolly, Fr\u00e9d\u00e9ric Lavancier, and Anne Philippe (2010). \u201cBasic\nproperties of the multivariate fractional Brownian motion\u201d. In: arXiv preprint arXiv:1007.0828.\nAndersen, Torben G, Tim Bollerslev, Francis X Diebold, and Heiko Ebens (2001). \u201cThe distribution of\nrealized stock return volatility\u201d. In: Journal of financial economics 61.1, pp. 43\u201376.\nAndersen, Torben G and Bent E S\u00f8rensen (1996). \u201cGMM estimation of a stochastic volatility model: A Monte\nCarlo study\u201d. In: Journal of Business & Economic Statistics 14.3, pp. 328\u2013352.\nArcones, Miguel A (1994). \u201cLimit theorems for nonlinear functionals of a stationary Gaussian sequence of\nvectors\u201d. In: The Annals of Probability, pp. 2242\u20132274.\nBayer, Christian, Peter K. Friz, and Jim Gatheral (2016). \u201cPricing under rough volatility\u201d. In: Quantitative\nFinance 16.6, pp. 887\u2013904.\nBayer, Christian, Peter K. Friz, A. Gulisashvili, B. Horvath, and B. Stemper (2019). \u201cShort-time near-the-\nmoney skew in rough fractional volatility models\u201d. In: Quant. Finance 19.5, pp. 779\u2013798.\nBennedsen, Mikkel, Asger Lunde, and Mikko S. Pakkanen (2021). \u201cDecoupling the Short- and Long-Term\nBehavior of Stochastic Volatility\u201d. In: Journal of Financial Econometrics 20.5, pp. 961\u20131006.\nBianchi, Sergio, Daniele Angelini, Augusto Pianese, and Massimiliano Frezza (2023). \u201cRough volatility via the\nLamperti transform\u201d. In: Communications in Nonlinear Science and Numerical Simulation 127, p. 107582.\nBolko, Anine E, Kim Christensen, Mikko S Pakkanen, and Bezirgen Veliyev (2023). \u201cA GMM approach to\nestimate the roughness of stochastic volatility\u201d. In: Journal of Econometrics 235.2, pp. 745\u2013778.\nCheridito, Patrick, Hideyuki Kawaguchi, and Makoto Maejima (2003). \u201cFractional ornstein-uhlenbeck pro-\ncesses\u201d. In: Electronic Journal of probability 8, pp. 1\u201314.\nChong, Carsten H., Marc Hoffmann, Yanghui Liu, Mathieu Rosenbaum, and Gr\u00e9goire Szymanski (June\n2024a). \u201cStatistical inference for rough volatility: Central limit theorems\u201d. In: The Annals of Applied\nProbability 34.3.\nChong, Carsten H., Marc Hoffmann, Yanghui Liu, Mathieu Rosenbaum, and Gr\u00e9goire Szymansky (2024b).\n\u201cStatistical inference for rough volatility: Minimax theory\u201d. In: The Annals of Statistics 52.4, pp. 1277\n\u20131306.\nComte, Fabienne and Eric Renault (1998). \u201cLong memory in continuous-time stochastic volatility models\u201d.\nIn: Math. Finance 8.4, pp. 291\u2013323.\nCordi, Marcus, Damien Challet, and Serge Kassibrakis (2021). \u201cThe market nanostructure origin of asset\nprice time reversal asymmetry\u201d. In: Quantitative Finance 21.2, pp. 295\u2013304.\nDelemotte, Jules, Stefano De Marco, and Florent Segonne (2023). \u201cTests for Hurst effect\u201d. In: Available at\nSSRN: https://ssrn.com/abstract=4428407.\nDiebold, Francis X and Kamil Yilmaz (2009). \u201cMeasuring financial asset return and volatility spillovers, with\napplication to global equity markets\u201d. In: The Economic Journal 119.534, pp. 158\u2013171.\nDiebold, Francis X and Kamil Yilmaz (2012). \u201cBetter to give than to receive: Predictive directional measure-\nment of volatility spillovers\u201d. In: International Journal of forecasting 28.1, pp. 57\u201366.\nDing, Zhuanxin and Clive WJ Granger (1996). \u201cModeling volatility persistence of speculative returns: a new\napproach\u201d. In: Journal of econometrics 73.1, pp. 185\u2013215.\nDugo, Ranieri, Giacomo Giorgio, and Paolo Pigato (2024). \u201cThe multivariate fractional Ornstein-Uhlenbeck\nprocess\u201d. In: arXiv preprint arXiv:2408.03051.\nEumenius-Schulz, Yaroslav (2020). \u201cSpot estimation for fractional Ornstein-Uhlenbeck stochastic volatility\nmodel: consistency and central limit theorem\u201d. In: Stat. Inference Stoch. Process. 23.2, pp. 355\u2013380.\nFouque, Jean-Pierre, George Papanicolaou, and K Ronnie Sircar (2000). \u201cMean-reverting stochastic volatility\u201d.\nIn: International Journal of theoretical and applied finance 3.01, pp. 101\u2013142.\nFriz, Peter K., Paul Gassiat, and Paolo Pigato (2022). \u201cShort-dated smile under rough volatility: asymptotics\nand numerics\u201d. In: Quantitative Finance 22.3, pp. 463\u2013480.\nFukasawa, Masaaki, Tetsuya Takabatake, and Rebecca Westphal (2022). \u201cConsistent estimation for fractional\nstochastic volatility model under high-frequency asymptotics\u201d. In: Mathematical Finance 32.4, pp. 1086\u2013\n1132.\n31\n\n\n--- Page 32 ---\nGatheral, Jim, Thibault Jaisson, and Mathieu Rosenbaum (2018). \u201cVolatility is rough\u201d. In: Quantitative\nFinance 18.6, pp. 933\u2013949.\nGuyon, Julien and Mehdi El Amrani (2023). \u201cDoes the Term-Structure of the At-the-Money Skew Really\nFollow a Power Law?\u201d In: Risk.\nHansen, Lars Peter (1982). \u201cLarge sample properties of generalized method of moments estimators\u201d. In:\nEconometrica: Journal of the econometric society, pp. 1029\u20131054.\nHayashi, Fumio (2011). Econometrics. Princeton University Press.\nJacod, Jean (2000). \u201cNon-parametric Kernel Estimation of the Coefficient of a Diffusion\u201d. In: Scandinavian\nJournal of Statistics 27.1, pp. 83\u201396.\nLang, Gabriel and Fran\u00e7ois Roueff (2001). \u201cSemi-parametric estimation of the H\u00f6lder exponent of a stationary\nGaussian process with minimax rates\u201d. In: Statistical Inference for Stochastic Processes 4, pp. 283\u2013306.\nLivieri, Giulia, Saad Mouti, Andrea Pallavicini, and Mathieu Rosenbaum (2018). \u201cRough volatility: Evidence\nfrom option prices\u201d. In: IISE Transactions 50.9, pp. 767\u2013776.\nPesaran, Hashem (1997). Working with Microfit 4.0: Interactive Econometric Analysis.\nPesaran, Hashem and Yongcheol Shin (1998). \u201cGeneralized impulse response analysis in linear multivariate\nmodels\u201d. In: Economics letters 58.1, pp. 17\u201329.\nPodobnik, Boris, D. Fu, H. Stanley, and Plamen Ivanov (2008). \u201cDetrended cross-correlation analysis: a new\nmethod for analyzing two nonstationary time series\u201d. In: Phys Rev Lett.\nPodobnik, Boris, DF Fu, H Eugene Stanley, and P Ch Ivanov (2007). \u201cPower-law autocorrelated stochastic\nprocesses with long-range cross-correlations\u201d. In: The European Physical Journal B 56.1, pp. 47\u201352.\nPodobnik, Boris, Duan Wang, Davor Horvatic, Ivo Grosse, and H Eugene Stanley (2010). \u201cTime-lag cross-\ncorrelations in collective phenomena\u201d. In: EPL (Europhysics Letters) 90.6, p. 68001.\nTieslau, Margie A., Peter Schmidt, and Richard T. Baillie (1996). \u201cA minimum distance estimator for\nlong-memory processes\u201d. In: Journal of Econometrics 71.1, pp. 249\u2013264.\nWang, Duan, Boris Podobnik, Davor Horvatic, and H. Eugene Stanley (2011). \u201cQuantifying and modeling\nlong-range cross correlations in multiple time series with applications to world stock indices\u201d. In: Phys.\nRev. E 83 (4), p. 046121.\nWang, Xiaohu, Weilin Xiao, and Jun Yu (2023). \u201cModeling and forecasting realized volatility with the\nfractional Ornstein-Uhlenbeck process\u201d. In: J. Econometrics 232.2, pp. 389\u2013415.\nWood, Andrew TA and Grace Chan (1994). \u201cSimulation of stationary Gaussian processes in [0, 1] d\u201d. In:\nJournal of computational and graphical statistics 3.4, pp. 409\u2013432.\nZumbach, Gilles (2009). \u201cTime reversal invariance in finance\u201d. In: Quantitative Finance 9.5, pp. 505\u2013515.\n32\n",
    "pages": 32
  },
  {
    "filename": "Multivariate GARCH.pdf",
    "text": "\n\n--- Page 1 ---\narXiv:2506.02796v1  [q-fin.CP]  3 Jun 2025\nDeep Learning Enhanced Multivariate\nGARCH\nHaoyuan Wang, Chen Liu, Minh-Ngoc Tran, and Chao Wang\u2217\nDiscipline of Business Analytics, The University of Sydney Business School\nAbstract\nThis paper introduces a novel multivariate volatility modeling framework, named\nLong Short-Term Memory enhanced BEKK (LSTM-BEKK), that integrates deep\nlearning into multivariate GARCH processes. By combining the flexibility of recur-\nrent neural networks with the econometric structure of BEKK models, our approach\nis designed to better capture nonlinear, dynamic, and high-dimensional dependence\nstructures in financial return data. The proposed model addresses key limitations\nof traditional multivariate GARCH-based methods, particularly in capturing per-\nsistent volatility clustering and asymmetric co-movement across assets. Leveraging\nthe data-driven nature of LSTMs, the framework adapts effectively to time-varying\nmarket conditions, offering improved robustness and forecasting performance. Em-\npirical results across multiple equity markets confirm that the LSTM-BEKK model\nachieves superior performance in terms of out-of-sample portfolio risk forecast, while\nmaintaining the interpretability from the BEKK models. These findings highlight\nthe potential of hybrid econometric-deep learning models in advancing financial risk\nmanagement and multivariate volatility forecasting.\nKeywords: multivariate volatility modeling, Long Short-Term Memory, portfolio opti-\nmization, high-dimensional finance.\n\u2217Corresponding author: chao.wang@sydney.edu.au.\n1\n\n\n--- Page 2 ---\n1\nIntroduction\nModeling financial market volatility has long been a central topic in econometrics due\nto its critical role in risk management, asset pricing, and portfolio optimization. Engle\n(1982) pioneered this line of research with the introduction of the Autoregressive Condi-\ntional Heteroskedasticity model, which characterizes time-varying volatility as a function\nof past shocks. This foundational framework was later generalized by Bollerslev (1986)\nthrough the GARCH model, which incorporated both lagged innovations and past vari-\nances, enabling improved modeling of persistent volatility behavior. In parallel to the\nGARCH family, stochastic volatility (SV) models emerged as an important alternative,\nmodeling volatility as an unobserved latent process governed by its own stochastic dy-\nnamics. This latent-state formulation allows SV models to capture the stylized facts often\nobserved in financial time series (Taylor, 1994; Asai et al., 2006).\nWhile univariate volatility models are effective in capturing the dynamics of individual\nasset volatility, financial markets are inherently multivariate, with assets exhibiting strong\ncomovements and spillovers. Accurate modeling of such joint dynamics is essential for\nsystemic risk monitoring, portfolio allocation, and derivative pricing. In this context,\nthe conditional covariance matrix plays a central role by describing the time-varying co-\nmovement among asset returns. As highlighted in Bollerslev et al. (1988), Engle (2002)\nand Bauwens et al. (2006), multivariate volatility modeling enables the quantification of\ninterdependencies across assets and enhances the effectiveness of financial decision-making\nunder uncertainty.\nTo extend volatility modeling to multivariate settings,\nmultivariate GARCH\n(MGARCH) models have been proposed as natural generalizations of the univariate frame-\nwork. Among these, the BEKK model introduced by Engle and Kroner (1995) stands out\ndue to its flexible parameterization and being a direct extension of univariate GARCH.\nThe BEKK specification guarantees the positive definiteness of the conditional covariance\nmatrix by construction and can capture dynamic spillovers between asset returns. This\nstructural advantage makes it particularly appealing for applications requiring robust co-\nvariance estimation in financial risk modeling and forecasting. Empirical studies (see, e.g.,\nSilvennoinen and Ter\u00e4svirta, 2009; Fang et al., 2015) have demonstrated the adaptabil-\nity of MGARCH frameworks in capturing time-varying correlations and volatilities under\nextreme events, reinforcing their relevance in financial risk management.\nTraditional MGARCH formulations such as BEKK encounter severe computational\nbottlenecks in high-dimensional settings due to the rapidly expanding parameter space\n(Ledoit and Wolf, 2012, 2015). To address this scalability issue, Engle (2002) proposed\nthe Dynamic Conditional Correlation (DCC) model, which simplifies estimation by decou-\npling univariate volatility and correlation dynamics. This reduction in complexity allows\nDCC to accommodate a larger number of assets while still capturing time-varying depen-\ndencies. Extensions such as the Student-t DCC (Ku, 2008) and Asymmetric DCC (Lai\nand Sheu, 2011) further enhance the modeling robustness under heavy tails and asym-\nmetric shocks, while the Dynamic Equicorrelation model (Engle and Kelly, 2012) further\n2\n\n\n--- Page 3 ---\nimproves computational tractability for the DCC framework.\nThese methodological contributions collectively underpin the development of modern\nmultivariate volatility modeling, which is central to understanding the dynamic behavior\nof financial markets and the evolving interdependencies among assets. While models such\nas BEKK, DCC and their extensions have significantly advanced the multivariate volatility\nmodeling, their reliance on the simple summation of lagged covariance matrices and outer\nproducts of return vectors limits their adaptability to complicated patterns and structural\nshifts often observed during periods of financial stress. These limitations motivate the\nintegration of more expressive modeling techniques, such as deep learning models, into\nmultivariate volatility modeling, offering potential new directions for advancing volatility\nmodeling in high-dimensional financial applications.\nRecent advances in deep learning provide compelling techniques for modeling com-\nplicated and high dimensional sequential data. Recurrent Neural Networks (RNNs), and\nLong Short-Term Memory (LSTM) networks in particular, offer a powerful mechanism for\nlearning temporal and comovement dependencies in sequential multivariate data. Their\nability to capture long-range interactions and nonlinear patterns has led to various success-\nful applications across a range of large-scale industry level tasks (Goodfellow et al., 2016).\nThis paper proposes a novel hybrid model, the LSTM-BEKK, which combines the econo-\nmetric rigor of the BEKK model with the adaptive capabilities of LSTMs. The LSTM-\nBEKK model leverages the structural strengths of BEKK while enhancing it with LSTM\u2019s\nability to capture nonlinear and long-term dynamics. By allowing LSTM-generated com-\nponents to directly influence the time-varying covariance matrix, this framework provides\na highly flexible tool for analyzing the evolving relationships among financial assets, par-\nticularly in high-dimensional contexts.\nThe design of the LSTM-BEKK model is inspired by recent advancements in inte-\ngrating deep learning with univariate volatility models, which have demonstrated superior\npredictive accuracy and the ability to capture nonlinearities in volatility. For example,\nthe work by Nguyen et al. (2022) explored the effectiveness of deep learning in enhancing\nGARCH-based volatility modeling. These studies highlight the advantages of incorporat-\ning neural networks into traditional econometric frameworks, improving the adaptability\nand forecasting performance of volatility models. Extending this approach to a multi-\nvariate setting introduces unique challenges, such as ensuring positive definiteness of the\ncovariance matrix and managing the curse of dimensionality (Ledoit and Wolf, 2012).\nThe primary innovation of the LSTM-BEKK model lies in its ability to utilize economic\ninformation instruments and adapt to changing market conditions. Unlike traditional\nBEKK models constrained by their inflexible parametric structure, the LSTM-BEKK\nmodel dynamically adjusts itself to capture evolving market relationships. Furthermore,\nthe LSTM\u2019s capacity to learn complex patterns enhances the model\u2019s responsiveness to\nturbulent market periods, improving the accuracy of volatility forecasts.\nSubstantial empirical results demonstrate that the LSTM-BEKK model outperforms\ntraditional BEKK and DCC models in terms of predictive accuracy, as measured by\nstandard evaluation metrics such as out-of-sample negative log-likelihood and annualized\n3\n\n\n--- Page 4 ---\nvolatility of global minimum variance portfolio. These findings are robust across datasets\ncovering portfolios constructed from the top companies by market capitalization in Japan,\nthe U.S., and the U.K., reflecting the model\u2019s strong generalization capability across dif-\nferent market environments. Moreover, in low-dimensional settings, the interpretability of\nthe LSTM-BEKK framework is enhanced by visualizing individual variance and covariance\ntrajectories, which reveal the model\u2019s ability to capture both abrupt volatility spikes and\ndirectional shifts in inter-asset correlations\u2014especially during periods of market stress.\nThese insights affirm not only its predictive power but also its value in understanding the\nevolving structure of financial return dynamics.\nThe paper is organized as follows. In Section 2, we review the relevant literature on\nMGARCH models and machine learning techniques, then detail the structure and the-\noretical foundations of the proposed LSTM-BEKK model. The estimation procedure of\nthe LSTM-BEKK is detailed in Section 3. Section 4 evaluates the empirical performance\nof LSTM-BEKK on high-dimensional datasets across multiple settings, comparing it with\nestablished benchmarks. A study focusing on global minimum variance portfolio is con-\nducted in Section 5. Section 6 concludes the paper and discusses future work. Technical\ndetails and further empirical study of LSTM-BEKK are included in the Appendix.\n2\nModeling Frameworks\n2.1\nFoundation Models\nThis section presents the foundation models from econometrics and machine learning\nthat form the building blocks for our proposed LSTM-BEKK model. We also present\nseveral benchmark multivariate volatility models that will be used to compare against\nLSTM-BEKK.\n2.1.1\nBEKK Models\nThe BEKK model is a representative within the MGARCH framework, designed to guar-\nantee the positive definiteness of the conditional covariance matrix while preserving flex-\nibility in modeling dynamic dependencies across financial assets.\nLet rt = (rt,1, . . . , rt,n)\u2032 denote the vector of de-meaned returns for n portfolio assets at\ntime t. The returns are assumed to follow a multivariate normal distribution conditional\non past information Ft\u22121:\nrt|Ft\u22121 \u223cN(0, Ht),\n(1)\nwhere Ht = cov(rt|Ft\u22121) represents the conditional covariance matrix of returns. While\nit is possible to consider more flat-tailed distributions such as a multivariate Student\u2019s t,\nwe use the multivariate normal distribution in this paper to facilitate exposition. This\n4\n\n\n--- Page 5 ---\ncovariance matrix Ht captures time-varying dependencies among portfolio assets, a crit-\nical element for financial applications, such as risk management and portfolio allocation\n(Bollerslev et al., 1988; Bauwens et al., 2006; McAleer et al., 2008).\nThe general BEKK(p, q) model specifies Ht as:\nHt = \u2126+\np\nX\ni=1\nAirt\u2212ir\u2032\nt\u2212iA\u2032\ni +\nq\nX\nj=1\nBjHt\u2212jB\u2032\nj,\n(2)\nwhere \u2126is a symmetric positive definite matrix, and Ai and Bj are n \u00d7 n coefficient\nmatrices capturing the effects of past shocks and past covariances, and p and q represent\nthe orders of the process (Francq and Zako\u00efan, 2012; Scherrer and Ribarits, 2007). To\nreduce complexity, the BEKK(1,1) model is commonly used, which assumes p = q = 1,\nleading to the formulation:\nHt = \u2126+ A1rt\u22121r\u2032\nt\u22121A\u2032\n1 + B1Ht\u22121B\u2032\n1.\n(3)\nA further simplification of the BEKK model is the Scalar BEKK specification, which\nsimplifies the parameterization by imposing the following constraints: \u2126= CC\u2032, A1 =\n\u221aaI, and B1 =\n\u221a\nbI, where C is a lower triangular matrix and I denotes the identity\nmatrix. This results in the more compact form Ht as:\nHt = CC\u2032 + art\u22121r\u2032\nt\u22121 + bHt\u22121.\n(4)\nHere, a, b \u22650 are scalar parameters representing the effects of past shocks and lagged\ncovariances, respectively. The diagonal elements of C are assumed to be strictly non-zero,\nguaranteeing the positive definiteness of \u2126= CC\u2032, hence Ht (Francq and Zakoian, 2019;\nMatsui and Pedersen, 2022; Hafner and Preminger, 2009). Moreover, the stationarity\ncondition a + b < 1 ensures the decay of volatility over time, preserving the long-run sta-\nbility of the process (Scherrer and Ribarits, 2007; Hafner et al., 2017). This parsimonious\nstructure drastically reduces the number of parameters in full BEKK models.\nFrom an econometric perspective, the parameters a and b have intuitive interpreta-\ntions. The parameter a measures the sensitivity of the conditional covariance matrix to\nthe past shocks, capturing the immediate effect of return innovations on volatility. Mean-\nwhile, b reflects the persistence of volatility over time, characterizing how past volatilities\ninfluence future dynamics. Together, these parameters provide a framework for under-\nstanding how risk propagates through time in financial markets.\nThe Scalar BEKK model postulates the conditional covariance matrix as a simple\nlinear summation between its lagged value and the outer product of the past shock vec-\ntor. While this assumption facilitates estimation and ensures computational tractability,\nit limits the model\u2019s ability to capture more complex, nonlinear relationships among as-\nsets (Caporin and McAleer, 2008; Hafner and Rombouts, 2007; Scherrer and Ribarits,\n2007). These limitations inspire the development of our LSTM-BEKK model, presented\n5\n\n\n--- Page 6 ---\nin Section 2.2, which integrates machine learning techniques into Scalar BEKK to en-\nhance its adaptability and ability to capture richer dynamics, while still maintaining its\neconometric interpretability.\nIn summary, the Scalar BEKK model strikes a balance between simplicity and efficacy,\nmaking it an essential tool for modeling multivariate volatility in financial markets. Its\nparsimonious structure and intuitive economic interpretation make it particularly suitable\nfor applications such as portfolio risk management, systemic risk analysis, and stress\ntesting. However, its reliance on over-parsimonious parameterization necessitates further\nextensions, to address the complexities of real-world financial data.\n2.1.2\nDynamic Conditional Correlation (DCC) Model\nThe DCC model, introduced by Engle (2002), represents a major advancement in mul-\ntivariate volatility modeling by efficiently capturing time-varying correlations in high-\ndimensional datasets. Unlike the full BEKK model, which suffers from parameter prolif-\neration in large systems, the DCC model decomposes the conditional covariance matrix\ninto conditional variances and correlations, enabling a computationally efficient estimation\nframework. The DCC model decomposes Ht as:\nHt = DtRtDt,\n(5)\nwhere Dt = diag(\np\nht,1, . . . ,\np\nht,n) is a diagonal matrix of conditional standard devia-\ntions, and Rt is the conditional correlation matrix.\nThe diagonal elements of Dt are modeled as univariate GARCH processes:\nhi,t = \u03c9i + \u03b1ir2\ni,t\u22121 + \u03b2ihi,t\u22121,\ni = 1, . . . , n,\n(6)\nwhere \u03c9i > 0, \u03b1i \u22650, and \u03b2i \u22650 and \u03b1i + \u03b2i < 1 ensure positivity and stationarity of the\nconditional variances. The correlation dynamics are governed by the intermediate matrix\nQt, updated recursively as:\nQt = (1 \u2212a \u2212b)S + azt\u22121z\u2032\nt\u22121 + bQt\u22121,\n(7)\nwhere zt = D\u22121\nt rt is the vector of standardized residuals, and S is the unconditional\ncovariance matrix of zt. In this formulation, the parameters a and b play central roles in\ndetermining the dynamics of the conditional correlation matrix. Specifically, a governs\nthe sensitivity of correlations to recent shocks in the standardized residuals (i.e., the\ninnovation effect), while b controls the persistence of past correlations. The sum a+b < 1\nis imposed to ensure stationarity and to guarantee that the conditional correlation matrix\nremains well-defined over time. Together, these parameters dictate the responsiveness and\nmemory of the correlation dynamics, with higher values indicating stronger persistence\nand slower adaptation to new information. The matrix S is typically estimated as the\nsample covariance of the standardized residuals during the initial estimation stage:\nS = 1\nT\nT\nX\nt=1\nztz\u2032\nt,\n(8)\n6\n\n\n--- Page 7 ---\nwhere T is the sample size. The conditional correlation matrix Rt is obtained by stan-\ndardizing Qt:\nRt = diag(Qt)\u22121/2Qtdiag(Qt)\u22121/2.\n(9)\nThis ensures that Rt is symmetric, positive definite, and has unit diagonal elements.\nThe modular structure of the DCC model facilitates efficient estimation. In the first\nstep, univariate GARCH models are estimated to compute Dt. In the second step, the\ncorrelation dynamics are estimated using the standardized residuals zt. This separation\nreduces computational complexity compared to fully parameterized MGARCH models\n(Francq and Zako\u00efan, 2012; Bauwens et al., 2006), making the DCC model scalable to\nhigh-dimensional datasets.\nThe DCC model has been widely adopted in applications\nsuch as portfolio optimization, risk management, and systemic risk evaluation (Bauwens\net al., 2006; Engle and Kelly, 2012). Extensions such as the corrected DCC model of\nAielli (2013) and regime-switching DCC variants of Bauwens and Otranto (2020) further\nimprove the adaptability of DCC in different application contexts.\nOverall, the DCC model represents a foundational tool in multivariate volatility\nmodeling due to its balance between parsimony and effectiveness.\nHowever, evolving\nmarket complexity increasingly demands more expressive models. Innovations such as\ndeep learning-augmented structures offer promising pathways to improve upon traditional\nframeworks and better accommodate the intricacies of modern asset return dynamics.\n2.1.3\nLSTM Model\nDeep learning methods, particularly RNNs, have proven to be powerful tools for modeling\nsequential multivariate data. Among these, the LSTM network, introduced by Hochreiter\nand Schmidhuber (1997), stands out to be one of the most commonly used and effective\nRNN models. Its gating mechanism enables the selective retention of long-term depen-\ndencies, making it particularly suitable for applications in time series analysis, including\nfinancial volatility modeling (Goodfellow et al., 2016). The architecture of LSTM net-\nworks comprises three primary gates: the input gate (git), forget gate (gft), and output\ngate (got). These gates regulate the flow of information, dynamically updating the cell\nstate (ct) and hidden state (ht) to capture long-term and short-term patterns. Let xt be\nthe input vector at time t, and yt be the forecast variable of interest. The evolution of\nthese components can be described by the following equations:\ngit = \u03c3 (Wi[ht\u22121, xt] + bi) ,\n(10a)\ngft = \u03c3 (Wf[ht\u22121, xt] + bf) ,\n(10b)\ngot = \u03c3 (Wo[ht\u22121, xt] + bo) ,\n(10c)\n\u02dcct = tanh (Wc[ht\u22121, xt] + bc) ,\n(10d)\nct = gft \u2299ct\u22121 + git \u2299\u02dcct,\n(10e)\nht = got \u2299tanh(ct),\n(10f)\n7\n\n\n--- Page 8 ---\nwhere \u03c3(\u00b7) is the sigmoid activation function, tanh(\u00b7) is the hyperbolic tangent function,\nand \u2299denotes element-wise multiplication. The hidden state ht is then linked to the\nforecast variable yt by a measurement equation specified depending on the application\ncontext. These equations allow the LSTM network to adaptively learn from sequential\ndata by managing the flow of information across time steps. The reader is referred to\nGoodfellow et al. (2016) for a more detailed introduction of RNN models.\nThe primary properties of the LSTM network include its ability to capture nonlin-\near relationships and long-term dependencies, which are crucial for financial time series\nexhibiting volatility clustering and structural breaks.\nAdditionally, its architecture is\nrobust to noise, allowing it to generalize well across varied datasets. These properties\nmake LSTM networks an ideal choice for modeling financial volatility, complementing\ntraditional econometric models.\n2.2\nThe LSTM-BEKK Model\nThe proposed LSTM-BEKK model represents a novel hybrid framework that integrates\nthe econometric structure of the Scalar BEKK model with the adaptive learning capa-\nbilities of LSTM neural networks. This approach enhances the traditional MGARCH\nframework by addressing its inherent linear assumptions and introducing the ability to\ncapture complex, nonlinear dynamics and temporal dependencies in financial volatility.\nSuch a framework is particularly suited for high-dimensional datasets and volatile market\nconditions, where conventional models often struggle to balance flexibility and computa-\ntional feasibility.\nThe LSTM-BEKK model extends the Scalar BEKK framework by incorporating a\ndynamic component, Ct, generated by a LSTM network.\nThe conditional covariance\nmatrix Ht is expressed as:\nHt = CC\u2032 + CtC\u2032\nt + art\u22121r\u2032\nt\u22121 + bHt\u22121,\n(11)\nwhere C is a static lower triangular matrix with suitable constraints ensuring the positive\ndefiniteness of Ht, and a, b \u22650 are scalar parameters capturing the impact of past shocks\nand volatilities. The LSTM-generated lower-triangular matrix Ct dynamically adapts to\nchanging market conditions, introducing flexibility to model nonlinear dependencies and\nevolving relationships among financial assets.\nThe dynamic update of Ct is modeled through an LSTM network, which takes the\nmost recent return vector rt\u22121 as the input,\n\u02dcCt = LSTM(ht\u22121, rt\u22121),\n(12)\nwith the output vector \u02dcCt reshaped to form the lower-triangular matrix Ct. The vector\n\u02dcCt serves as an intermediate latent representation that captures both short-term and\nlong-term dependencies in return series via the recurrent structure of LSTM. The LSTM\n8\n\n\n--- Page 9 ---\nunit utilizes gating mechanisms\u2014specifically, input, forget, and output gates\u2014to regulate\ninformation flow dynamically. At each time step t, the LSTM processes rt\u22121 and the\nprevious hidden state ht\u22121 to generate an updated hidden state ht, which encodes the\ninformation from past observations, before outputting \u02dcCt. More specifically, we compute\nCt as\nCt = LowerTriangular\n\u0010\n\u02dcCt\n\u0011\n,\nCt,ii \u2190Ct,ii \u00b7 \u03c3(\u03b2Ct,ii).\n(13)\nThe diagonal elements are regularized via the Swish activation function x \u00b7 \u03c3(\u03b2x), with\n\u03b2 being a learnable parameter. We observe empirically that its smooth, non-monotonic\nshape helps stabilize the learning process during covariance matrix construction.\nThe covariance structure in the LSTM-BEKK framework consists of two key compo-\nnents: the static matrix C and the dynamic component Ct. The static matrix C captures\nlong-term covariance structures, reflecting stable interdependencies among assets over ex-\ntended periods, while the dynamic component Ct adapts to short-term fluctuations and\nnonlinear relationships in asset correlations. This dynamic adaptation is particularly cru-\ncial during periods of financial stress when correlations between assets exhibit abrupt\nshifts. The LSTM\u2019s ability to update Ct in near real-time ensures that the model can\naccount for such changes effectively.\nThe BEKK component, art\u22121r\u2032\nt\u22121 + bHt\u22121, helps stabilize the modeling of the covari-\nance matrix Ht, while retaining the economic interpretability. The parameter a reflects\nthe immediate impact of past shocks, and b represents the persistence of volatility. This\ncombination enables the LSTM-BEKK framework to offer nuanced insights into both\nshort-term and long-term market dynamics, providing a more robust approach to model-\ning financial volatility and correlation structures (Engle, 2002; Nguyen et al., 2022; Liu,\n2019).\nCompared to traditional models, the LSTM-BEKK framework offers significant im-\nprovements. While the Scalar BEKK model is parsimonious and computationally efficient,\nit relies on linear relationships and fixed parameters, limiting its ability to capture evolv-\ning dynamics in financial markets. Similarly, the DCC model introduces flexibility in\nmodeling time-varying correlations but assumes constant dynamics for conditional vari-\nances, which may overlook nonlinear patterns and structural breaks. By contrast, the\nLSTM-BEKK model combines the strengths of both approaches while addressing their\nlimitations. Its integration of LSTM networks enables it to capture the complex, nonlinear\ndependencies that characterize modern financial systems.\nGiven the recursive nature in the construction (11) of the matrix Ht, its dynamics\nmight explode in terms of a matrix norm. Theorem 1 below studies sufficient conditions\nto prevent this issue. To impose these conditions in practice, apart from the condition\na, b \u22650, a + b < 1, it suffices to bound the maximum eigenvalue of CtC\u2032\nt.\nTheorem 1 Fix some matrix norm \u2225\u00b7\u2225and assume that \u2225CtC\u2032\nt\u2225is bounded almost surely\nfor all t. Furthermore, assume that a, b \u22650 and a + b < 1. Then, for any fixed, initial\n9\n\n\n--- Page 10 ---\nH0,\n\u2225E(Hk)\u2225\u22641 \u2212(a + b)k\n1 \u2212a \u2212b M + (a + b)k\u2225H0\u2225,\n(14)\nwhere M > 0 is a finite constant.\nThe proof can be found in the Appendix.\nIn summary, the LSTM-BEKK model represents a significant advancement in multi-\nvariate volatility modeling. By embedding long-term stability through C and introducing\nshort-term flexibility via Ct, the model offers a robust and flexible framework for capturing\npersistent and transitory volatility dynamics. Its capacity to adapt to market conditions\ndynamically positions it as an invaluable tool for applications such as portfolio optimiza-\ntion, systemic risk analysis, and stress testing, paving the way for further innovations in\nfinancial econometrics.\n3\nEstimation Procedure\n3.1\nLikelihood-Based Estimation\nThe LSTM-BEKK model parameters are estimated by minimizing the Negative Log-\nLikelihood (NLL) function, a standard approach in multivariate volatility modeling. As-\nsuming that the de-meaned return vector rt follows a multivariate normal distribution,\nthe log-likelihood based on a training data set of T observations is:\n\u2113(\u03b8) =\nT\nX\nt=1\nlog Lt = \u22121\n2\nT\nX\nt=1\n\u0000n log(2\u03c0) + log |Ht| + r\u2032\ntH\u22121\nt rt\n\u0001\n.\n(15)\nThe parameter set \u03b8 to be estimated includes the lower-triangular matrix C, the scalar\nparameters a and b, and the parameters of the LSTM network.\nFollowing Theorem 1, we impose the constraints a, b \u22650, a + b < 1; we observe that\nthis also promotes numerical stability during estimation.\nThis condition ensures that\nthe effects of past shocks and volatilities decay over time, preventing divergence of the\ncovariance matrix Ht. Additionally, the diagonal elements of C are ensured to be strictly\nnon-zero values to guarantee the positive definiteness of the static component CC\u2032, hence\nHt for all t. We found empirically that it was unnecessary to bound the norm of CtC\u2032\nt.\n3.2\nOptimization Techniques\nGiven the high-dimensional nature of the model and the presence of both static (BEKK)\nand dynamic (LSTM) parameters, efficient optimization techniques are critical to ensure\nnumerical stability and convergence.\n10\n\n\n--- Page 11 ---\nRMSprop Optimization Algorithm.\nTo minimize the NLL function, this study em-\nploys the RMSprop optimizer, a popular choice in deep learning due to its adaptability\nand numerical stability in high-dimensional problems. The update rule for parameter \u03b8\nat iteration k is given by:\n\u03b8k+1 = \u03b8k \u2212\u03b7\ngk\np\nE[g2\nk] + \u03f5\n,\n(16)\nwhere:\n\u2022 \u03b7 is the learning rate,\n\u2022 gk = \u2207\u03b8\u2113(\u03b8k) is the gradient of the NLL function with respect to \u03b8,\n\u2022 E[g2\nk] is the exponentially weighted moving average of the squared gradients,\n\u2022 \u03f5 is a small constant to ensure numerical stability, typically set to 10\u22128.\nInitialization and Hyperparameter Tuning.\nProper initialization of the parameters\nis essential for the stability and convergence of the optimization process. The static matrix\nC is initialized as a lower triangular matrix with non-zero values on the diagonal to ensure\nthe positive definiteness of Ht. The LSTM weights are initialized using standard methods\nsuch as Xavier initialization or He initialization to balance the scale of the input and\noutput gradients.\nThe architecture of the LSTM network is designed to adapt to the complexity of\nthe portfolio, with the hidden size set equal to the input size to maintain consistency in\nfeature representation. The number of hidden layers ranges from three to five, increasing\nas the number of assets in the portfolio grows. To prevent overfitting, dropout rates are\nset between 0.1 and 0.2, with higher values applied in more complex models.\nTo mitigate potential numerical instability arising from computing the determinant\n|Ht| and the inverse H\u22121\nt \u2014especially in high-dimensional settings\u2014the model employs\nCholesky decomposition, which enables more efficient and stable evaluation of the likeli-\nhood function. Furthermore, to avoid issues such as exploding gradients during training,\nregularization techniques including gradient clipping are incorporated into the optimiza-\ntion routine. The convergence of the training process is determined by monitoring the\nrelative change in the negative log-likelihood (NLL) between successive iterations, with\ntermination occurring once the change falls below a predefined threshold (typically 10\u22126).\nAdditionally, early stopping is implemented based on validation performance to guard\nagainst overfitting and promote generalizability. The pseudocode describes the estima-\ntion procedure for LSTM-BEKK is provided in the Appendix.\nIn summary, by leveraging the RMSprop optimizer and employing advanced numer-\nical techniques, the LSTM-BEKK model achieves efficient and stable convergence, even\nin high-dimensional settings. The combination of dynamic learning rates, robust gradi-\nent computation, and careful parameter initialization ensures that the model effectively\ncaptures the complex temporal and nonlinear dependencies inherent in financial markets.\n11\n\n\n--- Page 12 ---\n4\nEmpirical Study\n4.1\nData and Descriptive Statistics\nThe data employed in this study comprises daily log returns for the top 250 publicly\ntraded equities from the United States (U.S.), the United Kingdom (U.K.), and Japan,\nselected based on market capitalization. The data is sourced from Refinitiv data platform:\nhttps://www.refinitiv.com, which aggregates information from key exchanges includ-\ning NASDAQ OMX \u2013 NASDAQ BASIC, the New York Stock Exchange (NYSE), the\nLondon Stock Exchange (LSE), and the Tokyo Stock Exchange (TSE), thereby ensuring\ncomprehensive and high-quality market coverage across major global financial centers.\nThis diversified selection provides a robust basis for evaluating the proposed volatility\nmodeling framework across heterogeneous market environments.\nThe time coverage for each market differs slightly due to variations in trading calen-\ndars and data availability. Specifically, the U.S. dataset spans from March 2014 to De-\ncember 2023, the U.K. dataset from July 2014 to December 2023, and the Japan dataset\nfrom January 2014 to December 2023. All returns are computed as daily log returns and\nthen scaled by 100 to express them in percentage terms.\nThe total number of observations per asset reflects the respective market\u2019s trading\nactivity: the U.S. dataset contains 2,464 observations per stock, the U.K. dataset includes\n2,035 observations, and the Japan dataset provides 2433 observations. To facilitate rigor-\nous model training and evaluation, each return dataset is partitioned into 70% for training,\n15% for validation, and 15% for testing.\n4.1.1\nDescriptive Statistics\nTables 1, 2, and 3 summarize the key characteristics of daily log returns for the three mar-\nkets, including the mean, standard deviation, minimum and maximum values, skewness,\nand kurtosis.\nThe descriptive statistics in Table 1 reveal several important characteristics of the\nU.S. equity market. The average daily return across all assets is effectively zero, reflect-\ning the de-meaned nature of the log return series. The minimum and maximum daily\nreturns\u2014ranging from as low as \u221276.39% to as high as 55.76%\u2014indicate the presence of\nsubstantial market shocks and extreme events during the sample period. The standard\ndeviation of daily returns spans a wide range, with an average of approximately 1.85%,\nand reaching a maximum of 3.82%. These figures suggest considerable variation in risk\nacross different assets. The skewness values range from \u221211.67 to 1.51, with an average\nof \u22120.52, indicating that negative returns tend to occur more frequently than positive\nones\u2014a common characteristic in equity markets. Additionally, the kurtosis values, with\na mean of 16.23 and a maximum as high as 359.77, highlight the presence of heavy tails\n12\n\n\n--- Page 13 ---\nand extreme return distributions. These distributional features emphasize the necessity\nof adopting volatility modeling frameworks that can effectively capture such non-normal\nbehavior in financial time series.\nTable 1: Aggregated Descriptive Statistics of Daily Log Returns (%) for the Top 250 U.S.\nEquities.\nStatistic\nMinimum\nAverage\nMaximum\nMean Return (%)\n-0.006\n-0.000\n0.006\nStandard Deviation (%)\n1.142\n1.849\n3.821\nMinimum Return (%)\n-76.394\n-18.406\n-7.783\nMaximum Return (%)\n6.137\n15.135\n55.761\nSkewness (log returns)\n-11.672\n-0.515\n1.510\nKurtosis (log returns)\n3.544\n16.234\n359.772\nNote: The table summarizes key statistics of daily log returns, expressed as percentages,\nfor the top 250 U.S. equities from March 2014 to December 2023.\nThe statistics are\naggregated across all assets.\nThe descriptive statistics for the U.K. equity market in Table 2 reveal several no-\ntable features that distinguish it from the U.S. market. The mean daily return across\nassets remains near zero, as expected for de-meaned log returns. However, the range of\nobserved returns is significantly wider, with the most extreme negative daily return reach-\ning \u221283.97% and the highest positive return peaking at 87.39%, reflecting the presence\nof substantial outliers and episodic market shocks. Volatility, as indicated by the stan-\ndard deviation, shows an average of 1.94% and a maximum of 4.88%, slightly exceeding\nthose observed in the U.S. dataset. These values suggest that the U.K. market exhibits\nmarginally greater dispersion in daily returns across its top 50 equities. Skewness values\nrange from \u22124.90 to 3.86, with an average of \u22120.38, indicating a tendency for negative\nreturn asymmetry among U.K. equities. More strikingly, the kurtosis statistics are highly\nelevated, with a mean of 19.15 and a maximum of 552.30, far exceeding the Gaussian\nbenchmark of 3. This pronounced leptokurtosis points to the presence of extreme tail\nrisks and emphasizes the importance of adopting volatility models capable of capturing\nheavy-tailed behavior in return distributions.\n13\n\n\n--- Page 14 ---\nTable 2: Aggregated Descriptive Statistics of Daily Log Returns (%) for the Top 250 U.K.\nEquities.\nStatistic\nMinimum\nAverage\nMaximum\nMean Return (%)\n-0.068\n0.000\n0.045\nStandard Deviation (%)\n0.509\n1.937\n4.881\nMinimum Return (%)\n-83.974\n-18.412\n-4.292\nMaximum Return (%)\n3.897\n16.081\n87.385\nSkewness (log returns)\n-4.899\n-0.380\n3.856\nKurtosis (log returns)\n2.465\n19.151\n552.298\nNote: The table summarizes key statistics of daily log returns, expressed as percentages,\nfor the top 250 U.K. equities from January 2014 to December 2023. The statistics are\naggregated across all assets.\nThe descriptive statistics for the Japan equity market as shown in Table 3 reveal a\nmore stable return structure relative to the U.S. and U.K. counterparts. The average daily\nreturn across the top 250 equities is 0.003%, indicating a slightly positive drift in returns\nover the sample period. The observed minimum and maximum returns, at \u221230.54% and\n23.16% respectively, are less extreme than those in the U.K. market, reflecting compara-\ntively lower frequency of outlier events. The average standard deviation of daily returns\nis 1.94%, with a maximum of 3.01%, placing the Japan market in a similar volatility\nrange as the U.K. but slightly above that of the U.S. This suggests a moderate level of\ndaily fluctuations in asset prices, with sufficient variability to warrant dynamic volatility\nmodeling. In terms of distributional asymmetry, the skewness ranges from \u22121.11 to 0.86,\nwith a near-zero average of \u22120.01, implying a more balanced return distribution overall.\nThe kurtosis statistics, with an average of 5.69 and a maximum of 56.40, indicate the\npresence of heavy tails and occasional extreme movements, though less pronounced than\nin the U.K. market. These findings support the need for flexible, heavy-tail-aware volatil-\nity models that can accommodate both moderate skewness and leptokurtic behavior in\nJapan financial data.\n14\n\n\n--- Page 15 ---\nTable 3: Aggregated Descriptive Statistics of Daily Log Returns (%) for the Top 250\nJapan Equities.\nStatistic\nMinimum\nAverage\nMaximum\nMean Return (%)\n-0.004\n0.003\n0.010\nStandard Deviation (%)\n1.333\n1.939\n3.010\nMinimum Return (%)\n-30.544\n-13.010\n-7.085\nMaximum Return (%)\n6.971\n13.008\n23.159\nSkewness (log returns)\n-1.110\n-0.010\n0.856\nKurtosis (log returns)\n1.640\n5.693\n56.401\nNote: The table summarizes key statistics of daily log returns, expressed as percentages,\nfor the top 250 Japan equities from March 2014 to December 2023. The statistics are\naggregated across all assets.\n4.1.2\nImplications for Model Selection\nThese descriptive statistics offer valuable insights into the distributional properties and\nrisk profiles of equity returns across the U.S., U.K., and Japan markets. Although all\nthree markets exhibit near-zero average daily returns, they differ significantly in their\nvolatility levels, skewness, and kurtosis. Notably, the Japan market demonstrates the\nmost balanced return distribution with relatively lower skewness and moderate kurtosis,\nwhereas the U.K. market exhibits the most extreme tail behavior, with exceptionally high\nmaximum kurtosis and skewness values. The U.S. market falls between these two in terms\nof both volatility and tail risk.\nThese differences have important implications for volatility modeling. The presence of\nleptokurtic behavior and negative skewness across all markets signals a departure from the\nnormality assumption. Furthermore, the cross-market variation in volatility magnitudes\nand distributional shapes suggests that a single, rigid modeling framework may not be\nequally effective across different financial environments.\nConsequently, flexible and data-adaptive models, such as the proposed LSTM-BEKK,\nwhich integrate deep learning architectures with econometric structures, are better po-\nsitioned to capture the nonlinearities and heteroscedasticity inherent in global equity\nmarkets. Their capacity to adjust dynamically to distinct distributional patterns and\nstructural complexities makes them particularly well-suited for international applications\nwhere market characteristics vary substantially.\n4.2\nEmpirical Evaluation Framework\nBuilding on the dataset and market characteristics discussed in Section 4.1, this subsection\noutlines the empirical framework employed to evaluate the performance of the proposed\n15\n\n\n--- Page 16 ---\nLSTM-BEKK model.\nOur objective is to examine the model\u2019s capability to capture\nthe dynamics of financial return volatility, both qualitatively and quantitatively, across\nvarying portfolio dimensions and market environments.\nThe empirical strategy consists of two key components. First, we conduct an in-\nsample analysis using low-dimensional portfolios to visualize and compare the time-\nvarying covariance structures estimated by different models.\nThis allows for intuitive\nobservation of how LSTM-BEKK captures diagonal (variance) and off-diagonal (covari-\nance) dynamics relative to traditional approaches.\nSecond, we implement a comprehensive out-of-sample evaluation based on NLL, aim-\ning to rigorously assess the robustness and generalizability of each model. For robust-\nness checks, we construct 500 randomly selected 50-asset portfolios for each market and\nconduct repeated experiments to compare the performance. We apply paired t-tests to\nevaluate the statistical significance of the differences in out-of-sample NLL values between\nLSTM-BEKK and the competing models.\nTo investigate scalability and practical relevance, we further test the models\u2019 perfor-\nmance on the top 100, 175, and 250 equities by market capitalization in each market.\nThese single-run experiments are complemented by Global Minimum Variance (GMV)\nportfolio backtests to assess real-world risk control and capital allocation efficacy. Ad-\nditionally, we apply the Model Confidence Set (MCS) of Hansen et al. (2011) to the\nout-of-sample NLL results, identifying statistically superior models under different confi-\ndence thresholds.\nThis two-stage empirical setup enables us to evaluate the LSTM-BEKK model across\nmultiple dimensions\u2014visual interpretability, statistical robustness, and portfolio-level per-\nformance\u2014thereby offering a comprehensive view of its modeling advantages and practical\nviability.\n4.3\nIn-Sample Visualization: Low-Dimensional Covariance Dy-\nnamics\nTo better understand the in-sample behavior of different multivariate volatility models,\nwe begin our empirical evaluation with a low-dimensional case. Specifically, we construct\na 4-asset portfolio using U.S. equities to visualize the time-varying covariance dynamics\ncaptured by each model. The selected stocks include the two largest U.S. firms by mar-\nket capitalization\u2014MSFT.NB (Microsoft) and AAPL.NB (Apple)\u2014along with two stocks\n(SCHW.N and NEM.N) chosen to exhibit negative pairwise correlations with the market\nleaders. This selection allows us to examine both the variance structure of dominant mar-\nket assets and the model\u2019s ability to capture asymmetric dependence in the off-diagonal\nelements of the covariance matrix.\n16\n\n\n--- Page 17 ---\n4.3.1\nDiagonal Elements: Variance Dynamics\nFigures 1a and 1b compare the estimated variances (i.e., the diagonal elements of the con-\nditional covariance matrix) for MSFT.NB and AAPL.NB, respectively, across the Scalar\nBEKK, DCC, and LSTM-BEKK models. In both cases, all models exhibit broadly similar\nvolatility patterns during tranquil market periods, validating the baseline consistency of\neach specification.\nIt is important to note that in the DCC model, the diagonal elements of the condi-\ntional covariance matrix correspond directly to univariate GARCH(1,1) estimates for each\nasset. As such, these trajectories provide a standard benchmark for marginal volatility\ndynamics.\nSignificant divergence among the models emerges during episodes of market turbu-\nlence. In early 2020, corresponding to the outbreak of the COVID-19 pandemic, volatility\nsurged dramatically across both MSFT.NB and AAPL.NB. During this regime shift, the\nDCC model displays an exaggerated overshooting behavior in variance estimation, sug-\ngesting a delayed and unstable response to sudden structural changes. Unlike full BEKK\nor more adaptive structures, the Scalar BEKK model enforces homogeneity by applying\nthe same a and b parameters across all asset pairs. This design restricts its flexibility and\nessentially imposes a global GARCH-like volatility dynamic on the entire portfolio, which\ncan hinder its ability to capture heterogeneous shock responses across assets. As a result,\nthe model tends to produce smoothed volatility paths that may underreact to localized\nor asset-specific structural shifts.\nThe LSTM-BEKK model, by contrast, demonstrates a desirable combination of\nsmoothness and responsiveness. It aligns closely with Scalar BEKK during normal peri-\nods but adjusts more quickly and moderately to crisis-induced volatility spikes, providing\nmore balanced variance estimates. This behavior highlights the strength of the LSTM\narchitecture in extracting relevant temporal patterns while suppressing short-term noise.\n(a)\nIn-Sample\nVariance\nEstimation\nfor\nMSFT.NB Across Models.\n(b)\nIn-Sample\nVariance\nEstimation\nfor\nAAPL.NB Across Models.\nFigure 1: Volatility dynamics (i.e, the diagonal elements of the covariance matrix) across\nmodels.\nTo better understand these dynamics, Table 4 presents the estimated parameters for\n17\n\n\n--- Page 18 ---\neach model in the four-asset portfolio setting.\nIn particular, the sum a + b serves as\na common proxy for volatility persistence. For Scalar BEKK, the sum reaches 0.984,\nsuggesting a highly persistent volatility process that may lead to sluggish updates in\nrapidly changing environments. The LSTM-BEKK model, by contrast, yields a slightly\nlower persistence at 0.968, striking a balance between flexibility and memory. This subtle\ndifference becomes crucial in capturing abrupt regime shifts such as the COVID-19 shock.\nIn this setting, excessive persistence\u2014as in Scalar BEKK\u2014can hinder the model\u2019s ability\nto respond swiftly, whereas overly reactive models may introduce instability.\nLSTM-\nBEKK thus offers a middle ground, adapting promptly without overfitting to transitory\nnoise.\nIt is worth noting that, while the parameters a and b appear across all three models,\ntheir interpretations differ across modeling frameworks. In the BEKK-type models, a\ngoverns the response to past shocks, and b controls the persistence of past covariances.\nIn the DCC model, the a and b terms govern the evolution of standardized conditional\ncorrelations, rather than the conditional covariances.\nTable 4: U.S.: Parameter Estimates and Persistence for DCC, Scalar BEKK, and LSTM-\nBEKK Models (4 Assets).\nPortfolio Size\nModel\na\nb\na + b\n4\nDCC\n0.042\n0.871\n0.913\nScalar BEKK\n0.033\n0.952\n0.984\nLSTM-BEKK\n0.038\n0.930\n0.968\n4.3.2\nOff-Diagonal Elements: Covariance Dynamics\nFigures 2a and 2b illustrate the estimated covariances for two representative asset pairs:\nMSFT.NB & AAPL.NB and SCHW.N & NEM.N. These pairs are selected based on their\nhistorical sample covariances computed from the training set.\nSpecifically, MSFT.NB\n& AAPL.NB exhibit persistently positive covariance, while SCHW.N & NEM.N display\npredominantly negative covariance values, making them suitable for evaluating the mod-\nels\u2019 ability to capture both positive and negative co-movement patterns. These results\nemphasize the LSTM-BEKK model\u2019s ability to flexibly learn and replicate different types\nof co-movement patterns.\nFor the MSFT-AAPL pair, LSTM-BEKK captures the upward trending correlation\nstructure during bullish markets and the sharp co-movement under crisis conditions (e.g.,\nCOVID-19), consistent with DCC. However, it demonstrates enhanced numerical stabil-\nity and smoother transitions compared to DCC, which again tends to generate extreme\nfluctuations.\nMore importantly, for the SCHW-NEM pair, which shows a structurally negative\ncorrelation, LSTM-BEKK successfully tracks the time-varying negative covariance. Com-\n18\n\n\n--- Page 19 ---\npared to Scalar BEKK and DCC, the LSTM-based model is better able to model the\nreturn divergence during market shocks, without flipping signs or generating erratic out-\nliers. This highlights the flexibility of LSTM-BEKK in accommodating both positive and\nnegative dependencies in multivariate financial data.\n(a) MSFT.NB & AAPL.NB (predominantly\npositive covariance).\n(b) SCHW.N & NEM.N (predominantly neg-\native covariance with occasional reversals).\nFigure 2: Covariance dynamics comparison across models for asset pairs with differing\ncorrelation structures.\nOverall, these low-dimensional visualizations provide compelling empirical evidence\nthat the LSTM-BEKK model not only replicates the well-established volatility dynam-\nics of traditional MGARCH models but also offers greater adaptability to complex and\nheterogeneous covariance structures, particularly under market stress conditions.\n4.4\nAssessing Model Generalization: Out-of-Sample Tests on 50-\nAsset Portfolios\n4.4.1\nExperimental Design and Objective\nThis subsection investigates the out-of-sample performance of the multivariate volatility\nmodels through repeated experiments on medium-sized portfolios. Specifically, we evalu-\nate their generalization and robustness by applying them to randomly generated 50-asset\nportfolios across the three equity markets: the U.S., the U.K., and Japan.\nFor each market, we construct 500 distinct portfolios, each consisting of 50 assets\nchosen randomly. This setting is designed to capture diverse correlation structures and\nvolatility regimes within each market, thereby enabling a thorough assessment of the mod-\nels\u2019 adaptability. All experiments are conducted using the out-of-sample data, ensuring a\nfair evaluation of predictive performance under realistic conditions.\nWe adopt a fixed-parameter evaluation scheme: model parameters are estimated once\nusing the training and validation data and then held fixed throughout the test period. The\ntest performance is measured using NLL, which captures the accuracy of the predicted\ncovariance matrix in explaining the realized return series. To ensure statistical credibility,\n19\n\n\n--- Page 20 ---\nwe conduct 500 runs per market and aggregate the results to analyze mean performance\nand variability across random samples.\nThe same expanding window strategy and test NLL metric are employed in the subse-\nquent high-dimensional evaluation in Section 4.5, where we compare model performance\nacross larger portfolio sizes of 100, 175, and 250 assets. This consistency ensures that\ninsights obtained from the medium-scale experiments generalize meaningfully to more\ncomplex portfolio settings.\n4.4.2\nModel Performance Across Random Portfolios\nTable 5 presents the aggregated out-of-sample NLL values and estimated model parame-\nters for the DCC, Scalar BEKK, and LSTM-BEKK models across 500 randomly selected\n50-asset portfolios in the U.S., U.K., and Japan equity markets. The reported NLL values\nreflect average performance over 500 independent test sets, while the values in parentheses\nrepresent the corresponding standard deviations.\nAcross all three markets, the LSTM-BEKK model consistently achieves the lowest\naverage NLL, indicating superior ability to capture dynamic covariance structures in\nout-of-sample scenarios. In the U.S. market, the LSTM-BEKK model attains a mean\nNLL of 85.031, with the lowest standard deviation of 1.484\u2014outperforming both Scalar\nBEKK (85.278, 1.535) and DCC (86.549, 1.644). This suggests that in addition to better\nin-sample fit, LSTM-BEKK exhibits greater forecast accuracy across varying portfolio\ncompositions.\nIn the Japan market, a similar trend is observed: LSTM-BEKK achieves the best\naverage NLL of 86.832 with a standard deviation of 1.707, again surpassing Scalar BEKK\n(87.214, 1.746) and DCC (87.254, 1.752). These results confirm the model\u2019s robustness in\ncapturing return dynamics even under differing volatility regimes and correlation struc-\ntures.\nThe U.K. market presents a more nuanced case. While LSTM-BEKK still achieves\nthe best mean NLL of 93.328, its standard deviation of 2.479 is marginally higher than\nthat of Scalar BEKK (2.408). This suggests that although LSTM-BEKK performs better\non average, Scalar BEKK may yield more consistent results under certain U.K. specific\nmarket conditions. Nevertheless, the gap in mean NLL remains notable, underscoring\nLSTM-BEKK\u2019s enhanced capacity for learning complex cross-asset relationships.\nTaken together, these findings provide compelling evidence of the generalizability\nand robustness of the LSTM-BEKK model. The model not only delivers the best average\nfit across all markets but also maintains competitive\u2014if not superior\u2014stability across\nrepeated portfolio simulations. This provides strong evidence that LSTM-enhanced co-\nvariance structures can effectively generalize to unseen data, validating their applicability\nin practical financial risk modeling contexts.\n20\n\n\n--- Page 21 ---\nTable 5: Parameter Estimates and NLL for DCC, Scalar BEKK, and LSTM-BEKK Mod-\nels (Portfolio Size = 50).\nMarket\nModel\nNLL\na\nb\nU.S.\nDCC\n86.549\n(1.644)\n0.023\n0.704\nScalar BEKK\n85.278\n(1.535)\n0.008\n0.975\nLSTM-BEKK\n85.031\n(1.484)\n0.008\n0.974\nU.K.\nDCC\n93.758\n(3.005)\n0.013\n0.699\nScalar BEKK\n93.587\n(2.408)\n0.009\n0.971\nLSTM-BEKK\n93.328\n(2.479)\n0.008\n0.978\nJapan\nDCC\n87.254\n(1.752)\n0.010\n0.710\nScalar BEKK\n87.214\n(1.746)\n0.011\n0.934\nLSTM-BEKK\n86.832\n(1.707)\n0.004\n0.991\nNote: Values in parentheses denote standard deviations across 500 portfolios.\n4.4.3\nStatistical Significance Tests\nTo further assess whether the observed improvements in out-of-sample NLL by the LSTM-\nBEKK model are statistically significant, we conduct paired t-tests between LSTM-BEKK\nand the two benchmark models (DCC and Scalar BEKK) across the 500 randomly gen-\nerated 50-asset portfolios for each market. The results are reported in Table 6.\nIn the U.S. market, the LSTM-BEKK model significantly outperforms both bench-\nmarks. The average NLL improvement over DCC is substantial (\u22121.518), with a highly\nsignificant t-statistic of \u221215.326 (p < 0.001), confirming consistent superiority. The im-\nprovement over Scalar BEKK is more modest (\u22120.247) but still statistically significant\n(p = 0.009).\nIn the U.K. market, the difference between LSTM-BEKK and DCC remains signif-\nicant (p = 0.014), albeit at a smaller magnitude (\u22120.430). The comparison with Scalar\nBEKK yields a p-value of 0.094, indicating marginal significance at the 10% level. This\nsuggests that while LSTM-BEKK still shows performance gains, the statistical strength is\nweaker compared to the U.S. case, potentially reflecting heavier tails and increased model\n21\n\n\n--- Page 22 ---\nuncertainty in the U.K. equity returns.\nFor the Japan market, both tests produce highly significant results: LSTM-BEKK\noutperforms DCC and Scalar BEKK with mean NLL improvements of \u22120.422 and \u22120.383,\nrespectively, both with p < 0.01. These results reinforce the robustness of the proposed\nmodel across distinct market environments.\nOverall, the paired t-test analysis confirms that the LSTM-BEKK model\u2019s perfor-\nmance improvements are not only economically meaningful but also statistically signifi-\ncant across most comparisons. This provides strong evidence in favor of its generalization\ncapability and robustness in capturing return dynamics across diverse asset universes.\nTable 6: Paired t-test Results on Test NLL Differences Across 500 Portfolios.\nComparison\nMean NLL Difference\nt-statistic\np-value\nU.S. Market\nLSTM-BEKK \u2212DCC\n-1.518\n-15.326\n<0.000***\nLSTM-BEKK \u2212Scalar BEKK\n-0.247\n-2.587\n0.009***\nU.K. Market\nLSTM-BEKK \u2212DCC\n-0.430\n-2.468\n0.014**\nLSTM-BEKK \u2212Scalar BEKK\n-0.259\n-1.676\n0.094*\nJapan Market\nLSTM-BEKK \u2212DCC\n-0.422\n-3.856\n<0.001***\nLSTM-BEKK \u2212Scalar BEKK\n-0.383\n-3.498\n0.001***\nNote: Negative values indicate LSTM-BEKK achieves lower NLL. Significance levels: *p < 0.1, **p <\n0.05, ***p < 0.01.\n4.4.4\nCross-Market Robustness Analysis\nThe previous analyses across the U.S., U.K., and Japan markets offer a compelling basis to\nevaluate the cross-market robustness of the LSTM-BEKK model. Although the magnitude\nof performance gains varies across markets, the model consistently demonstrates improved\nout-of-sample performance over both the DCC and Scalar BEKK models, as evidenced\nby lower average test NLL values across all 500 portfolio replications.\nIn the U.S. market, where return distributions are relatively less heavy-tailed, the\nLSTM-BEKK model achieves the most pronounced gains, with statistically significant\nimprovements over both benchmarks. In contrast, the U.K. market presents greater mod-\neling challenges due to more extreme kurtosis and skewness, leading to comparatively\nsmaller and less statistically robust gains, particularly against Scalar BEKK. The Japan\nmarket offers a middle ground, where LSTM-BEKK again achieves consistent and statis-\ntically significant outperformance.\n22\n\n\n--- Page 23 ---\nImportantly, these findings highlight the model\u2019s adaptability across heterogeneous\nfinancial environments. Despite differences in market structures, volatility regimes, and re-\nturn characteristics, the LSTM-BEKK model maintains its relative advantage in volatility\nforecasting. This cross-market consistency underscores its potential utility as a general-\npurpose volatility modeling framework for global asset allocation and risk management\napplications.\n4.5\nOut-of-Sample Evaluation on High-Dimensional Portfolios\nTo further evaluate the scalability and generalizability of the proposed LSTM-BEKK\nmodel, this section conducts an out-of-sample assessment on high-dimensional portfolios\nconstructed from the top 100, 175, and 250 equities by market capitalization in each of\nthe three markets: the United States, the United Kingdom, and Japan. These selections\nreflect increasingly complex asset universes and serve as representative high-dimensional\nsettings commonly encountered in institutional portfolio management. Unlike Section 4.4\nwhere repeated sampling of 50-asset portfolios was employed to evaluate robustness and\nconduct t-tests, to reduce computation in the high-dimensional setting, we opt to report\nthe results for a single representative portfolio at each dimensional level.\nBy increasing the portfolio dimension, we aim to assess each model\u2019s ability to scale\nunder rising parameter complexity and intensified correlation structure. The following\nsubsections present a comparative analysis of test NLL values across different dimensional\ntiers and markets, followed by risk-aware backtesting (GMV portfolios) and formal model\nconfidence set (MCS) inference.\n4.5.1\nEmpirical Results for the U.S. Market\nTable 7 reports the out-of-sample NLL values and corresponding parameter estimates for\nthe DCC, Scalar BEKK, and LSTM-BEKK models across high-dimensional U.S. equity\nportfolios with 100, 175, and 250 assets. The results show a clear and consistent advantage\nof the LSTM-BEKK model in terms of model fit.\nAcross all three portfolio sizes, the LSTM-BEKK model achieves the lowest NLL\nvalues: 166.090 (100 assets), 285.557 (175 assets), and 417.614 (250 assets), outperforming\nboth Scalar BEKK and DCC. The margin of improvement becomes more pronounced as\nportfolio dimensionality increases. This trend underscores the ability of the LSTM-BEKK\nframework to scale effectively in high-dimensional.\n23\n\n\n--- Page 24 ---\nTable 7: U.S.: Parameter Estimates and NLL for DCC, Scalar BEKK, and LSTM-BEKK\nModels.\nPortfolio Size\nNLL\na\nb\n100\nDCC\n169.119\n0.019\n0.691\nScalar BEKK\n166.325\n0.011\n0.886\nLSTM-BEKK\n166.090\n0.005\n0.975\n175\nDCC\n291.569\n0.013\n0.700\nScalar BEKK\n288.875\n0.010\n0.891\nLSTM-BEKK\n285.557\n0.003\n0.980\n250\nDCC\n423.776\n0.010\n0.698\nScalar BEKK\n419.853\n0.002\n0.993\nLSTM-BEKK\n417.614\n0.007\n0.980\nThe parameter estimates provide insights into how each model captures volatility\ndynamics.\nThe DCC model consistently yields the highest a values among the three\nmodels across all portfolio sizes, suggesting that it places greater weight on immediate\nreturn shocks when updating its correlation dynamics.\nHowever, its b values remain\nmoderate (around 0.69\u20130.70), reflecting limited persistence relative to the BEKK-type\nmodels.\nIn contrast, the LSTM-BEKK model consistently achieves a desirable balance: it\nexhibits the lower a values (indicating lower sensitivity to noise) and the higher b values\n(reflecting strong volatility persistence), with values close to or exceeding those of Scalar\nBEKK. This highlights the role of the LSTM in capturing nonlinear dependencies and\nlong-memory behavior more effectively than its counterparts.\nOverall, the LSTM-BEKK model demonstrates strong generalization capabilities and\nscalability in high-dimensional settings, providing more stable and accurate volatility\nestimates than traditional econometric models.\n4.5.2\nEmpirical Results for the U.K. Market\nTable 8 presents the out-of-sample NLL values and estimated parameters for the DCC,\nScalar BEKK, and LSTM-BEKK models applied to high-dimensional U.K. equity portfo-\nlios. As in the U.S. market, the LSTM-BEKK model consistently attains the lowest NLL\nvalues across all three portfolio sizes\u2014182.545 (100 assets), 324.577 (175 assets), and\n467.977 (250 assets)\u2014demonstrating its strong generalization capacity and adaptability\nto a different market environment.\n24\n\n\n--- Page 25 ---\nTable 8: U.K.: Parameter Estimates and NLL for DCC, Scalar BEKK, and LSTM-BEKK\nModels.\nPortfolio Size\nNLL\na\nb\n100\nDCC\n184.649\n0.013\n0.669\nScalar BEKK\n183.450\n0.008\n0.932\nLSTM-BEKK\n182.545\n0.008\n0.948\n175\nDCC\n328.327\n0.007\n0.690\nScalar BEKK\n326.875\n0.009\n0.890\nLSTM-BEKK\n324.577\n0.003\n0.984\n250\nDCC\n472.528\n0.007\n0.697\nScalar BEKK\n471.964\n0.004\n0.952\nLSTM-BEKK\n467.977\n0.006\n0.942\nThe parameter patterns echo those observed in the U.S. market, with LSTM-BEKK\nmaintaining relatively low a values and consistently high b values across all dimensions.\nThese estimates reflect the model\u2019s capacity to capture persistent volatility clustering.\nUnlike the U.S. market, however, the U.K. equity return distribution exhibits heavier\ntails and higher kurtosis, increasing the likelihood of extreme return events. This feature\nposes significant challenges to conventional models such as DCC and Scalar BEKK, which\nare grounded in conditional normality assumptions. The LSTM-BEKK model, benefiting\nfrom its deep learning structure and Swish activation dynamics, offers additional flex-\nibility to accommodate these non-Gaussian features, as evidenced by its superior NLL\nperformance.\nInterestingly, the gap between Scalar BEKK and LSTM-BEKK narrows in this mar-\nket, particularly at 250 dimensions. This reflects the relatively strong performance of\nScalar BEKK in moderately heavy-tailed environments, though the LSTM-BEKK model\nstill prevails overall. These results reaffirm the robustness of LSTM-BEKK across both\nmarket regimes and portfolio complexities.\n4.5.3\nEmpirical Results for the Japan Market\nTable 9 presents the parameter estimates and NLL values for the Japan equity market.\nIn line with the results observed in the U.S. and U.K. markets, the LSTM-BEKK model\nconsistently achieves the lowest NLL values across all portfolio sizes. Specifically, for 100,\n175, and 250-asset portfolios, the LSTM-BEKK records NLLs of 162.731, 285.631, and\n417.788, respectively\u2014outperforming both DCC and Scalar BEKK models.\n25\n\n\n--- Page 26 ---\nTable 9: Japan: Parameter Estimates and NLL for DCC, Scalar BEKK, and LSTM-\nBEKK Models.\nPortfolio Size\nNLL\na\nb\n100\nDCC\n164.067\n0.009\n0.697\nScalar BEKK\n163.322\n0.007\n0.931\nLSTM-BEKK\n162.731\n0.002\n0.998\n175\nDCC\n289.456\n0.005\n0.696\nScalar BEKK\n289.320\n0.006\n0.945\nLSTM-BEKK\n285.631\n0.002\n0.993\n250\nDCC\n423.885\n0.004\n0.698\nScalar BEKK\n421.414\n0.003\n0.971\nLSTM-BEKK\n417.788\n0.002\n0.997\nThe Japan market is characterized by moderate volatility persistence and relatively\nlower short-term shock sensitivity compared to the U.K. and U.S. markets. Across all\nportfolio sizes, although the DCC model exhibits the hightest a values, it maintains the\nleast persistence in volatility, with b values around 0.69. The Scalar BEKK model improves\nupon this by increasing both a and b, reflecting a stronger response to market conditions.\nHowever, the LSTM-BEKK model achieves the best balance: it maintains the lowest\na values\u2014indicating robustness to short-term noise\u2014while consistently exhibiting the\nhighest b values, approaching unity. This suggests that LSTM-BEKK excels at capturing\nlong-range dependencies and volatility clustering.\nOverall, the results reaffirm the LSTM-BEKK model\u2019s superior adaptability and mod-\neling capacity, even in markets with more muted short-term volatility shocks but persistent\nstructural dynamics.\n4.5.4\nModel Confidence Set Analysis\nTo evaluate the statistical significance of the observed model performance differences\nacross markets and portfolio sizes, we employ the Model Confidence Set (MCS) procedure\nproposed by Hansen et al. (2011). The MCS framework identifies a set of superior models\n(SSM) from a pool of competing models based on their predictive performance, while\naccounting for sampling uncertainty.\nLet M denote the set of all candidate models. For each model i \u2208M, we define the\nloss function Li,t (in our case, the test negative log-likelihood, NLL) at time t. The loss\ndifferential between models i and j is defined as:\ndi,j,t = Li,t \u2212Lj,t.\n(17)\n26\n\n\n--- Page 27 ---\nThe null hypothesis of equal predictive ability across all models is:\nH0 : \u00b5i,j = E[di,j,t] = 0,\n\u2200i, j \u2208M.\n(18)\nThe MCS procedure performs a sequence of hypothesis tests to iteratively eliminate\nthe worst-performing model until the null hypothesis of equal predictive accuracy can no\nlonger be rejected. At a chosen confidence level (90% in this paper), the surviving models\nconstitute the SSM. Each model is associated with a p-value indicating the probability\nthat it belongs to the set of superior models. Lower p-values reflect weaker statistical\nsupport.\nTable 10 reports the p-values and inclusion indicators of each model across all combi-\nnations of market (U.S., U.K., and Japan) and portfolio dimensions (N = 100, 175, 250).\nA model is included in the 90% MCS if its p-value exceeds 0.10.\nThe results reveal that the LSTM-BEKK model is consistently included in the MCS\nacross all nine experimental settings, with a p-value of 1.000 in every case. This strongly\nsupports its status as the most robust and statistically superior model. In contrast, the\nDCC and Scalar BEKK models are excluded in the majority of settings due to low p-\nvalues. Notably, DCC is only retained once (U.K., N = 250), and Scalar BEKK is never\nretained, highlighting its instability under the MCS test.\nThese findings validate the empirical advantage of the LSTM-BEKK model not only\nin terms of raw performance (e.g., lower NLL) but also under formal statistical scrutiny.\nThe consistent MCS inclusion underscores the reliability and generalizability of its su-\nperior forecasting performance across high-dimensional and heterogeneous financial envi-\nronments.\n27\n\n\n--- Page 28 ---\nTable 10: Model Confidence Set (MCS) Inclusion Based on Test NLL p-values.\nMarket\nPortfolio Size\nModel\np-value\nMCS (90%)\nU.S.\nDCC\n0.031\n\u2717\n100\nScalar BEKK\n0.031\n\u2717\nLSTM-BEKK\n1.000\n\u2713\nU.S.\nDCC\n0.000\n\u2717\n175\nScalar BEKK\n0.000\n\u2717\nLSTM-BEKK\n1.000\n\u2713\nU.S.\nDCC\n0.019\n\u2717\n250\nScalar BEKK\n0.030\n\u2717\nLSTM-BEKK\n1.000\n\u2713\nU.K.\nDCC\n0.032\n\u2717\n100\nScalar BEKK\n0.032\n\u2717\nLSTM-BEKK\n1.000\n\u2713\nU.K.\nDCC\n0.000\n\u2717\n175\nScalar BEKK\n0.000\n\u2717\nLSTM-BEKK\n1.000\n\u2713\nU.K.\nDCC\n0.184\n\u2713\n250\nScalar BEKK\n0.001\n\u2717\nLSTM-BEKK\n1.000\n\u2713\nJapan\nDCC\n0.095\n\u2717\n100\nScalar BEKK\n0.095\n\u2717\nLSTM-BEKK\n1.000\n\u2713\nJapan\nDCC\n0.098\n\u2717\n175\nScalar BEKK\n0.000\n\u2717\nLSTM-BEKK\n1.000\n\u2713\nJapan\nDCC\n0.005\n\u2717\n250\nScalar BEKK\n0.005\n\u2717\nLSTM-BEKK\n1.000\n\u2713\nNote: Models with p-value > 0.10 are retained in the 90% Model Confidence Set (MCS).\n\u2713denotes inclusion, \u2717denotes exclusion.\n4.5.5\nSummary of High-Dimensional Evaluation\nThe high-dimensional out-of-sample evaluation across the U.S., U.K., and Japan markets\nprovides strong evidence of the robustness and adaptability of the LSTM-BEKK model\nin modeling complex volatility structures. Across all three markets and all portfolio sizes\n(N = 100, 175, 250), the LSTM-BEKK model consistently achieves the lowest or near-\n28\n\n\n--- Page 29 ---\nlowest NLL values, demonstrating superior predictive performance relative to the DCC\nand Scalar BEKK models.\nThis advantage becomes increasingly pronounced as portfolio dimensionality in-\ncreases. In particular, the gap between LSTM-BEKK and the traditional models widens\nin the 175- and 250-asset portfolios, indicating that the deep learning-based architecture\nis particularly well-suited to capturing the nonlinearities and higher-order dependencies\npresent in large asset spaces.\nComplementing the NLL results, the Model Confidence Set analysis further reinforces\nthe statistical significance of these findings. At the 90% confidence level, LSTM-BEKK\nis retained in the MCS in all nine experimental configurations, with p-values equal to 1\nthroughout. In contrast, Scalar BEKK and DCC are frequently excluded from the MCS,\nhighlighting their relative instability and inferior forecasting accuracy. This result affirms\nthat the performance gains achieved by LSTM-BEKK are not attributable to chance, but\nreflect meaningful improvements in modeling efficacy.\nIn summary, the empirical and statistical evidence confirm that LSTM-BEKK gener-\nalizes well across markets and scales effectively with portfolio dimensionality. These prop-\nerties make it a compelling alternative to conventional MGARCH models, especially in\nmodern financial applications requiring accurate, stable, and scalable multivariate volatil-\nity estimation.\n5\nGlobal Minimum Variance Portfolio\n5.1\nTheoretical Background\nThe Global Minimum Variance (GMV) portfolio aims to construct a portfolio that mini-\nmizes the overall risk, as measured by the portfolio variance, without considering expected\nreturns. This approach is particularly relevant in volatile financial markets, where accu-\nrate estimation of expected returns can be challenging. The GMV portfolio is defined as\nthe solution to the following optimization problem:\nmin\nw w\u2032Htw,\nsubject to\nw\u20321 = 1,\n(19)\nwhere w is the vector of portfolio weights, Ht is the conditional covariance matrix of asset\nreturns at time t, and 1 is a vector of ones ensuring that the portfolio is fully invested.\nThe optimal weights for the GMV portfolio can be derived as:\nwGMV\nt\n= H\u22121\nt 1\n1\u2032H\u22121\nt 1.\n(20)\nHere, the inverse of the covariance matrix H\u22121\nt\nplays a critical role in determining the\nportfolio weights. Accurate estimation of Ht is therefore essential for constructing the\n29\n\n\n--- Page 30 ---\nGMV portfolio. In this section, we evaluate the performance of the proposed LSTM-\nBEKK model by examining its resulting GMV portfolio.\n5.2\nPerformance Measures\nTo evaluate the performance of the GMV portfolios constructed using the three models\n(DCC, Scalar BEKK, and LSTM-BEKK), we use the following performance measures.\nAnnualized Return (AR).\nThe annualized return is calculated as:\nAR = \u00afr \u00d7 252,\n(21)\nwhere \u00afr represents the mean portfolio return of out-of-sample returns.\nAnnualized Volatility (AV).\nThe annualized volatility is given by:\nAV =\nv\nu\nu\nt\n1\nT \u22121\nT\nX\nt=1\n(rt \u2212\u00afr)2 \u00d7\n\u221a\n252,\n(22)\nMaximum Drawdown (MDD).\nThe maximum drawdown measures the largest de-\ncline in the portfolio value from a peak to a trough:\nMDD = min\nt\u2208[0,T]\n\u0012Vt \u2212maxs\u2208[0,t] Vs\nmaxs\u2208[0,t] Vs\n\u0013\n,\n(23)\nwhere Vt is the portfolio value at time t.\nHigher values of AR are preferred as they indicate stronger portfolio growth, whereas\nlower AV and MDD values are desirable as they correspond to reduced risk exposure and\nenhanced drawdown resilience.\nIn the Appendix, we further assess the performance of GMV portfolios, constructed\nbased on DCC, Scalar BEKK, and LSTM-BEKK, using the commonly used financial tail\nrisk measures Value-at-Risk and Expected Shortfall.\n5.3\nPerformance Analysis of GMV Portfolios\nBuilding upon the high-dimensional evaluation in Section 4.5, we further assess model\nperformance from a portfolio construction perspective. Specifically, we examine the ef-\nfectiveness of the LSTM-BEKK model in generating GMV portfolios across three major\n30\n\n\n--- Page 31 ---\nfinancial markets: U.S., U.K., and Japan. For each market, we utilize the covariance ma-\ntrices estimated from the top 100, 175, and 250 market-capitalization equities to simulate\nhigh-dimensional portfolio settings.\nThe equally weighted (EW) portfolio is used as a baseline due to its simplicity and\nstrong empirical performance. As discussed in DeMiguel et al. (2009), the 1/N alloca-\ntion strategy avoids estimation errors inherent in parametric optimization methods, thus\nproviding stable out-of-sample results. However, EW does not explicitly minimize risk,\nmaking it suboptimal for volatility-sensitive investors.\nIn contrast, GMV portfolios explicitly seek to minimize portfolio variance subject\nto budget constraints. Their effectiveness hinges critically on the quality of the input\ncovariance matrix.\nWe therefore evaluate how each model\u2014DCC, Scalar BEKK, and\nLSTM-BEKK\u2014impacts GMV performance across three dimensions: AR, AV, and MDD.\nThe AR is included for completeness but not emphasized, given its dependence on returns\nrather than risk control.\nTo ensure robustness, all backtests are conducted using only out-of-sample data, i.e.\ntest set. This mirrors real-world usage, where covariance estimates are based only on\nhistorical information and applied to future decisions.\n5.3.1\nU.S. Market Analysis\nTable 11 presents the performance of GMV portfolios in the U.S. equity market based\non the top 100, 175, and 250 stocks by market capitalization. The EW portfolio delivers\nthe highest AR, particularly at N = 100 with 0.124, which supports its role as a strong\nbenchmark. However, its AV consistently exceeds that of all optimized GMV portfolios,\nindicating limited effectiveness in risk control.\nAmong the GMV strategies, the LSTM-BEKK model demonstrates a clear advantage\nin minimizing volatility. It achieves the lowest AV across all portfolio sizes: 0.114 at\nN = 100, 0.112 at N = 175, and 0.111 at N = 250. These values are consistently lower\nthan those produced by both the DCC and Scalar BEKK models, underscoring LSTM-\nBEKK\u2019s robustness in modeling high-dimensional risk structures. Notably, as portfolio\ndimensionality increases, the performance gap in AV widens in favor of LSTM-BEKK,\nhighlighting its scalability.\nA similar pattern emerges in maximum drawdown (MDD) performance. The LSTM-\nBEKK model consistently reduces downside exposure across most portfolio sizes, with\ntotal MDD values of \u22120.154, \u22120.104, and \u22120.161 for N = 100, 175, and 250, respectively.\nCompared to DCC (e.g., MDD as high as \u22120.260 at N = 250) and Scalar BEKK (e.g.,\n\u22120.194 at N = 250), LSTM-BEKK exhibits substantially greater resilience during market\ndownturns.\nIt should be noted that at N = 100, the LSTM-BEKK model records an MDD\n31\n\n\n--- Page 32 ---\nof \u22120.154, which is marginally higher than the EW portfolio\u2019s \u22120.125. However, this\nisolated instance does not undermine the broader trend: LSTM-BEKK demonstrates\nstronger drawdown protection in larger portfolio configurations (N = 175 and N = 250),\nwhere controlling downside risk becomes increasingly challenging.\nThis reinforces its\nutility in real-world portfolio management, particularly in high-dimensional settings where\ntraditional methods tend to deteriorate.\nOverall, these empirical results provide strong evidence that the LSTM-BEKK model\noffers superior risk-adjusted performance in high-dimensional GMV portfolio construction.\nIts consistent outperformance in both AV and MDD metrics suggests that integrating deep\nlearning structures into volatility modeling leads to more stable, resilient, and defensible\nportfolios under real-world market conditions.\nTable 11: U.S.: Performance Comparison of GMV Portfolios: Return, Risk, and Draw-\ndown\nEW\nDCC\nScalar BEKK\nLSTM BEKK\nN=100\nAR\n0.124\n-0.057\n-0.084\n-0.052\nAV\n0.152\n0.131\n0.118\n0.114\nMDD\n-0.125\n-0.186\n-0.166\n-0.154\nN=175\nAR\n0.086\n-0.040\n-0.008\n-0.015\nAV\n0.168\n0.131\n0.120\n0.112\nMDD\n-0.171\n-0.158\n-0.124\n-0.104\nN=250\nAR\n-0.032\n-0.125\n-0.027\n-0.002\nAV\n0.185\n0.136\n0.115\n0.111\nMDD\n-0.244\n-0.260\n-0.194\n-0.161\nNote: Bold values represent the lowest AV and MDD for each portfolio size.\n5.3.2\nU.K. Market Analysis\nTable 12 summarizes the performance of GMV portfolios in the U.K. equity market based\non the top 100, 175, and 250 stocks. The EW portfolio continues to exhibit the highest\nAR across all portfolio sizes; however, this is again accompanied by higher AV, reaffirming\nits limitations as a risk-agnostic strategy.\nAmong the GMV portfolios, the LSTM-BEKK model demonstrates clear superiority\nin volatility minimization. It achieves the lowest AV in every portfolio size: 0.097 at\nN = 100, 0.093 at N = 175, and 0.074 at N = 250, all of which are notably lower than\nthose achieved by DCC and Scalar BEKK. These results reflect the LSTM-BEKK model\u2019s\n32\n\n\n--- Page 33 ---\nenhanced capacity to adapt to complex volatility dynamics and provide stable covariance\nestimates, particularly in the presence of heavy-tailed return distributions observed in the\nU.K. market.\nIn terms of downside risk, the LSTM-BEKK model also performs competitively in\nMDD reduction.\nIt achieves the lowest MDD at both N = 175 (\u22120.150) and N =\n250 (\u22120.130), highlighting its robustness during adverse market periods. While Scalar\nBEKK demonstrates marginal strength at smaller portfolio sizes, the LSTM-BEKK model\nultimately offers the best balance between risk reduction and volatility control in high-\ndimensional settings.\nOverall, the empirical evidence from the U.K. market corroborates the results ob-\ntained in the U.S. case.\nThe LSTM-BEKK model delivers more effective and consis-\ntent volatility management across all tested portfolio sizes, outperforming traditional\nMGARCH models in both AV and MDD.\nTable 12: U.K.: Performance Comparison of GMV Portfolios: Return, Risk, and Draw-\ndown.\nEW\nDCC\nScalar BEKK\nLSTM BEKK\nN=100\nAR\n-0.013\n-0.049\n-0.073\n-0.075\nAV\n0.124\n0.113\n0.101\n0.097\nMDD\n-0.204\n-0.165\n-0.188\n-0.187\nN=175\nAR\n-0.020\n-0.087\n-0.022\n-0.035\nAV\n0.137\n0.107\n0.102\n0.093\nMDD\n-0.238\n-0.204\n-0.159\n-0.150\nN=250\nAR\n-0.027\n-0.080\n-0.026\n-0.037\nAV\n0.142\n0.076\n0.077\n0.074\nMDD\n-0.242\n-0.169\n-0.133\n-0.130\nNote: Bold values represent the lowest AV and MDD for each portfolio size.\n5.3.3\nJapan Market Analysis\nThe empirical results for the Japan equity market, shown in Table 13, offer further insights\ninto the comparative performance of GMV models under a stable but moderately volatile\nmarket environment. As in previous markets, the EW portfolio achieves the highest AR\nacross most portfolio sizes, reaching 0.150 at N = 100. However, this performance comes\nat the cost of higher volatility, with an AV of 0.150, significantly above that of the GMV\nportfolios.\n33\n\n\n--- Page 34 ---\nAmong the GMV models, LSTM-BEKK continues to lead in volatility reduction,\nachieving the lowest AV in 29 out of 50 portfolio combinations. Its average AV across\nthe three sizes\u20140.110, 0.097, and 0.099\u2014is consistently below both Scalar BEKK (0.119,\n0.104, 0.103) and DCC (0.128, 0.111, 0.108). This highlights its capacity to generalize\neffectively across different markets, including the relatively lower-volatility Japan market.\nAlthough Scalar BEKK shows stronger competitiveness here than in the U.S. or U.K.\nmarkets\u2014recording the lowest AV in 19 out of 50 portfolios\u2014LSTM-BEKK remains the\noverall leader.\nIn the N = 25 and N = 30 configurations, the two models perform\nsimilarly, but LSTM-BEKK regains its advantage at higher dimensions such as N = 175\nand N = 250.\nIn terms of MDD, LSTM-BEKK demonstrates robust downside risk control, outper-\nforming all other models at N = 175 and N = 250.\nThe only exception appears at\nN = 100, where Scalar BEKK slightly outperforms with an MDD of \u22120.067 compared to\nLSTM-BEKK\u2019s \u22120.070. Despite this marginal difference, the overall trend indicates that\nLSTM-BEKK offers stronger drawdown resilience across larger portfolio dimensions.\nThe results from the Japan market reinforce the consistent effectiveness of LSTM-\nBEKK in controlling volatility, even under less turbulent market conditions. While Scalar\nBEKK exhibits increased competitiveness compared to other markets, LSTM-BEKK still\nrecords the lowest AV in the majority of cases and achieves superior drawdown protection\nat larger portfolio sizes.\nTaken together with the U.S. and U.K. findings, these results underscore the adapt-\nability and robustness of LSTM-enhanced volatility modeling frameworks.\nBy dy-\nnamically adjusting to changing market regimes and capturing nonlinear dependencies,\nLSTM-BEKK continues to outperform traditional MGARCH models, especially in high-\ndimensional risk-sensitive applications.\n34\n\n\n--- Page 35 ---\nTable 13: Japan: Performance Comparison of GMV Portfolios: Return, Risk, and Draw-\ndown.\nEW\nDCC\nScalar BEKK\nLSTM BEKK\nN=100\nAR\n0.150\n0.165\n0.157\n0.189\nAV\n0.150\n0.128\n0.119\n0.110\nMDD\n-0.090\n-0.086\n-0.067\n-0.070\nN=175\nAR\n0.141\n0.109\n0.119\n0.133\nAV\n0.143\n0.111\n0.104\n0.097\nMDD\n-0.088\n-0.090\n-0.057\n-0.046\nN=250\nAR\n0.127\n0.091\n0.150\n0.155\nAV\n0.137\n0.108\n0.103\n0.099\nMDD\n-0.086\n-0.088\n-0.055\n-0.049\nNote: Bold values represent the lowest AV and MDD for each portfolio size.\n5.4\nSummary of GMV Portfolio Performance\nThe above comprehensive evaluation of GMV portfolios across the U.S., U.K., and Japan\nequity markets confirms the superior performance of the LSTM-BEKK model in mini-\nmizing portfolio risk. Across all three markets and high-dimensional settings (N = 100,\n175, and 250), LSTM-BEKK consistently achieves the lowest annualized volatility (AV)\nin the majority of portfolio combinations, demonstrating its effectiveness in capturing dy-\nnamic, time-varying dependencies in asset returns. This robustness affirms its suitability\nfor investors seeking stable and risk-sensitive portfolio strategies.\nIn the U.S. market, LSTM-BEKK dominates in volatility control and significantly im-\nproves MDD outcomes, especially as portfolio size increases. While its MDD at N = 100\nslightly exceeds that of the EW benchmark, this deviation is outweighed by its pronounced\nadvantages in larger, more volatile configurations. In the U.K. market, LSTM-BEKK once\nagain leads in AV reduction across most configurations, clearly outperforming DCC and\nshowing broader stability than Scalar BEKK. These results reflect the model\u2019s adaptabil-\nity even in markets with heavier tails and higher tail-risk exposure.\nThe Japan market presents a more competitive landscape, where Scalar BEKK\ndemonstrates stronger performance relative to other regions. Nevertheless, LSTM-BEKK\nremains the top-performing model overall, achieving the lowest AV in the majority of\ncases.\nIts ability to maintain both low AV and robust MDD\u2014despite Scalar BEKK\nachieving marginally lower drawdown at N = 100\u2014reinforces its practical value in man-\n35\n\n\n--- Page 36 ---\naging risk across heterogeneous environments.\nTaken together, the results highlight the limitations of traditional models, particularly\ntheir diminished performance in high-dimensional settings and under structural shifts. By\ncontrast, the LSTM-BEKK model integrates the flexibility of deep learning architectures\nwith the interpretability of econometric structures, yielding superior risk profiles under\nout-of-sample conditions. While some marginal trade-offs exist in specific instances, the\noverall dominance of LSTM-BEKK in both volatility and drawdown metrics underscores\nits generalizability and resilience.\nIn summary, the LSTM-BEKK model offers a compelling advancement for GMV\nportfolio construction. It surpasses both the equally weighted benchmark and traditional\ncovariance estimators in risk control across a wide range of market conditions. These\nfindings advocate for the continued exploration of deep learning-based volatility models,\nparticularly in hybrid frameworks that balance statistical rigor with nonlinear predictive\npower in portfolio optimization.\n6\nConclusion\nThis\npaper\nintroduces\na\nnovel\ndeep\nlearning\nenhanced\nmultivariate\nvolatility\nmodel\u2014LSTM-BEKK\u2014that integrates the structural interpretability of econometric\nmodels with the dynamic learning capability of recurrent neural networks. The model is\ndesigned to capture complex nonlinear dependencies and time-varying covariance struc-\ntures in financial markets. Through a comprehensive empirical study, we validate the\nrobustness and effectiveness of LSTM-BEKK across multiple dimensions.\nThe first stage of our analysis focuses on low-dimensional in-sample visualization.\nUsing a 4-asset portfolio from the U.S. market, we assess how well LSTM-BEKK es-\ntimates individual variances and covariances compared to traditional DCC and Scalar\nBEKK models. LSTM-BEKK closely tracks volatility during calm periods while adapt-\ning more promptly during stress episodes (e.g., COVID-19 outbreak), effectively balancing\nsmoothness and responsiveness.\nWe then evaluate the model\u2019s generalization ability through repeated out-of-sample\nexperiments on 500 randomly sampled 50-asset portfolios across the U.S., U.K., and\nJapan equity markets. The LSTM-BEKK model consistently achieves the lowest average\ntest NLL in all markets. Paired t-tests confirm that these improvements are statistically\nsignificant, particularly in markets with heavier tails such as Japan. This underscores the\nmodel\u2019s robustness in heterogeneous return environments.\nTo test high-dimensional scalability, we apply the model to market-wide portfolios of\nthe top 100, 175, and 250 equities by market capitalization in each region. LSTM-BEKK\nmaintains consistent superiority in predictive log-likelihood (NLL) over DCC and Scalar\nBEKK. Model Confidence Set analysis further supports these findings, with LSTM-BEKK\n36\n\n\n--- Page 37 ---\nbeing the only model retained across all nine settings at the 90% confidence level.\nFinally, we evaluate practical implications through global minimum variance portfo-\nlio backtests. Across all markets and portfolio sizes, LSTM-BEKK achieves the lowest\naverage volatility in most configurations and consistently delivers competitive or supe-\nrior performance in maximum drawdown. In high-dimensional settings, the model offers\nrobust tail risk mitigation and smoother risk estimates, essential for institutional asset\nmanagers.\nIn conclusion, LSTM-BEKK offers a powerful and scalable solution to modern volatil-\nity modeling challenges. It combines the theoretical grounding of MGARCH models with\nthe adaptability of deep learning, enabling better predictive accuracy and portfolio risk\nmanagement across diverse financial environments. Future research can extend this frame-\nwork to other deep architectures and explore its integration into broader asset pricing and\nrisk management systems.\n37\n\n\n--- Page 38 ---\nAppendix\nAppendix A: Pseudocode for estimating the LSTM-BEKK\nThe following pseudocode outlines the parameter estimation process for the LSTM-BEKK\nmodel:\nAlgorithm 1 LSTM-BEKK Parameter Estimation Process\nRequire: Initialized parameters: C (static lower triangular matrix), a, b, LSTM weights,\nand Swish activation parameter \u03b2\nRequire: Hyperparameters: learning rate \u03b7, RMSprop settings, and maximum number\nof epochs (max_epochs)\n1: Split data into training, validation, and testing sets\n2: Initialize optimizer (RMSprop) with \u03b7 and regularization parameters\n3: for epoch = 1 to max_epochs do\n4:\nReset cumulative training loss to zero\n5:\nfor each training batch of returns r1:T do\n6:\nfor each time step t = 1 to T do\n7:\nGenerate dynamic component Ct using LSTM: \u02dcCt = LSTM( \u02dcCt\u22121, rt\u22121)\n8:\nCompute conditional covariance Ht:\nHt = CC\u2032 + CtC\u2032\nt + art\u22121r\u2032\nt\u22121 + bHt\u22121\n9:\nAccumulate negative log-likelihood (NLL) for batch:\nNLL = NLL +\n\u0000log |Ht| + r\u2032\ntH\u22121\nt rt\n\u0001\n10:\nend for\n11:\nCompute gradients of NLL with respect to all parameters\n12:\nUpdate parameters using RMSprop\n13:\nend for\n14:\nEvaluate validation loss\n15:\nif validation loss does not improve for patience epochs then\n16:\nApply learning rate scheduler and/or early stopping\n17:\nBreak\n18:\nend if\n19: end for\n20: Return optimized parameters: C, a, b, LSTM weights, and \u03b2\n38\n\n\n--- Page 39 ---\nAppendix B: Proof of Theorem 1\nLet Ft = \u03c3(ys, s \u2264t). We have that Ct, Ht \u2208Ft\u22121, E(rtr\u2032\nt|Ft\u22121) = Ht, and that\nHt = CC\u2032 + CtC\u2032\nt + art\u22121r\u2032\nt\u22121 + bHt\u22121, \u2200t \u22651.\nNote that,\nE(Ht+1|Ft\u22121)\n=\nCC\u2032 + E(Ct+1C\u2032\nt+1|Ft\u22121) + aE(rtr\u2032\nt|Ft\u22121) + bHt\n=\nCC\u2032 + E(Ct+1C\u2032\nt+1|Ft\u22121) + (a + b)Ht.\nBy the assumption on the bounded norm of CtC\u2032\nt, there exists a finite constant M > 0\nsuch that\n\u2225E(Ht+1|Ft\u22121)\u2225\u2264M + (a + b)\u2225Ht\u2225, a.s.\n(24)\nNow, observe that\nE(rt+1r\u2032\nt+1|Ft\u22121) = E\n\u0000E(rt+1r\u2032\nt+1|Ft)|Ft\u22121\n\u0001\n= E(Ht+1|Ft\u22121).\nHence,\nE(Ht+2|Ft\u22121)\n=\nCC\u2032 + E(Ct+2C\u2032\nt+2|Ft\u22121) + aE(rt+1r\u2032\nt+1|Ft\u22121) + bE(Ht+1|Ft\u22121)\n=\nCC\u2032 + E(Ct+2C\u2032\nt+2|Ft\u22121) + (a + b)E(Ht+1|Ft\u22121),\nand thus\n\u2225E(Ht+2|Ft\u22121)\u2225\n\u2264\nM + (a + b)\u2225E(Ht+1|Ft\u22121)\u2225\n\u2264\n(1 + (a + b))M + (a + b)2\u2225Ht\u2225, a.s.\n(25)\nBy induction, it can be seen that, for any k \u22651,\n\u2225E(Ht+k|Ft\u22121)\u2225\n\u2264\n\u00001 + (a + b) + ... + (a + b)k\u22121\u0001\nM + (a + b)k\u2225Ht\u2225\n(26)\n=\n1 \u2212(a + b)k\n1 \u2212a \u2212b M + (a + b)k\u2225Ht\u2225a.s.\n(27)\nLet t = 0 we obtain (14).\nAppendix C: Tail Risk Forecast of GMV Portfolios\nIn this section, we assess the performance of Global Minimum Variance portfolios, gen-\nerated by Scalar BEKK, DCC and LSTM-BEKK in terms of tail risk measures, Value\nat Risk (VaR) and Expected Shortfall (ES). VaR represents the quantile of the return\ndistribution at a specified confidence level \u03b1, while ES quantifies the conditional expec-\ntation of losses exceeding the VaR threshold, offering a more comprehensive view of tail\nrisk. This section analyzes the performance of DCC, Scalar BEKK, and LSTM BEKK\nmodels in forecasting these measures, leveraging theoretical advancements in elicitable\nrisk measures and robust regression.\n39\n\n\n--- Page 40 ---\nQuantile Loss and Joint Loss Framework\nThe evaluation of VaR forecasts is commonly conducted using the quantile loss function,\nwhich assesses the accuracy of predicted quantiles against observed returns. VaR, as a key\nrisk measure, captures the maximum potential loss over a given time horizon at a specified\nconfidence level \u03b1. The quantile regression framework, introduced by Koenker and Bassett\n(1978), provides a robust method for estimating VaR by minimizing deviations at the true\nquantile level. The quantile loss function is formally defined as:\nQloss\u03b1 =\nT\nX\nt=1\n(\u03b1 \u2212I(yt < Q\u03b1\nt ))(yt \u2212Q\u03b1\nt ),\n(28)\nwhere Q\u03b1\nt represents the forecast \u03b1-quantile VaR of the return series yt, and I(\u00b7) is an\nindicator function that takes the value 1 if yt < Q\u03b1\nt , and 0 otherwise. This loss function\npenalizes deviations proportionally, ensuring that the expected loss is minimized when\nthe forecast aligns with the true quantile.\nWhile VaR provides a single quantile-based measure of risk, it does not account for\nlosses exceeding this threshold. To address this limitation, ES has been proposed as a\ncomplementary metric, representing the average loss conditional on exceeding VaR. The\njoint evaluation of VaR and ES forecasts is facilitated by the Asymmetric Laplace (AL)\nloss function, which is strictly consistent for both measures, as demonstrated by Fissler\nand Ziegel (2016). This joint loss function, introduced in Taylor (2019), is given by:\nJointLoss\u03b1 = 1\nT\nT\nX\nt=1\n\u0014\n\u2212log\n\u0012\u03b1 \u22121\nES\u03b1\nt\n\u0013\n\u2212(yt \u2212Q\u03b1\nt )(\u03b1 \u2212I(yt \u2264Q\u03b1\nt ))\n\u03b1 \u00b7 ES\u03b1\nt\n\u0015\n,\n(29)\nwhere ES\u03b1\nt denotes the forecast Expected Shortfall at time t.\nThe AL loss function offers several advantages. First, it enables the simultaneous\nevaluation of VaR and ES, which are jointly elicitable, ensuring that the forecasts align\nwith their theoretical definitions. Second, it penalizes deviations in a manner consistent\nwith the relative importance of VaR and ES in risk management. Together, the quantile\nloss and joint loss functions provide a comprehensive framework for assessing tail risk\nmeasures.\nEmpirical Analysis of Tail Risk Measures\nAs highlighted by Fissler and Ziegel (2016), the joint elicitation of Value-at-Risk (VaR)\nand Expected Shortfall (ES) offers a coherent framework for evaluating tail risk forecasts.\nTable 16 reports quantile loss and joint loss metrics across three markets and various\nportfolio sizes. At the 5% risk level, the LSTM-BEKK model consistently achieves the\nlowest QLoss5% and JointLoss5% across nearly all settings, underscoring its effectiveness\nin modeling moderate tail risk under complex market dynamics.\n40\n\n\n--- Page 41 ---\nAt the more extreme 1% level, however, the picture is more nuanced. While LSTM-\nBEKK continues to perform well, particularly in the U.S. and U.K. markets, the DCC\nmodel occasionally records the lowest QLoss1% (e.g., JP-100), suggesting that its parsimo-\nnious, shock-driven specification may retain advantages when predicting rare tail events.\nIn contrast, LSTM-BEKK appears better suited for managing broader risk exposures by\ncapturing richer temporal dependencies and nonlinearities in return distributions.\nOverall, the LSTM-BEKK model offers a favorable balance between flexibility and\ntail sensitivity, yielding consistently strong performance in joint loss metrics\u2014particularly\nat the 5% level\u2014across diverse portfolio configurations and international markets.\nTable 14: U.S.:Performance Comparison of GMV Portfolios: Quantile Loss and Joint\nLoss.\nPortfolio Size\nQLoss1%\nQLoss5%\nJointLoss1%\nJointLoss5%\n100\nDCC\n8.955\n34.853\n2.374\n1.867\nScalar BEKK\n7.249\n26.332\n2.580\n1.763\nLSTM-BEKK\n6.291\n24.355\n2.417\n1.646\n175\nDCC\n16.032\n49.509\n2.591\n2.194\nScalar BEKK\n9.145\n30.339\n2.454\n1.562\nLSTM-BEKK\n7.114\n28.445\n2.167\n1.529\n250\nDCC\n33.408\n77.311\n3.259\n2.710\nScalar BEKK\n15.142\n49.291\n2.199\n1.778\nLSTM-BEKK\n11.344\n41.198\n2.127\n1.581\nTable 15: U.K.:Performance Comparison of GMV Portfolios: Quantile Loss and Joint\nLoss.\nPortfolio Size\nQLoss1%\nQLoss5%\nJointLoss1%\nJointLoss5%\n100\nDCC\n5.922\n23.222\n2.414\n1.595\nScalar BEKK\n8.811\n30.392\n2.440\n1.636\nLSTM-BEKK\n8.646\n29.371\n2.434\n1.607\n175\nDCC\n10.431\n33.960\n2.390\n1.585\nScalar BEKK\n9.145\n30.339\n2.454\n1.562\nLSTM-BEKK\n9.039\n29.099\n2.445\n1.541\n250\nDCC\n11.148\n29.411\n1.964\n1.442\nScalar BEKK\n8.971\n25.320\n1.898\n1.270\nLSTM-BEKK\n8.223\n24.337\n1.922\n1.237\n41\n\n\n--- Page 42 ---\nTable 16: Japan:Performance Comparison of GMV Portfolios: Quantile Loss and Joint\nLoss.\nPortfolio Size\nQLoss1%\nQLoss5%\nJointLoss1%\nJointLoss5%\n100\nDCC\n7.725\n31.698\n2.397\n1.676\nScalar BEKK\n10.992\n33.243\n3.107\n2.014\nLSTM-BEKK\n12.328\n35.190\n3.333\n2.147\n175\nDCC\n9.019\n30.624\n2.186\n1.617\nScalar BEKK\n7.414\n25.543\n2.346\n1.544\nLSTM-BEKK\n7.865\n25.180\n2.491\n1.605\n250\nDCC\n9.935\n30.143\n2.131\n1.608\nScalar BEKK\n6.485\n25.712\n2.083\n1.466\nLSTM-BEKK\n7.341\n24.794\n2.168\n1.488\nCross-Market Comparison and Implications\nAcross all three markets, the LSTM-BEKK model consistently outperforms traditional\nMGARCH models in terms of predictive accuracy and adaptability to high-dimensional\nsettings.\nThis is reflected in its uniformly lower out-of-sample negative log-likelihood\n(NLL) values across all portfolio sizes in the U.S., U.K., and Japan, confirming its superior\nability to model time-varying covariance structures more effectively than both the DCC\nand Scalar BEKK models.\nThe LSTM-BEKK model demonstrates notable robustness across market-specific con-\nditions. In the U.K. market, where extreme return events are more frequent, traditional\nmodels suffer from underestimating tail risks. LSTM-BEKK, by contrast, accommodates\nthese dynamics through its flexible architecture and achieves better tail-sensitive metrics\nsuch as joint loss at the 5% level. In the U.S. market, the model achieves the lowest average\nvolatility and maximum drawdown in GMV portfolio tests, supporting its effectiveness\nin minimizing downside risk. Even in scenarios where DCC performs competitively in\nextreme quantile loss (e.g., at 1% thresholds), LSTM-BEKK maintains stronger overall\njoint performance across broader risk metrics.\nOverall, these results affirm the advantages of integrating deep learning with struc-\ntured econometric modeling. The LSTM-BEKK framework not only offers superior sta-\ntistical fit and predictive performance, but also generalizes well across heterogeneous\nmarket regimes.\nIts ability to balance responsiveness to market shocks with long-run\nstability makes it a promising tool for risk management, volatility forecasting, and high-\ndimensional portfolio construction.\n42\n\n\n--- Page 43 ---\nConclusion\nOverall, the LSTM-BEKK model demonstrates its effectiveness in capturing multivariate\nvolatility dynamics across different financial markets.\nIts ability to incorporate deep\nlearning techniques enables superior predictive accuracy, making it a valuable tool for risk\nmanagement, portfolio optimization, and stress testing. By integrating both econometric\nand deep learning methodologies, the LSTM-BEKK model provides a more flexible and\naccurate representation of financial market volatility, paving the way for further research\ninto hybrid modeling approaches.\nReferences\nAielli, G. P. (2013).\nDynamic conditional correlation: On properties and estimation.\nJournal of Business & Economic Statistics, 31(3):282\u2013299.\nAsai, M., McAleer, M., and Yu, J. (2006). Multivariate stochastic volatility: A review.\nEconometric Reviews, 25(2-3):145\u2013175.\nBauwens, L., Laurent, S., and Rombouts, J. V. K. (2006). Multivariate GARCH models:\nA survey. Journal of Applied Econometrics, 21(1):79\u2013109.\nBauwens, L. and Otranto, E. (2020). Nonlinearities and regimes in conditional correlations\nwith different dynamics. Journal of Econometrics, 217(2):496\u2013522.\nBollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. Journal\nof Econometrics, 31(3):307\u2013327.\nBollerslev, T., Engle, R. F., and Wooldridge, J. M. (1988). A capital asset pricing model\nwith time-varying covariances. Journal of Political Economy, 96(1):116\u2013131.\nCaporin, M. and McAleer, M. (2008).\nScalar BEKK and indirect DCC.\nJournal of\nForecasting, 27(6):537\u2013549.\nDeMiguel, V., Garlappi, L., and Uppal, R. (2009).\nOptimal versus naive diversifica-\ntion: How inefficient is the 1/n portfolio strategy? The Review of Financial Studies,\n22(5):1915\u20131953.\nEngle, R. F. (1982). Autoregressive Conditional Heteroscedasticity with Estimates of the\nVariance of United Kingdom Inflation. Econometrica, 50(4):987\u20131007.\nEngle, R. F. (2002).\nDynamic conditional correlation: A simple class of multivariate\nGARCH models. Journal of Business & Economic Statistics, 20(3):339\u2013350.\nEngle, R. F. and Kelly, B. (2012).\nDynamic equicorrelation.\nJournal of Business &\nEconomic Statistics, 30(4):384\u2013397.\n43\n\n\n--- Page 44 ---\nEngle, R. F. and Kroner, K. F. (1995). Multivariate simultaneous Generalized ARCH.\nEconometric Theory, 11(1):122\u2013150.\nFang, Y., Liu, L., and Liu, J. (2015). A dynamic double asymmetric copula generalized\nautoregressive conditional heteroskedasticity model: application to China\u2019s and US\nstock market. Journal of Applied Statistics, 42(2):327\u2013346.\nFissler, T. and Ziegel, J. F. (2016). Higher order elicitability and Osband\u2019s principle. The\nAnnals of Statistics, 44(4):1680\u20131707.\nFrancq, C. and Zakoian, J.-M. (2019). GARCH Models: Structure, Statistical Inference,\nand Financial Applications. John Wiley & Sons, Hoboken, NJ, 2nd edition.\nFrancq, C. and Zako\u00efan, J.-M. (2012). QML estimation of a class of multivariate asym-\nmetric GARCH models. Econometric Theory, 28(1):179\u2013206.\nGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press.\nHafner, C. M., Laurent, S., and Violante, F. (2017). Weak diffusion limits of dynamic\nconditional correlation models. Econometric Theory, 33(3):691\u2013716.\nHafner, C. M. and Preminger, A. (2009). Asymptotic theory for a factor GARCH model.\nEconometric Theory, 25(2):336\u2013363.\nHafner, C. M. and Rombouts, J. V. (2007). Semiparametric multivariate volatility models.\nEconometric Theory, 23(2):251\u2013280.\nHansen, P. R., Lunde, A., and Nason, J. M. (2011). The Model Confidence Set. Econo-\nmetrica, 79(2):453\u2013497.\nHochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computa-\ntion, 9(8):1735\u20131780.\nKoenker, R. and Bassett, G. (1978). Regression quantiles. Econometrica: journal of the\nEconometric Society, pages 33\u201350.\nKu, Y.-H. H. (2008). Student-t distribution based VAR-MGARCH: an application of\nthe DCC model on international portfolio risk management.\nApplied Economics,\n40(13):1685\u20131697.\nLai, Y.-S. and Sheu, H.-J. (2011). On the importance of asymmetries for dynamic hedging\nduring the subprime crisis. Applied Financial Economics, 21(11):801\u2013813.\nLedoit, O. and Wolf, M. (2012).\nNonlinear shrinkage estimation of large-dimensional\ncovariance matrices. Annals of Statistics, 40(2):1024\u20131060.\nLedoit, O. and Wolf, M. (2015). Spectrum estimation: A unified framework for covariance\nmatrix estimation and PCA in large dimensions.\nJournal of Multivariate Analysis,\n139:360\u2013384.\n44\n\n\n--- Page 45 ---\nLiu, Y. (2019). Novel volatility forecasting using deep learning\u2013long short term memory\nrecurrent neural networks. Expert Systems with Applications, 132:99\u2013109.\nMatsui, M. and Pedersen, R. S. (2022). Characterization of the tail behavior of a class\nof BEKK processes: A stochastic recurrence equation approach. Econometric Theory,\n38(1):1\u201334.\nMcAleer, M., Chan, F., Hoti, S., and Lieberman, O. (2008). Generalized autoregressive\nconditional correlation. Econometric Theory, 24(6):1554\u20131583.\nNguyen, T.-N., Tran, M.-N., and Kohn, R. (2022). Recurrent Conditional Heteroskedas-\nticity. Journal of Applied Econometrics, 37(5):1031\u20131054.\nScherrer, W. and Ribarits, E. (2007). On the parametrization of multivariate GARCH\nmodels. Econometric Theory, 23(3):464\u2013484.\nSilvennoinen, A. and Ter\u00e4svirta, T. (2009). Multivariate GARCH models. Handbook of\nFinancial Time Series, pages 201\u2013229.\nTaylor, J. W. (2019). Forecasting value at risk and expected shortfall using a semipara-\nmetric approach based on the asymmetric Laplace distribution. Journal of Business &\nEconomic Statistics, 37(1):121\u2013133.\nTaylor, S. J. (1994). Modeling stochastic volatility: A review and comparative study.\nMathematical Finance, 4(2):183\u2013204.\n45\n",
    "pages": 45
  },
  {
    "filename": "Compounding effects in EFTs.pdf",
    "text": "\n\n--- Page 1 ---\narXiv:2504.20116v1  [q-fin.ST]  28 Apr 2025\nCompounding E\ufb00ects in Leveraged ETFs:\nBeyond the Volatility Drag Paradigm\u2217\nChung-Han Hsieh1, Jow-Ran Chang1, and Hui Hsiang Chen1\n1Department of Quantitative Finance, National Tsing Hua University, Hsinchu, Taiwan, 30004\nAbstract\nA common belief is that leveraged ETFs (LETFs) su\ufb00er long-term performance decay due to\nvolatility drag. We show that this view is incomplete: LETF performance depends fundamentally\non return autocorrelation and return dynamics. In markets with independent returns, LETFs\nexhibit positive expected compounding e\ufb00ects on their target multiples. In serially correlated\nmarkets, trends enhance returns, while mean reversion induces underperformance. With a uni-\n\ufb01ed framework incorporating AR(1) and AR-GARCH models, continuous-time regime switching,\nand \ufb02exible rebalancing frequencies, we demonstrate that return dynamics\u2014including return au-\ntocorrelation, volatility clustering, and regime persistence\u2014determine whether LETFs outper-\nform or underperform their targets. Empirically, using about 20 years of SPDR S&P 500 ETF\nand Nasdaq-100 ETF data, we con\ufb01rm these theoretical predictions. Daily-rebalanced LETFs\nenhance returns in momentum-driven markets, whereas infrequent rebalancing mitigates losses\nin mean-reverting regimes.\n1\nIntroduction\nExchange-traded funds (ETFs) have transformed the investment landscape, o\ufb00ering investors cost-\ne\ufb00ective, liquid, and diversi\ufb01ed exposure to various asset classes, e.g., see Malkiel and Radisich\n(2001), Ben-David et al. (2017).\nAs of December 2023, global ETF assets under management\nsurpassed $11.5 trillion, driven by an annual compound growth rate of 18.9% over the preceding\n\ufb01ve years.1 See also\nLeveraged ETFs (LETFs), specialized products within this market, typically implement a con-\nstant daily leverage strategy, e.g., see Trainor Jr and Baryla Jr (2008), Leung and Santoli (2016),\nensuring that leverage exposure is reset at the end of each trading session. For example, a 2x LETF\nseeks to deliver twice the daily return of its benchmark index. In practice, this leverage is typically\nachieved through derivatives such as total return swaps (TRS) and futures contracts; Leung and\nSantoli (2016).\n\u2217This manuscript is a preprint and is currently under review for publication.\n1According to the survey published by PwC (ETFs 2028: Shaping the future), assets under management of global\nETFs experienced a compound annual growth rate (CAGR) of 18.9% over the last \ufb01ve years. Moreover, they increased\nby over 25% since December 2022, reaching a new milestone of nearly $11.5 trillion by the end of 2023.\n1\n\n\n--- Page 2 ---\nDespite their rapid growth, LETFs remain controversial due to concerns over long-term perfor-\nmance decay from volatility drag, which refers to the phenomenon where the compounded (geomet-\nric) return of an LETF is less than its arithmetic average return due to the e\ufb00ects of volatility; e.g.,\nsee Avellaneda and Zhang (2010), Trainor (2012), Tang and Xu (2013). See also U.S. Securities\nand Exchange Commission (2009) for an alert announcement from SEC regarding the riskiness of\nLETFs. Prior studies have primarily focused on volatility drag as the dominant driver of LETF\nunderperformance, e.g., see Jarrow (2010), Charupat and Miu (2011), Trainor (2017), often em-\nphasizing the negative e\ufb00ects of rebalancing on long-term returns.\nHowever, we argue that this conventional view is incomplete. The literature has placed less\nemphasis on aspects of return behavior that fundamentally alter LETF performance dynamics.\nFor example, Lu et al. (2009) showed empirically that LETFs can meet their leverage targets over\nhorizons shorter than a month but may deviate when held beyond a quarter. Carver (2009) pointed\nout that the long-term returns of high-leveraged ETFs may decay toward zero, as indicated by the\nconcept of growth-optimal portfolios. Further studies, such as Guedj et al. (2010), demonstrated\nthat LETF investors could incur losses even when the underlying index exhibits positive returns,\nemphasizing the risks of long-term LETF holdings. Recently, Dai et al. (2023) studied an optimal\nrebalancing model for LETFs under market closure and frictions. Yet these analyses fail to fully\ncharacterize the mechanism driving LETF performance.\nThroughout this paper, we use the term return dynamics to refer broadly to the time-series\nstructure of returns\u2014including independent returns, serial dependence (e.g., autocorrelation), volatil-\nity clustering (e.g., GARCH e\ufb00ects), and regime-driven variation in drift and volatility. This en-\ncompasses both stationary i.i.d. settings and more complex dynamic models such as AR-GARCH\nand regime-switching processes\nWe demonstrate that return autocorrelation and return dynamics play a primary role in driving\ncompounding deviations under realistic return dynamics. Speci\ufb01cally, in markets with independent\nreturns, we show that LETFs exhibit positive expected compounding e\ufb00ects to their target multi-\nples, contrary to conventional wisdom. In serially correlated markets, trends enhance returns while\nmean reversion induces underperformance. These \ufb01ndings challenge the established paradigm that\nvolatility drag alone dictates LETF performance.\nOur theoretical framework and empirical validation over approximately 20 years, spanning\nmultiple LETFs and market crises, con\ufb01rm that LETF performance cannot be fully explained by\nvolatility drag alone. Rather, it is fundamentally in\ufb02uenced by return autocorrelation, volatility\nclustering, and regime persistence\u2014factors less emphasized in prior research that relies solely on\nrealized historical volatility. This comprehensive analysis resolves the apparent contradictions in\nexisting literature and provides a more nuanced understanding of when LETFs outperform or\nunderperform their targets under various market conditions.\n1.1\nKey Contributions and Paper Organization\nThe key contributions of this paper are summarized as follows:\n\u2022 Return Dynamics as Primary Driver of LETF Performance: We demonstrate that return\nautocorrelation and return dynamics, not simply volatility drag, play a primary role in driving\n2\n\n\n--- Page 3 ---\ncompounding deviations under realistic return dynamics.\nOur analysis provides su\ufb03cient\nconditions for LETF outperformance in trending markets and underperformance in oscillating\nregimes, resolving contradictions in previous empirical \ufb01ndings.\n\u2022 Uni\ufb01ed Theoretical Framework: We develop a uni\ufb01ed framework incorporating discrete-time\nand continuous-time regime switching dynamics, allowing precise characterization of LETF\nbehavior across diverse market conditions.\n\u2022 Empirical Validation: Using approximately 20 years of historical data from S&P 500 ETF\n(Ticker: SPY) and Nasdaq-100 ETF (Ticker: QQQ), we demonstrate that the associated LETF\nperformance systematically outperforms in momentum regimes and underperforms during\nmean-reverting periods.\n\u2022 Practical Guidance for Investors: Our \ufb01ndings reveal that daily-rebalanced LETFs are op-\ntimal in momentum markets, while weekly/monthly rebalancing reduces losses in mean-\nreverting conditions.\nThis insight is critical for hedge funds, portfolio managers, and in-\ndividual traders optimizing leveraged positions.\nThe remainder of this paper is organized as follows. Section 2 develops the theoretical framework\nfor analyzing the compounding e\ufb00ect under di\ufb00erent return structures. Section 3 presents both\ntheoretical analysis and simulations to evaluate LETF performance. Subsequently, in Section 4, we\nvalidate our theoretical \ufb01ndings against historical data from SPY and QQQ, covering multiple market\nregimes such as the 2008 \ufb01nancial crisis and the COVID-19 pandemic. Each section integrates\ntheoretical insights with practical implications, and concludes in Section 5.\n2\nFramework for Analyzing LETF Behavior\nThis section introduces the concept of compounding e\ufb00ect, which plays a central role in understand-\ning LETF performance deviations.\n2.1\nCompounding E\ufb00ect on LETF\nSince LETFs rely on daily rebalancing to maintain a \ufb01xed leverage ratio, they re\ufb02ect the target\nleverage only on a daily basis. Over longer holding periods, compounding e\ufb00ects cause the cu-\nmulative returns of LETFs to deviate from their expected multiple, a phenomenon extensively\ndocumented in prior studies, e.g., see Avellaneda and Zhang (2010). This deviation reveals the\nimportance of understanding LETF performance beyond short-term horizons.\nExample 2.1 (Daily Compounding and E\ufb00ective Leverage). Consider an index that rises by 6%\non the \ufb01rst day and falls by 4% the next day; the two-period cumulative return is 1.76%. Now, if\nan investor holds a 2x leveraged ETF based on this index, they will initially gain 12% in returns\non the \ufb01rst day but then experience an 8% loss on the second. This compounding e\ufb00ect reduces\nthe e\ufb00ective leverage ratio to 3.04%/1.76% = 1.73, signi\ufb01cantly below the intended leverage of 2.\nThis example highlights how LETFs can deviate from their target leverage over multiple periods\ndue to compounding, even when the underlying index experiences modest \ufb02uctuations.\n3\n\n\n--- Page 4 ---\nBelow, we de\ufb01ne the compounding e\ufb00ect as the di\ufb00erence between the cumulative return of an\nLETF and the corresponding multiple of the underlying ETF\u2019s cumulative return. Unlike prior\nstudies that focus solely on negative deviations and label them as volatility drag, see Avellaneda\nand Zhang (2010), Trainor Jr and Carroll (2013), we adopt a broader perspective by recognizing\nthat the compounding e\ufb00ect can be either positive or negative, depending on market conditions.\nDe\ufb01nition 2.1 (Compounding E\ufb00ect). Let \u03b2 \u2208Z \\ {0} denote the leverage ratio, and let XLETF\nt\nand XETF\nt\nrepresent the returns at time t for the LETF and the corresponding benchmark ETF,\nrespectively.\nThen, the compounding e\ufb00ect over an n-period horizon, call it CEn, is de\ufb01ned as\nthe di\ufb00erence between the cumulative return of the LETF and \u03b2 times the cumulative return of\nthe ETF:\nCEn := RLETF,\u03b2\nn\n\u2212\u03b2RETF\nn\n,\n(1)\nwhere RLETF,\u03b2\nn\n= Qn\nt=1(1 + XLETF\nt\n) \u22121 and RETF\nn\n= Qn\nt=1(1 + XETF\nt\n) \u22121 represent the n-period\ncumulative returns of the LETF with leverage ratio \u03b2 and the underlying benchmark ETF, respec-\ntively.\nRemark 2.1. A positive compounding e\ufb00ect implies that the LETF outperforms its target leverage\nmultiple, while a negative compounding e\ufb00ect implies underperformance. The returns in De\ufb01ni-\ntion 2.1 can represent daily, weekly, monthly, or other periodic returns, depending on the rebalanc-\ning frequency. Common values for the leverage ratio in practice include \u03b2 \u2208{\u22122, \u22121, 2, 3}, see also\nSections 3 and 4. When \u03b2 is negative, these LETFs are called inverse ETFs.\n2.2\nFees and Tracking Error Considerations\nReal-world LETFs incur annual expense fees, which can signi\ufb01cantly impact performance over\ntime. ETF fees are typically much smaller than LETF fees and tracking errors. Although ETF\nfees exist, they typically contribute only a marginal correction relative to the dominant e\ufb00ects in\nLETF performance. Henceforth, we take the ETF\u2019s daily return to be XETF\nt\n= Xt. The LETF\u2019s\ndaily return is modi\ufb01ed to include fees and tracking errors:\nXLETF\nt\n= \u03b2Xt \u2212f + et,\nwhere \u03b2 is the leverage ratio, f is the daily cost (incorporating expense ratio and transaction costs,\ne.g., f \u2248\n1\n252(fexpense + ftransaction), and et is a zero-mean tracking error with variance \u03c4 2.\nTheorem 2.1 (Expected Compounding E\ufb00ect Approximation). Let \u03b2 \u2208Z \\ {0}. Assume that the\nunderlying daily returns {Xt}t\u22651 is strictly stationary with mean \u00b5 = E[Xt] and autocorrelation\n\u03b3k = E[XtXt+k]. Let f denote the daily fee, and {et} be the tracking error and |Xt|, |f|, |et| \u2264M,\nfor some su\ufb03ciently small M \u226a1 with E[et] = E[etXs] = E[etes] = 0 for t \u0338= s.\nThen, the\n4\n\n\n--- Page 5 ---\ncompounding e\ufb00ect over n days satis\ufb01es\nE[CEn] = \u2212nf +\nn\u22121\nX\nk=1\n(n \u2212k) (\u03b2(\u03b2 \u22121)\u03b3k \u22122\u03b2f\u00b5) +\n\u0012n\n2\n\u0013\nf 2 + O(n3M3).\nProof. See Appendix A.1.\nRemark 2.2. (i) Theorem 2.1 shows that the expected compounding e\ufb00ect depends on autocor-\nrelation E[XtXs] = \u03b3|t\u2212s| rather than just volatility. If there are no fees and no tracking error,\ni.e., f = et = 0, then the \ufb01rst-order term vanishes. We obtain E[CEn] \u2248Pn\u22121\nk=1(n \u2212k)\u03b2(\u03b2 \u22121)\u03b3k,\nshowing that autocorrelation, not volatility, drives expected compounding; see Lemmas 3.1 and 3.2.\nMoreover, under the i.i.d. model \u03b3k = 0 for k \u22651, hence E[CEn] = \u2212nf, which is strictly negative\nonly because of costs. (ii) If there are nonzero fees f > 0 and nonzero tracking error et, then the\nfees contribute a \ufb01rst-order negative term \u2212nf, and the second-order correction further adjusts\nthe compounding e\ufb00ect. For realistic (small) values of f and \u03c4, these corrections will dampen the\ncompounding e\ufb00ect or even reverse its sign if costs are su\ufb03ciently high.\n3\nCompounding E\ufb00ect Analysis: A Revisit\nThis section presents a comprehensive analysis of the compounding e\ufb00ect, examining how changes\nin key parameters\u2014leverage ratio, volatility, and rebalancing frequency\u2014a\ufb00ect LETF performance.\nBy exploring these factors across independent, serially correlated return scenarios, as well as\ncontinuous-time regime switching models, we aim to provide actionable insights for managing\nLETFs under varying market conditions. To isolate the compounding e\ufb00ect, we henceforth as-\nsume there are no fees and no tracking error.\n3.1\nSerially Independent Returns Case: A Discrete-Time Benchmark\nWe \ufb01rst analyze the compounding e\ufb00ect under the assumption of serially independent returns,\nrepresenting a baseline scenario in an idealized market. Speci\ufb01cally, we consider the case where\nthe returns of the underlying ETF, XETF\nt\n, are independent, with a common mean \u00b5 \u2208R and\nvariance \u03c32 > 0. The returns of the LETF, XLETF\nt\n, are assumed to perfectly track the underlying\nETF with a leverage ratio \u03b2 \u2208Z\\{0} with no fees. Notably, due to the limited liability for investors\nregarding ETFs and LETFs, we have \u03b2\u00b5 > \u22121 in practice, i.e., the LETF does not collapse. Under\nthese circumstances, the following lemma indicates that the expected compounding e\ufb00ect is always\nnonnegative, regardless of the leverage ratio or the holding period.\nLemma 3.1 (Nonnegative Expected Compounding E\ufb00ect). Let {Xt}t\u22651 be a sequence of indepen-\ndent returns with mean E[Xt] = \u00b5 and let \u03b2 \u2208Z \\ {0, 1} be the leverage ratio with \u03b2\u00b5 > \u22121. Then,\nthe expected compounding e\ufb00ect over an n-period horizon satis\ufb01es\nE [CEn] = [(1 + \u03b2\u00b5)n \u22121] \u2212\u03b2[(1 + \u00b5)n \u22121] \u22650 for all n \u22651.\n(2)\nMoreover, if \u00b5 = 0, then E [CEn] = 0 for all n \u22651.\n5\n\n\n--- Page 6 ---\nProof. See Appendix A.\nRemark 3.1. Lemma 3.1 implies that for independent returns, LETFs tend to outperform expec-\ntations regardless of the leverage ratio and holding period. This is also evident from the subsequent\nsimulation results in Section 4. If leverage is not allowed, i.e., \u03b2 = 1, then CEn = 0 with probability\none. Lastly, while the lemma proves this result for integer values of the leverage ratio \u03b2, in theory,\nit can take any real value; however, practical trading restricts \u03b2 to integers.\n3.1.1\nMonte Carlo Simulations with Independent Return Model.\nTo validate our result, we use daily adjusted closing price data for SPY, an ETF that tracks the\nS&P 500 index, spanning from January 1993 to December 2023 with about a total of 7,800 trading\ndays.\nSPY is chosen for its broad market representation and high liquidity, making it an ideal\ncandidate for evaluating LETF performance. Using this data, we estimate the daily returns and\ntheir standard deviation, which is approximately 1%.\nCompounding E\ufb00ect with Various Leverage Ratios. To understand how leverage ratios \u03b2 a\ufb00ect\nthe expected compounding e\ufb00ect E[CEn], we assume that the daily returns are normally distributed,\nwith an annualized mean ranging from -20% to 20%. For each simulation, we independently draw\ndaily return samples from this distribution to generate 100,000 return paths over one year across\nvarious combinations.2\nThe theoretical and simulated results, shown in Figure 1, demonstrate that LETFs generally\nhave a positive expected compounding e\ufb00ect across various leverage levels, indicating that LETFs\ngenerally outperform their underlying target on average. From the \ufb01gure, we also see that the com-\npounding e\ufb00ect diminishes as expected returns \u00b5 approach zero, and it becomes more pronounced\nwhen it deviates signi\ufb01cantly from zero. These \ufb01ndings are consistent with Lemma 3.1, which pre-\ndicts a positive expected compounding e\ufb00ect for independent returns. Moreover, consistent with\nexisting literature, e.g., Trainor Jr and Carroll (2013), Abdou (2017), we see that LETFs with\nhigher leverage ratios exhibit a stronger compounding e\ufb00ect.\nCompounding E\ufb00ect with Di\ufb00erent Levels of Volatility. After examining the impact of di\ufb00erent\nleverage levels \u03b2 on the compounding e\ufb00ect, we investigate how varying volatility levels \u03c3 in\ufb02uence\nthe expected compounding e\ufb00ect E[CEn]. Speci\ufb01cally, we simulate scenarios with daily volatility\nlevels of 0.5%, 1%, and 1.5%, where the 1% level corresponds to the volatility estimate derived\nfrom SPY return data mentioned in Section 3.1.1. The other two levels represent adjustments of 0.5\nstandard deviations above and below this baseline.\nThe results, shown in Figure 2, indicate that when volatility changes, the expected compounding\ne\ufb00ect remains relatively stable. This suggests that, for ETFs with similar expected returns, the\nexpected compounding e\ufb00ect exhibits consistent behavior despite variations in volatility.3 However,\nas we shall see later in the case of serially correlated returns, the volatility level would impact the\ncompounding e\ufb00ect.\n2We consider one unit of leveraged ETF paired with \u03b2 (its corresponding leverage ratio) unit of ETF as a combi-\nnation or a portfolio. Here, we consider \u03b2 \u2208{\u22122, \u22121, 2, 3}, which is commonly found in the market.\n3Here, we only present the results when the leverage ratio \u03b2 = 2 since similar patterns are shown across di\ufb00erent\nleverage combinations, presenting a similar shape of their respective curves.\n6\n\n\n--- Page 7 ---\nExpected Compounding Effect\nFigure 1: Simulated and Theoretical Compounding E\ufb00ect E[CEn] with Di\ufb00erent Leverage Ratios\n(Independent Return Case) where Theoretical E[CEn] is computed by Lemma 3.1 and Simulated\nE[CEn] is obtained via Monte Carlo simulations. A positive e\ufb00ect is observed across di\ufb00erent values\nof \u03b2 and \u00b5.\nExcpected Compounding Effect\nFigure 2: Compounding E\ufb00ect with Di\ufb00erent Volatility (Independent Return Case).\n7\n\n\n--- Page 8 ---\nExcpected Compounding Effect\nFigure 3: Compounding E\ufb00ect with Di\ufb00erent Rebalancing Frequencies (Independent Return Case).\nRebalancing Frequency E\ufb00ects.\nNext, we analyze how the rebalancing frequency of LETFs\nin\ufb02uences the expected compounding e\ufb00ect by conducting simulations with daily, weekly, and\nmonthly rebalancing intervals. Using a daily volatility level of 1%, we compare the results across\ndi\ufb00erent rebalancing frequencies. The results in Figure 3 demonstrate that changing the rebalancing\nfrequency does not a\ufb00ect the compounding e\ufb00ect when returns are independent. This result suggests\nthat, in the absence of serial correlation, rebalancing frequency has minimal impact on LETF\nperformance. Consequently, under such conditions, investors may choose less frequent rebalancing\nstrategies to reduce transaction costs without sacri\ufb01cing performance.\n3.1.2\nPortfolio Construction Implications: Independent Return Model.\nUnder the frictionless independent returns benchmark, we can construct arbitrage-like portfolios by\ncombining long positions in LETFs with short positions in the corresponding ETFs. For leverage\n\u03b2 > 1, a portfolio with one unit of LETF and \u03b2 units of short position in the underlying ETF\ncaptures the positive expected compounding e\ufb00ect E[CEn]. For \u03b2 < 0, we substitute short ETF\npositions with long positions in inverse ETFs. The return of such a portfolio is\nRportfolio := RLETF\nn\n\u2212\u03b2RETF\nn\n= CEn\nand the expected return is simply E[Rportfolio] = E[CEn] > 0 under independence and \u00b5 \u0338= 0,\nas established in Lemma 3.1.\nThis theoretical property o\ufb00ers potential arbitrage opportunities\nin markets where returns approximate independence. However, as shown in subsequent sections,\nreturn autocorrelation signi\ufb01cantly alters compounding behavior, and thus the pro\ufb01tability of such\nstrategies is highly regime-dependent. See also Rappoport W and Tuzun (2018) for a empirical\nevidence that ETF arbitrage may be less e\ufb00ective under illiquid condition.\n8\n\n\n--- Page 9 ---\n3.2\nSerially Correlated Returns\nWe now extend our analysis to the case of serially correlated returns, where market dynamics\nexhibit short-term trends or reversals. To capture the temporal dependence, we begin with an\nautoregressive process of order 1 (AR(1)) model.\n3.2.1\nReturns with AR(1) Model.\nSpeci\ufb01cally, we model the returns Xt using an AR(1) process: Xt = c + \u03c6Xt\u22121 + \u03b5t with zero\nintercept c = 0, mean-zero innovation E[\u03b5t] = 0, and constant variance var(\u03b5t) = \u03c32 > 0, and \u03b5t\nare independent of Xt. Moreover, we assume E[Xt] = 0 and E[XtXt\u22121] = \u03c6\n\u03c32\n1\u2212\u03c62 , where |\u03c6| < 1.\nThis setup re\ufb02ects scenarios where returns exhibit serial correlation, either positive (momentum)\nor negative (mean reversion), which can signi\ufb01cantly alter the compounding e\ufb00ect compared to\nthe independent return case. The following lemma indicates that the expected compounding e\ufb00ect\ndepends on the sign of the autocorrelation coe\ufb03cient \u03c6.\nLemma 3.2 (Compounding E\ufb00ect for AR(1) Return Model). Let \u03b2 \u2208Z \\ {0, 1} be the leverage\nratio, and let {Xt}t\u22651 follow an AR(1) model. Suppose there exists a constant m \u2208(0, 1) such that\nP(|Xt| < m) = 1 for all t = 1 . . . , n. Then, the following statements hold:\n(i) If \u03c6 > 0, then E[CEn] > 0.\n(ii) If \u03c6 < 0, then E[CEn] < 0.\nProof. See Appendix A.2.\nRemark 3.2. Lemma 3.2 establishes that short-term serial dependence\u2014speci\ufb01cally, positive or\nnegative autocorrelation\u2014signi\ufb01cantly in\ufb02uences LETF performance. When returns exhibit posi-\ntive autocorrelation, positive correlation ampli\ufb01es returns aligned with the prevailing trend, thereby\namplifying gains. In contrast, negative autocorrelation induces a mean-reverting pattern that mis-\naligns rebalancing with price movements, leading to a performance drag due to buying high and\nselling low.\nWhile the AR(1) model suggests that \u03c6 > 0 leads to LETF outperformance and \u03c6 < 0 leads\nto underperformance, this relationship weakens in the presence of volatility clustering. As we will\nsee in Section 3.2.4, the AR(1)-GARCH(1,1) model reveals that volatility persistence dominates,\nreducing the predictive power of \u03c6 in real-world markets.\n3.2.2\nMonte Carlo Simulations with AR(1) Model.\nIn this section, we simulate how LETF performance varies with di\ufb00erent values of AR coe\ufb03cient\n\u03c6, using Monte Carlo simulations. Speci\ufb01cally, we generate 10, 000 sample paths of daily returns\nover a one-year horizon and examine how the expected compounding e\ufb00ect E[CEn] varies across a\nrange of AR coe\ufb03cients with \u03c6 \u2208[\u22120.9, 0.9].\nCompounding E\ufb00ect with Various Leverage Ratios. Figure 4 demonstrates the impact of dif-\nferent AR coe\ufb03cients \u03c6 on the expected compounding e\ufb00ect E[CEn] across various combinations\n9\n\n\n--- Page 10 ---\nof LETFs. Unlike the independent return case, where the expected compounding e\ufb00ect is posi-\ntive, the AR(1) structure induces sign dependence: For \u03c6 > 0 (momentum-driven markets), the\nexpected compounding e\ufb00ect is positive, enhancing LETF performance. Conversely, the expected\ncompounding e\ufb00ect becomes negative for \u03c6 < 0 (mean-reverting markets), indicating performance\ndrag due to return reversals. This observation highlights the critical role of return dynamics in\ndetermining LETF performance and the necessity of accounting for temporal correlation when\nevaluating LETF strategies.\nFigure 4: Simulated and Theoretical Compounding E\ufb00ect with Di\ufb00erent Leverage Ratio \u03b2. (AR(1)\nReturns where theoretical E[CEn] is computed by Lemma 3.1 and simulated E[CEn] is obtained via\nMonte Carlo simulations with 10, 000 sample paths for each \u03c6.)\nCompounding E\ufb00ect with Di\ufb00erent Levels of Volatility. We examine how volatility modulates\nthe expected compounding e\ufb00ect in the presence of serial correlation among returns. As shown\nin Figure 5, increased volatility ampli\ufb01es the expected compounding e\ufb00ect when the AR coe\ufb03-\ncient \u03c6 > 0, re\ufb02ecting enhanced return potential under momentum-driven market conditions. In\nsuch scenarios, LETFs bene\ufb01t from larger price movements, as rebalancing adjustments amplify\nreturns in the direction of the trend. However, when the positive correlation weakens, increased\nvolatility introduces greater randomness, reducing the e\ufb00ectiveness of rebalancing.\nConversely, when \u03c6 < 0, corresponding to mean-reverting markets, higher volatility exacerbates\nreturn \ufb02uctuations and induces performance drag. Rebalancing in this regime ampli\ufb01es exposure\nduring losses and suppresses it during recoveries, generating a pronounced negative compounding\ne\ufb00ect. These \ufb01ndings highlight the dual role of volatility: while it can enhance LETF performance\nin trending markets, it poses signi\ufb01cant risks in mean-reverting environments.\nCompounding E\ufb00ect with Varying Rebalancing Frequencies. We examine how rebalancing fre-\nquency impacts the compounding e\ufb00ect under AR(1) return model. We simulate daily, weekly,\nand monthly rebalancing intervals.\nThe results, presented in Figure 6, indicate that when the\nAR coe\ufb03cient \u03c6 is positive (indicating momentum-driven markets), portfolios with more frequent\nrebalancing exhibit a stronger positive compounding e\ufb00ect. This outcome shows the importance\n10\n\n\n--- Page 11 ---\nFigure 5: Expected Compounding E\ufb00ect with Di\ufb00erent Volatility (AR(1) Returns).\n11\n\n\n--- Page 12 ---\nof timely rebalancing in capturing upward trends, as daily rebalancing allows LETFs to e\ufb03ciently\ncapitalize on market movements.\nIn contrast, when the AR coe\ufb03cient is negative (mean-reverting markets), frequent rebalancing\nbecomes detrimental, leading to a cycle of buying high and selling low. Interestingly, extending the\nrebalancing interval to a week reduces the negative compounding e\ufb00ect, while monthly rebalancing\nnearly eliminates it. These results suggest that in oscillating markets, less frequent rebalancing can\nmitigate performance drag, providing a strategic advantage to investors who choose LETFs with\nlonger rebalancing intervals.\nFigure 6: Expected Compounding E\ufb00ect with Di\ufb00erent Rebalancing Frequencies (AR(1) Returns.)\n3.2.3\nPortfolio Construction Implications: AR(1) Model.\nUnder AR(1) dynamics, arbitrage-like constructions, as seen in Section 3.1.2, analogous to the\nindependent case may persist when the AR coe\ufb03cient \u03c6 > 0 (momentum regime), as established in\nLemma 3.2. However, when \u03c6 < 0 such portfolios exhibit negative expected compounding, making\nthe strategy counterproductive.\n12\n\n\n--- Page 13 ---\n3.2.4\nReturns with AR-GARCH Models.\nWe now extend our analysis from synthetic return models to an empirically calibrated AR-GARCH\nmodel. The preceding sections considered independent returns and AR(1) returns with controlled\nparameters to isolate key drivers of the compounding e\ufb00ect.\nHere, we adopt a more realistic\nframework by estimating model parameters directly from historical data.\nRather than conducting parameter sweeps, we estimate parameters via the maximum likelihood\nmethod using daily returns for SPY and LETFs, then assess their implications for the compounding\ne\ufb00ect through Monte Carlo simulation. Let Xt represent daily returns. A general ARMA(p, q)-\nGARCH(r, s) structure is given by:\nXt = \u00b5 +\np\nX\ni=1\n\u03c6iXt\u2212i +\nq\nX\nj=1\n\u03b8j\u01ebt\u2212j + \u01ebt,\n\u01ebt = \u03c3tzt,\nzt \u223cN(0, 1),\n\u03c32\nt = \u03c9 +\nr\nX\ni=1\n\u03b1i\u01eb2\nt\u2212i +\ns\nX\nj=1\n\u03b2j\u03c32\nt\u2212j.\nFor tractability and empirical relevance, we adopt the AR(1)-GARCH(1,1) speci\ufb01cation:\nXt = \u00b5 + \u03c6Xt\u22121 + \u01ebt,\n\u03c32\nt = \u03c9 + \u03b1\u01eb2\nt\u22121 + \u03b2\u03c32\nt\u22121.\nOur empirical results reveal that, under AR(1)-GARCH(1,1), the dominant driver of LETF per-\nformance is volatility persistence, as captured by the sum \u03b1+\u03b2, rather than return autocorrelation.\nWhile the AR(1) coe\ufb03cient \u03c6 remains statistically signi\ufb01cant, its impact on LETF performance is\ndiminished by the presence of volatility clustering.\nIndeed, we \ufb01t the AR(1)-GARCH(1,1) model to historical daily returns for SPY and sev-\neral LETFs (SSO, SPXL, SDS, SH) using maximum likelihood estimation (MLE), see Casella and\nBerger (2024). The data ranges from February 2010 to December 2023. The estimation results,\nreported in Table 1, indicate mild mean-reversion (negative AR(1) coe\ufb03cients) and strong volatility\nclustering, with signi\ufb01cant GARCH parameters.\nTable 1: AR(1)-GARCH(1,1) Parameter Estimates for ETFs and LETFs\nETF\n\u00b5 (Const.)\nAR(1) \u03c6\n\u03c9\n\u03b1\n\u03b2\nSPY\n0.0918\u2217\u2217\u2217(0.0130)\n-0.0490\u2217\u2217\u2217(0.0188)\n0.0357\u2217\u2217\u2217(0.0071)\n0.1747\u2217\u2217\u2217(0.0221)\n0.7969\u2217\u2217\u2217(0.0212)\nSSO\n0.1674\u2217\u2217\u2217(0.0259)\n-0.0433\u2217\u2217(0.0191)\n0.1434\u2217\u2217\u2217(0.0286)\n0.1786\u2217\u2217\u2217(0.0223)\n0.7939\u2217\u2217\u2217(0.0211)\nSPXL\n0.2413\u2217\u2217\u2217(0.0389)\n-0.0384\u2217\u2217(0.0191)\n0.3268\u2217\u2217\u2217(0.0634)\n0.1805\u2217\u2217\u2217(0.0220)\n0.7923\u2217\u2217\u2217(0.0208)\nSDS\n-0.1973\u2217\u2217\u2217(0.0260)\n-0.0483\u2217\u2217\u2217(0.0187)\n0.1338\u2217\u2217\u2217(0.0280)\n0.1760\u2217\u2217\u2217(0.0212)\n0.7976\u2217\u2217\u2217(0.0209)\nSH\n-0.0975\u2217\u2217\u2217(0.0130)\n-0.0487\u2217\u2217\u2217(0.0187)\n0.0340\u2217\u2217\u2217(0.0069)\n0.1770\u2217\u2217\u2217(0.0220)\n0.7963\u2217\u2217\u2217(0.0207)\nStandard errors in parentheses; signi\ufb01cance: \u2217\u2217\u2217p < 0.01 and \u2217\u2217p < 0.05.\nMonte Carlo Simulation and Compounding E\ufb00ect. We use the estimated AR(1)-GARCH(1,1)\nmodel parameters to simulate 10,000 return paths for SPY and its leveraged counterparts over a\n13\n\n\n--- Page 14 ---\none-year horizon (252 trading days). From these simulations, we rigorously quantify the compound-\ning e\ufb00ect:\nCE252 =\n 252\nY\nt=1\n(1 + \u03b2Xt) \u22121\n!\n\u2212\u03b2\n 252\nY\nt=1\n(1 + Xt) \u22121\n!\n,\n(3)\nwhere \u03b2 denotes the ETF leverage ratio and Xt denotes simulated daily returns of SPY. The\nresults from these comprehensive simulations are summarized in Table 2. Speci\ufb01cally, the table\ndemonstrates signi\ufb01cant compounding e\ufb00ects that become larger as leverage increases.\nWe see\nthat volatility clustering a\ufb00ects the performance of LETFs, producing consistent deviations from\nthe stated leverage multiples, con\ufb01rming the importance of explicitly modeling volatility and serial\ndependence in the model.\nTable 2: Simulated Compounding E\ufb00ects Using AR(1)-GARCH(1,1) for SPY and Its Associated\nLETFs\nLETF Ticker (\u03b2)\nExpected Compounding E\ufb00ect\nStandard Deviation\nSSO (2x)\n0.0744\n0.1844\nSPXL (3x)\n0.2443\n0.6383\nSDS (-2x)\n0.1664\n0.2756\nSH (-1x)\n0.0597\n0.1063\nAutocorrelation Versus Volatility Clustering.\nWhile the AR(1) coe\ufb03cients estimated in Ta-\nble 1 are statistically signi\ufb01cant, their magnitudes are modest compared to the strong persistence\nexhibited in the GARCH terms (i.e., high values of \u03b1 + \u03b2). This suggests that, in empirical re-\nturn dynamics, volatility clustering exerts a more dominant in\ufb02uence on compounding e\ufb00ects than\nreturn autocorrelation alone.\nHowever, this does not contradict our theoretical \ufb01ndings. Instead, it shows that the relative\nimpact of autocorrelation versus volatility varies by market regime and time scale. Speci\ufb01cally, in\nlow-volatility environments, autocorrelation dominates, see Section 3.2.2, whereas in high-volatility\nperiods with clustering, compounding is more sensitive to volatility shocks.\nThis interplay implies that the predictive power of autocorrelation diminishes in the presence of\nhighly persistent volatility, as shown in our simulations under the AR(1)-GARCH(1,1) framework.\nA full decomposition of these e\ufb00ects remains an important direction for future work. In summary,\nautocorrelation e\ufb00ects govern compounding behavior in stable regimes, whereas in highly persistent\nvolatility regimes, volatility clustering may overshadow autocorrelation-induced deviations from\nLETF targets.\nAdditional Robustness Test: QQQ and Its Associated LETFs. To test the robustness of our\n\ufb01ndings, we expand our empirical analysis beyond the S&P 500 ETF (SPY) to the NASDAQ-100\nETF (QQQ) and its leveraged counterparts.\nGiven that QQQ tracks a more volatile technology-\nfocused index, this serves as a stress test for our theoretical framework. We apply the same AR(1)-\nGARCH(1,1) estimation and Monte Carlo simulations to assess whether LETFs on QQQ exhibit\nsimilar compounding e\ufb00ects as those based on SPY. The result is summarized in Table 3. The\nlarger compounding e\ufb00ect observed for QQQ-based LETFs compared to SPY-based LETFs can be\n14\n\n\n--- Page 15 ---\nattributed to QQQ\u2019s historically higher realized volatility. Since volatility drag increases nonlinearly\nwith volatility, this leads to more pronounced deviations in QQQ-based LETFs.\nThe results above suggest that autocorrelation-driven e\ufb00ects (trend-following bene\ufb01ts or mean-\nreverting losses) might be weaker in markets where volatility clustering is strong. While our \ufb01ndings\nhold under AR(1)-GARCH(1,1) dynamics, future work could explore alternative speci\ufb01cations, such\nas regime-switching or rough volatility models, to further validate the generality of our results.\nTable 3: Simulated Compounding E\ufb00ects Using AR(1)-GARCH(1,1) for QQQ and Its Associated\nLETFs\nLETF Ticker (\u03b2)\nMean Compounding E\ufb00ect\nStandard Deviation\nQLD (2x)\n0.1206\n0.2564\nTQQQ (3x)\n0.4027\n0.9647\nQID (-2x)\n0.2483\n0.3659\nSQQQ (-3x)\n0.4560\n0.6313\n3.2.5\nPortfolio Construction Implications: AR-GARCH Model.\nIn the AR-GARCH setting, the presence of volatility clustering diminishes the predictive power of\nshort-term autocorrelation. While simulations suggest positive compounding e\ufb00ect under estimated\nparameters, the sign of E[CEn] is not guaranteed in theory. Therefore, arbitrage-like portfolios based\non compounding asymmetry require careful calibration to prevailing volatility and autocorrelation\nregimes.\n3.3\nCompounding E\ufb00ect for Continuous-Time Model\nThis section examines a continuous-time regime-switching model. Let St be the ETF price process\nthat evolves according to the regime-dependent geometric Brownian motion, see Di Masi et al.\n(1995), i.e., with S0 > 0,\ndSt = \u00b5ZtStdt + \u03c3ZtStdWt\nwhere {Zt}t\u22650 \u2208{1, 2, . . . , M} is a continuous-time Markov chain with generator matrix Q =\n(Qij)1\u2264i,j\u2264M, Zt governs both the drift \u00b5Zt and volatility \u03c3Zt in each regime, and {Wt}t\u22650 is the\nWiener process on a \ufb01ltered probability space (\u2126, F, P) with \ufb01ltration Ft.\nWe further assume\nthat Z(\u00b7) and W(\u00b7) are independent. With S0 > 0, the solution of St is given by\nSt = S0 exp\n\u0012Z t\n0\n\u0012\n\u00b5Zs \u22121\n2\u03c32\nZs\n\u0013\nds +\nZ t\n0\n\u03c3ZsdWs\n\u0013\n.\n(4)\nFor an LETF with leverage ratio \u03b2 \u2208Z \\{0}, the corresponding price dynamics, denoted by Lt,\ncan be derived from the underlying ETF price St as:\ndLt\nLt\n= \u03b2 dSt\nSt\n\u2212fdt.\n(5)\n15\n\n\n--- Page 16 ---\nThis formulation re\ufb02ects how LETFs magnify the returns of their underlying ETFs by a \ufb01xed\nleverage factor \u03b2. Additionally, the LETF price dynamics (5) with leverage \u03b2 \u2208Z\\{0} has solution\nLt = L0 exp\n\u0012Z t\n0\n\u0012\n\u03b2\u00b5Zs \u22121\n2\u03b22\u03c32\nZs\n\u0013\nds \u2212ft + \u03b2\nZ t\n0\n\u03c3ZsdWs\n\u0013\n.\n(6)\nThe derivation of Equation (6) is relegated to Appendix A.3.\nRemark 3.3. (i) The source of autocorrelation is the persistence of the Markov state Zt: If\nP(Zt+1 = Zt) > 1/2, then the return process exhibits positive autocorrelation.\nNegative au-\ntocorrelation can be generated by alternating regimes (e.g., mean-reversion via switching between\nup/down trends). (ii) The continuous-time model implicitly assumes instantaneous and frictionless\nrebalancing, so that leverage is continuously maintained at level \u03b2. This di\ufb00ers from discrete-time\nimplementations where rebalancing occurs on daily or longer intervals. In practice, continuous re-\nbalancing is unachievable; however, the model provides a theoretical upper bound for compounding\ne\ufb00ects.\nTheorem 3.1 (No Intrinsic Volatility Drag in Expectation). Let \u03b2 \u2208Z \\ {0} denote the leverage\nratio, and consider the continuous-time returns of an ETF and a corresponding LETF, denoted\nby RETF\nt\nand RLETF,\u03b2\nt\n, respectively. Then, the expected compounding e\ufb00ect can be approximated by\nE[CEt] \u2248\nM\nX\nj=1\n\u03c0j(t) [exp ((\u03b2\u00b5j \u2212f)t) \u2212\u03b2 exp (\u00b5jt)] + (\u03b2 \u22121).\n(7)\nwhere \u03c0j(t) := E\nh\n1\nt\nR t\n0 1{Zs=j}ds\ni\nfor each j = 1, . . . , M.\nProof. See Appendix A.3.\nRemark 3.4. (i) Equation (7) becomes exact in the limit of constant regime, i.e., when Zt = j for\nall t \u2208[0, T]; see Corollary 3.1 to follow. More generally, it serves as a \ufb01rst-order approximation\nassuming that regime switches are relatively infrequent over the horizon [0, t], and that Z and W\nare independent. In this case, the expected occupation measure \u03c0j(t) re\ufb02ects the time-averaged\nregime weights and provides a tractable means of interpreting the compounding e\ufb00ect. (ii) When Zt\n\ufb02uctuates frequently, the approximation becomes less accurate. Nevertheless, the representation\nin (7) o\ufb00ers a useful analytical insight into how compounding varies with regime dominance.\n3.3.1\nThree-Regimes Case\n. Let \u03b2 \u2208Z \\ {0} denote the leverage ratio, f \u22650 the \ufb01xed expense fee, and let \u00b5j \u2208R denote the\ndrift in regime j \u2208{1, 2, 3}, characterized as follows:\n\u2022 Regime j = 1: Trending Up (\u00b51 > 0)\n\u2022 Regime j = 2: Trending Down (\u00b52 < 0)\n16\n\n\n--- Page 17 ---\n\u2022 Regime j = 3: Oscillating (Mean-Reverting), where \u00b53 \u22480 and \u03c32\n3 \u226b|\u00b53|\nIn Regime 3, although \u00b53 \u22480, the volatility \u03c33 is large, leading to substantial pathwise \ufb02uctuation.\nLet \u03c0j(t) := E\nh\n1\nt\nR t\n0 1{Zs=j}ds\ni\ndenote the occupation measure of regime j over the interval [0, t],\nand de\ufb01ne the regime-speci\ufb01c compounding kernel:\n\u03a6j(t; \u03b2, f) := exp((\u03b2\u00b5j \u2212f)t) \u2212\u03b2 exp(\u00b5jt).\nThen, the expected compounding e\ufb00ect is approximated by:\nE[CEt] \u2248\n3\nX\nj=1\n\u03c0j(t)\u03a6j(t; \u03b2, f) + (\u03b2 \u22121).\nTheorem 3.2 (Sign of Expected CE under Regime Switching). Let \u03b2 \u2208Z \\ {0} and f \u22650 be\n\ufb01xed, and suppose (\u00b51, \u00b52, \u00b53) are given constants with \u00b51 > 0, \u00b52 < 0, and \u00b53 \u22480. Then the\nsign of E[CEt] is determined entirely by the regime occupation vector \u03c0(t) = (\u03c01(t), \u03c02(t), \u03c03(t)).\nSpeci\ufb01cally:\n\u2022 If P3\nj=1 \u03c0j(t)\u03a6j(t; \u03b2, f) > 1 \u2212\u03b2, then E[CEt] > 0.\n\u2022 If P3\nj=1 \u03c0j(t)\u03a6j(t; \u03b2, f) < 1 \u2212\u03b2, then E[CEt] < 0.\n\u2022 If P3\nj=1 \u03c0j(t)\u03a6j(t; \u03b2, f) = 1 \u2212\u03b2, then E[CEt] = 0.\nProof. See Appendix A.3.\nCorollary 3.1 (Nonnegative Compounding E\ufb00ect in Single Regime). Let \u03b2 \u2208Z\\{0, 1} and Zt \u2261j\nbe constant, i.e., the process remains in a single regime. Then for any \u00b5j \u2208R, f = 0, and all t \u22650,\nwe have:\nE[CEt] = exp(\u03b2\u00b5jt) \u2212\u03b2 exp(\u00b5jt) + (\u03b2 \u22121) \u22650\nwith equality if and only if t = 0 or \u00b5j = 0 or \u03b2 = 1.\nProof. See Appendix A.3.\nRemark 3.5 (Compounding E\ufb00ect Under Regime Switching). (i) In a regime-switching model\nwhere \u03c0j(t) varies over time and no single regime dominates, the E[CEt] may be negative even though\neach \u03a6j(t; \u03b2, f) \u22650 in isolation (as shown in the corollary for f = 0). This occurs because regime\nmixing destroys the directional coherence necessary for compounding to accumulate e\ufb00ectively.\n(ii) While GARCH models account for conditional heteroskedasticity, regime-switching models\ncapture drift and volatility persistence via state dynamics. Volatility clustering in AR-GARCH is\nmimicked in our model by allowing high-volatility states with persistent occupation probabilities.\nHence, the regime-switching framework can subsume AR-GARCH-like behavior when transition\nrates are su\ufb03ciently sticky.\n17\n\n\n--- Page 18 ---\n3.3.2\nPortfolio Construction Implications: Regime Switching Model/\nIn the continuous-time regime-switching model, expected compounding e\ufb00ect is a function of regime\noccupation weights \u03c0j(t). Arbitrage-like portfolios, as seen in Section 3.1.2, may yield positive\nexpected returns when the process spends signi\ufb01cant time in trending regimes with positive drift.\nHowever, extended periods in oscillating or high-volatility regimes may negate the compounding\nbene\ufb01t. Thus, strategy e\ufb00ectiveness depends critically on regime persistence\n4\nEmpirical Studies\nThis section empirically examines the compounding e\ufb00ect of LETFs using historical data from the\nSPDR S&P 500 ETF (Ticker: SPY) and the Nasdaq-100 ETF (Ticker: QQQ) under varying market\nconditions. We aim to validate our theoretical \ufb01ndings that LETFs outperform in trending markets\nand underperform in mean-reverting markets due to volatility.\n4.1\nHypotheses and Empirical Strategy\nTo rigorously test our theoretical predictions, we formulate the following hypotheses:\nH1 (Trending Markets): LETFs exhibit a positive compounding e\ufb00ect in strongly trending\nmarkets, regardless of direction (e.g., Financial Crisis, Post-Crisis Recovery).\nH2 (Mean-Reverting Markets): LETFs exhibit a negative compounding e\ufb00ect in oscillating\nor mean-reverting markets, as frequent rebalancing ampli\ufb01es losses (e.g., Sideways Markets).\nH3 (Volatility Impact): The compounding e\ufb00ect is stronger for higher leverage ratios (e.g.,\n\u03b2 =\n\u00b1 3), with larger deviations in high-volatility environments.\nTo test these hypotheses, we analyze LETF performance across distinct market regimes and\ncompare theoretical with empirical compounding e\ufb00ects. We begin with simulated, synthetic LETF\nreturns and then compare them to realized returns from actual LETF products, adjusting for real-\nworld frictions such as fees and tracking errors.\n4.2\nMarket Regimes and Compounding E\ufb00ects\nBelow, we focus on six key periods that capture diverse market regimes: the \ufb01nancial crisis (Octo-\nber 2007 to March 2009), its subsequent post-crisis recovery (April 2009 to March 2013), a period of\nsideways market (February 2014 to September 2015), the COVID-19 pandemic (February 2020 to\nMarch 2020), the post-COVID recovery (April 2020 to December 2021), and the 2022 bear market.\nEach timeframe represents a di\ufb00erent market phase, allowing us to assess how LETFs behave under\nsharply declining, recovering, or stagnant markets.\n4.2.1\nTheoretical Compounding E\ufb00ects via Synthetic LETFs.\nTable 4 presents theoretical compounding e\ufb00ects for synthetic LETFs. Here, theoretical volatility\nrefers to the e\ufb00ect of synthetic LETFs constructed using the benchmark ETFs SPY and QQQ with\ndi\ufb00erent leverage ratios \u03b2 \u2208{\u22123, \u22122, \u22121, 2, 3}, assuming perfect replication without tracking error\n18\n\n\n--- Page 19 ---\nor fees. Later in this section, we will compare this with the empirical compounding e\ufb00ect, computed\nby using the actual LETFs data.\nThe \ufb01nancial crisis (October 2007 to March 2009) and the subsequent post-crisis recovery\n(April 2009 to March 2013) provide contrasting phases of sharp decline and sustained growth.\nDuring the crisis, the S&P 500 index lost more than 50% of its value, falling from over 1500 to\nbelow 700 points.\nIn response, the Federal Reserve introduced quantitative easing, fostering a\nlow-interest-rate environment that gradually fueled economic recovery. By 2013, the index had\nfully recovered to pre-crisis levels. As shown in Table 4, positive-leverage synthetic LETFs exhibit\npositive compounding e\ufb00ects in both recovery periods, particularly strong for \u03b2 = 3. For inverse\nLETFs (\u03b2 < 0), the compounding e\ufb00ects during the \ufb01nancial crisis are predominantly negative\nfor QQQ, while SPY shows mixed results with \u03b2 = \u22121 exhibiting a small positive compounding ef-\nfect, but negative e\ufb00ects for \u03b2 = \u22122 and \u03b2 = \u22123. This suggests that LETFs tend to outperform\nexpectations in strongly trending markets, making them attractive for trend following strategies.\nHowever, in mean-reverting markets, the e\ufb00ects di\ufb00er by market regime. During the Sideways\nMarket period, positive-\u03b2 synthetic LETFs consistently show negative compounding e\ufb00ects for both\nETFs. During the COVID-19 pandemic period, QQQ exhibits negative compounding e\ufb00ects across all\nleverage ratios, while SPY shows predominantly negative e\ufb00ects with a negligible positive exception\nfor \u03b2 = 3. The compounding e\ufb00ect is generally strongest in magnitude for highly leveraged LETFs\n(\u03b2 = \u00b13) and becomes more pronounced in high-volatility environments. The robustness check\nusing QQQ con\ufb01rms these \ufb01ndings, with stronger deviations due to sector volatility.\nThese results largely support Hypothesis H1: In upward-trending regimes (2009\u201313, 2020\u2013\n21), positive-\u03b2 synthetic LETFs exhibit consistently positive compounding e\ufb00ects. For downward-\ntrending regimes (2007\u201309), with positive-\u03b2 LETFs showing positive compounding e\ufb00ects as ex-\npected, but inverse LETFs showing mixed results that partially support the hypothesis. The data\nalso strongly con\ufb01rms Hypothesis H2: In the Sideways Market period, positive-\u03b2 LETFs for both\nETFs exhibit negative compounding e\ufb00ects. Finally, our \ufb01ndings provide clear evidence for Hy-\npothesis H3: Across all market regimes, compounding e\ufb00ects are consistently largest in magnitude\nfor the highest leverage ratios (\u03b2 = \u00b13), with extreme deviations occurring during high-volatility\nperiods such as the Post-COVID Recovery and Financial Crisis.\n4.2.2\nEmpirical Compounding E\ufb00ects and Robustness Analysis.\nWe now conduct our empirical analysis using historical LETF data as presented in Table 5. For\nthe SPY ETF benchmark, we examine four leveraged ETFs: SDS (\u03b2 = \u22122), SH (\u03b2 = \u22121), SSO\n(\u03b2 = +2), and SPXL (\u03b2 = +3). As a robustness check, we also analyze the QQQ ETF with its\ncorresponding leveraged ETFs: SQQQ (\u03b2 = \u22123), QID (\u03b2 = \u22122), QLD (\u03b2 = 2), TQQQ (\u03b2 = 3).\nTable 5 shows a consistent sign match as predicted by the theoretical synthetic LETFs case in\nTable 4 with a certain deviation. The divergence between theoretical and empirical compounding\ne\ufb00ects increases with |\u03b2|, especially during volatile periods like the COVID-19 crash. Note that\ntransaction costs and fees account for approximately 0.8 to 1.0 percentage points per annum of the\nobserved underperformance, while bid-ask spreads, slippage, and tracking error might contribute\nadditional deviations; see Abdi and Ranaldo (2017) for a simple estimation of bid-ask spread.\n19\n\n\n--- Page 20 ---\nTable 4: Theoretical Compounding E\ufb00ects of Portfolios Across Six Market Regimes\nLeverage Ratio (\u03b2)\nMarket Condition\nBenchmark ETF\n-3x\n-2x\n-1x\n2x\n3x\nFinancial Crisis\nSPY\n-0.733\n-0.144\n0.034\n0.160\n0.475\nQQQ\n-0.883\n-0.293\n-0.035\n0.105\n0.346\nPost-Crisis Recovery\nSPY\n2.344\n1.349\n0.515\n0.651\n1.863\nQQQ\n3.011\n1.770\n0.696\n1.005\n3.038\nSideways Market\nSPY\n-0.016\n-0.016\n-0.008\n-0.018\n-0.064\nQQQ\n0.117\n0.048\n0.011\n-0.007\n-0.043\nCOVID-19 Pandemic\nSPY\n-0.332\n-0.141\n-0.037\n-0.007\n0.005\nQQQ\n-0.398\n-0.189\n-0.057\n-0.034\n-0.076\nPost-COVID Recovery\nSPY\n2.034\n1.177\n0.459\n0.756\n2.670\nQQQ\n2.657\n1.561\n0.618\n1.047\n3.654\n2022 Bear Market\nSPY\n-0.251\n-0.105\n-0.027\n0.003\n0.011\nQQQ\n-0.187\n-0.018\n0.018\n0.066\n0.214\nNote: Values in bold represent substantial outperformance (de\ufb01ned as greater than 1.0).\nIn particular, the COVID-19 pandemic onset (February 2020 to March 2020) caused a rapid 34%\ndrop in the S&P 500 as global lockdowns disrupted economic activity.\nSwift monetary policy\ninterventions enabled a quick recovery, with the index reaching new highs by late 2021. Interestingly,\nduring the crash, inverse ETF portfolios underperformed expectations. This indicates that LETFs\nmay fail to deliver the expected outperformance in highly volatile, mean-reverting markets due\nto erratic price movements. These results validate Hypothesis H3 and align with our theoretical\npredictions: Higher leverage ratios (\u03b2 = 3) exhibit greater deviations from theoretical predictions,\nparticularly in high-volatility environments like the COVID-19 pandemic period.\nThe 2022 bear market was marked by a 20% decline in S&P 500, driven by rising in\ufb02ation\nand geopolitical uncertainties, see Bouri et al. (2023) and news released by CNBC (2022).\nAs\nshown in Table 5, empirical LETF performance during this period resembled that of the COVID-\n19 pandemic period. The compounding e\ufb00ect remained weak due to increased market uncertainty,\nand as expected, inverse LETFs continued to underperform due to tracking error accumulation.\nThe sideways market (2014\u20132015) presents a unique challenge for LETFs, as frequent rebal-\nancing results in negative compounding e\ufb00ects. While most portfolios exhibited the negative com-\npounding e\ufb00ects, an exception arises for SQQQ, which yielded a small positive CE. This re\ufb02ects net\nupward drift in QQQ over the window. Frequent rebalancing leads to systematic buying high and\nselling low, which ampli\ufb01es losses over time. Although mean-reverting markets generally induce\nnegative compounding e\ufb00ects due to rebalancing drag, directional drift\u2014even if modest\u2014can yield\na net positive e\ufb00ect, as observed in SQQQ during 2014\u201315. Thus, H2 holds in expectation but may\nbe violated in certain drifting but volatile environments.\nA robustness check using QQQ-based LETFs, see Table 5, reveals similar trends but with stronger\ndeviations. The tech-heavy Nasdaq-100 exhibited higher volatility, leading to greater tracking errors\nand magni\ufb01ed deviations in leveraged products.\n20\n\n\n--- Page 21 ---\n4.2.3\nSummary.\nTables 4 and 5 are directionally consistent: they agree on when CE should be positive versus neg-\native across \u03b2 and market regimes. The empirical CE is systematically smaller in magnitude (due\nto fees, \ufb01nancing costs, and imperfect tracking), and some leverage ratios do not exist for the full\nsample. The robustness check using QQQ con\ufb01rms these \ufb01ndings while highlighting stronger devia-\ntions in tech-heavy indices. In summary, the empirical results support our theoretical hypotheses:\nH1 is con\ufb01rmed: LETFs outperform in strongly trending regimes, regardless of direction, due to\npath-dependent compounding. H2 holds broadly but admits exceptions: While most LETFs ex-\nhibit negative compounding e\ufb00ects in the sideways market as predicted, inverse LETFs occasionally\noutperform in \ufb02at markets with su\ufb03cient directional drift, as observed with SQQQ during 2014-15.\nThis nuance enhances our understanding of when the volatility decay hypothesis may be counter-\nbalanced by other factors. H3 is robustly validated across all regimes: Empirical CE magnitudes\nscale with leverage and volatility, with the largest deviations occurring in high-volatility periods.\nTable 5: Empirical Compounding E\ufb00ects of Portfolios Across Six Market Regimes\nLeverage Ratio (\u03b2) and Product Ticker\nMarket Condition\nBenchmark ETF\n\u22123\u00d7\n\u22122\u00d7\n\u22121\u00d7\n2\u00d7\n3\u00d7\n(SQQQ)\n(SDS/QID)\n(SH)\n(SSO/QLD)\n(SPXL/TQQQ)\nFinancial Crisis\nSPY\n\u2014\n0.053\n-0.050\n0.145\n\u2014\nQQQ\n-1.219\n-0.307\n\u2014\n0.085\n\u2014\nPost-Crisis Recovery\nSPY\n\u2014\n1.342\n0.500\n0.467\n1.389\nQQQ\n3.189\n1.762\n\u2014\n0.787\n\u2014\nSideways Market\nSPY\n\u2014\n-0.021\n-0.018\n-0.043\n-0.114\nQQQ\n0.112\n0.041\n\u2014\n-0.039\n-0.096\nCOVID-19 Pandemic\nSPY\n\u2014\n-0.156\n-0.039\n-0.015\n-0.019\nQQQ\n-0.424\n-0.198\n\u2014\n-0.040\n-0.087\nPost-COVID Recovery\nSPY\n\u2014\n1.173\n0.451\n0.666\n2.434\nQQQ\n2.656\n1.557\n\u2014\n0.936\n3.370\n2022 Bear Market\nSPY\n\u2014\n-0.051\n-0.002\n-0.024\n-0.014\nQQQ\n-0.109\n0.035\n\u2014\n0.051\n0.198\nNote: Missing data points (\u2014) indicate values not available for the speci\ufb01c ETF at the given leverage factor.\nFor ex-\nample, TQQQ wasn\u2019t launched until February 2010, so data is unavailable for the Financial Crisis period. Similarly, SPXL\nlaunched in November 2008; no continuous daily series exists for the full crisis window. Positive values indicate outperfor-\nmance relative to the target multiple, while negative values indicate underperformance. Values in bold represent substantial\noutperformance (> 1.0).\n4.3\nTime-Varying Compounding E\ufb00ect: A Rolling Window Analysis\nWhile the preceding sections demonstrate that return autocorrelation is a key determinant of LETF\nperformance, the evolution of this relationship over time remains to be characterized. To address\nthis, we examine how the realized compounding e\ufb00ect (CE) varies across market regimes and\nassess its connection to short-run return persistence. Speci\ufb01cally, we compute 60-, 90-, and 120-\nday rolling-window estimates of compounding e\ufb00ects for a range of LETFs based on SPY and QQQ,\nspanning multiple leverage ratios \u03b2 \u2208{\u22122, \u22121, 2, 3}.\nFigures 7\u201312 display the rolling compounding e\ufb00ects from 2009 (SPY) and 2010 (QQQ) to 2024\nwhere the shaded regions represent major market events, including oil crash (mid 2015 to early 2016),\n21\n\n\n--- Page 22 ---\nand COVID-19 crash (early 2020 to late 2021), and in\ufb02ation-driven bear market (early-2022).\nSeveral systematic patterns emerge. First, positive CE is observed during persistent upward-\ntrending markets\u2014notably the post-2009 recovery, 2016\u20132017, and the 2020\u20132021 bull run\u2014particularly\nfor \u03b2 = 2 and \u03b2 = 3. In contrast, negative CE is concentrated in volatile or directionless periods,\nsuch as 2014\u20132015 and 2022, where rebalancing erodes returns. These patterns are consistent with\nthe theoretical results in Section 3.\n2010\n2012\n2014\n2016\n2018\n2020\n2022\n2024\nDate\n\u22120.5\n\u22120.4\n\u22120.3\n\u22120.2\n\u22120.1\n0.0\n0.1\n0.2\nCompounding Effect\n-2x\n-1x\n2x\n3x\nFigure 7: 60-Day Rolling Compounding E\ufb00ect from 2009 to 2024 (SPY)\n2010\n2012\n2014\n2016\n2018\n2020\n2022\n2024\nDate\n\u22120.6\n\u22120.4\n\u22120.2\n0.0\n0.2\nCompounding Effect\n-2x\n-1x\n2x\n3x\nFigure 8: 90-Day Rolling Compounding E\ufb00ect from 2009 to 2024 (SPY)\nTo examine the role of short-term momentum, we compute rolling AR(1) coe\ufb03cients over the\nsame intervals, presented in Figures 13\u201318.\nThese plots reveal \ufb02uctuations in \u03c6, often ranging\nbetween \u22120.4 and +0.6. Periods with elevated AR(1) coe\ufb03cients, such as 2010\u20132011, 2016\u20132017,\nand early 2020, correspond closely to intervals of strongly positive compounding e\ufb00ect. Conversely,\nduring stretches of low or negative autocorrelation\u2014such as late 2015 or mid-2022\u2014compounding\ne\ufb00ects decay sharply.\n22\n\n\n--- Page 23 ---\n2010\n2012\n2014\n2016\n2018\n2020\n2022\n2024\nDate\n\u22120.6\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nCompounding Effect\n-2x\n-1x\n2x\n3x\nFigure 9: 120-Day Rolling Compounding E\ufb00ect from 2009 to 2024 (SPY)\n2010\n2012\n2014\n2016\n2018\n2020\n2022\n2024\nDate\n\u22120.8\n\u22120.6\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nCompounding Effect\n-3x\n-2x\n2x\n3x\nFigure 10: 60-Day Rolling Compounding E\ufb00ect from 2010 to 2024 (QQQ)\n2010\n2012\n2014\n2016\n2018\n2020\n2022\n2024\nDate\n\u22120.8\n\u22120.6\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\n0.6\nCompounding Effect\n-3x\n-2x\n2x\n3x\nFigure 11: 90-Day Rolling Compounding E\ufb00ect from 2010 to 2024 (QQQ)\n23\n\n\n--- Page 24 ---\n2010\n2012\n2014\n2016\n2018\n2020\n2022\n2024\nDate\n\u22120.75\n\u22120.50\n\u22120.25\n0.00\n0.25\n0.50\n0.75\n1.00\nCompounding Effect\n-3x\n-2x\n2x\n3x\nFigure 12: 120-Day Rolling Compounding E\ufb00ect from 2010 to 2024 (QQQ)\nComparing SPY and QQQ, we observe that QQQ-based LETFs generally exhibit more volatile and\nextreme CE \ufb02uctuations, as well as higher-amplitude AR(1) dynamics. This is consistent with the\nhigher baseline volatility of the Nasdaq-100 and more frequent regime shifts in its constituent \ufb01rms.\nFor instance, QQQ CE spikes during both the 2020 pandemic rebound and late 2021 coincide with\nsharp increases in \u03c6, reinforcing the amplifying role of trend-following dynamics under high leverage.\nOverall, the rolling-window analysis reveals that LETF compounding behavior is not static, but\ndynamically linked to the prevailing autocorrelation environment. Positive short-run autocorrela-\ntion enhances compounding gains by aligning leverage with trending returns, while mean-reverting\nor noisy regimes diminish performance. These empirical observations further validate the central\ntheoretical result of this paper: return persistence is a critical driver of LETF performance beyond\nvolatility alone.\n2010\n2012\n2014\n2016\n2018\n2020\n2022\n2024\nDate\n\u22120.6\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\n0.6\nAR(1) Coefficient\nSDS\nSH\nSPXL\nSPY\nSSO\nFigure 13: 60-Day Rolling AR(1) Coe\ufb03cients from 2009 to 2024 (SPY)\n24\n\n\n--- Page 25 ---\n2010\n2012\n2014\n2016\n2018\n2020\n2022\n2024\nDate\n\u22120.6\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nAR(1) Coefficient\nSDS\nSH\nSPXL\nSPY\nSSO\nFigure 14: 90-Day Rolling AR(1) Coe\ufb03cients from 2009 to 2024 (SPY)\n2010\n2012\n2014\n2016\n2018\n2020\n2022\n2024\nDate\n\u22120.6\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nAR(1) Coefficient\nSDS\nSH\nSPXL\nSPY\nSSO\nFigure 15: 120-Day Rolling AR(1) Coe\ufb03cients from 2009 to 2014 (SPY)\n2010\n2012\n2014\n2016\n2018\n2020\n2022\n2024\nDate\n\u22120.6\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\n0.6\nAR(1) Coefficient\nQID\nQLD\nQQQ\nSQQQ\nTQQQ\nFigure 16: 60-Day Rolling AR(1) Coe\ufb03cients from 2010 to 2024 (QQQ)\n25\n\n\n--- Page 26 ---\n2010\n2012\n2014\n2016\n2018\n2020\n2022\n2024\nDate\n\u22120.6\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nAR(1) Coefficient\n90-Day Rolling AR(1) Coefficients from 2010 to 2014 (QQQ)\nQID\nQLD\nQQQ\nSQQQ\nTQQQ\nFigure 17: 90-Day Rolling AR(1) Coe\ufb03cients from 2010 to 2024 (QQQ)\n2010\n2012\n2014\n2016\n2018\n2020\n2022\n2024\nDate\n\u22120.6\n\u22120.4\n\u22120.2\n0.0\n0.2\nAR(1) Coefficient\nQID\nQLD\nQQQ\nSQQQ\nTQQQ\nFigure 18: 120-Day Rolling AR(1) Coe\ufb03cients from 2010 to 2024 (QQQ)\n26\n\n\n--- Page 27 ---\n5\nConcluding Remarks\nThis paper explores the compounding e\ufb00ect of LETFs under various return dynamics, volatility\nlevels, and rebalancing frequencies. Our central \ufb01nding is that return autocorrelation and return\ndynamics\u2014not the volatility alone\u2014determine whether LETFs outperform or underperform their\ntargets. In particular, momentum improves compounding, while mean reversion undermines it,\nwith these e\ufb00ects magni\ufb01ed under frequent rebalancing.\nWe develop a uni\ufb01ed theoretical framework encompassing AR(1), AR-GARCH, and continuous-\ntime regime-switching models. These models reveal that volatility clustering interacts with auto-\ncorrelation to shape LETF behavior, particularly under persistent shocks or regime transitions.\nThis insight re\ufb01nes the conventional volatility-drag narrative by emphasizing the joint role of au-\ntocorrelation and volatility persistence.\nIn the benchmark environments with independent returns, LETFs tend to outperform their tar-\nget multiples on average, and changes in volatility or rebalancing frequency have minimal impact. In\ncontrast, in serially correlated markets, LETF performance is sensitive to the autocorrelation, return\ndynamics, and the rebalancing interval. Daily rebalancing enhances returns in trending markets,\nwhereas weekly or monthly rebalancing mitigates losses in oscillating or mean-reverting regimes.\nEmpirical validation using about 20 years of SPY and QQQ data across diverse market regimes\ncon\ufb01rms these predictions. LETFs exhibit positive compounding during directional trends, but\nunderperform in sideways markets where rebalancing erodes returns. For trend-following investors,\ndaily-rebalanced LETFs o\ufb00er superior performance. In contrast, when autocorrelation is weak or\nnegative, longer rebalancing intervals reduce performance decay. Future work could extend this\nframework by incorporating rough volatility models or non-Markovian regime-switching dynamics\nto capture \ufb01ner features of real-world return processes.\nReferences\nAbdi, F. and Ranaldo, A. (2017). A Simple Estimation of Bid-Ask Spreads from Daily Close, High, and Low\nPrices. The Review of Financial Studies, 30(12):4437\u20134480.\nAbdou, A. (2017). Accounting for Volatility Decay in Time Series Models for Leveraged Exchange Traded\nFunds. Available at SSRN 2980208.\nAvellaneda, M. and Zhang, S. (2010). Path-Dependence of Leveraged ETF Returns. SIAM Journal on\nFinancial Mathematics, 1(1):586\u2013603.\nBen-David, I., Franzoni, F. A., and Moussawi, R. (2017). Exchange Traded Funds (ETFs). Annual Review\nof Financial Economics, 9:2016\u201322.\nBouri, E., Gabauer, D., Gupta, R., and Kinateder, H. (2023).\nGlobal Geopolitical Risk and In\ufb02ation\nSpillovers across European and North American Economies. Research in International Business and\nFinance, 66:102048.\nCarver, A. B. (2009). Do Leveraged and Inverse ETFs Converge to Zero? ETFs and Indexing, 2009(1):144\u2013\n149.\nCasella, G. and Berger, R. (2024). Statistical Inference. CRC Press.\nCharupat, N. and Miu, P. (2011). The Pricing and Performance of Leveraged Exchange-Traded Funds.\nJournal of Banking & Finance, 35(4):966\u2013977.\n27\n\n\n--- Page 28 ---\nCNBC\n(2022).\nStocks\nFall\nto\nEnd\nWall\nStreet\u2019s\nWorst\nYear\nSince\n2008,\nS&P\n500\nFinishes\n2022\nDown\nNearly\n20%.\nAvailable\nat:\nhttps://www.cnbc.com/2022/12/29/stock-market-futures-open-to-close-news.html.\nCvitanic, J. and Zapatero, F. (2004). Introduction to the Economics and Mathematics of Financial Markets.\nMIT Press.\nDai, M., Kou, S., Soner, H. M., and Yang, C. (2023). Leveraged Exchange-Traded Funds with Market\nClosure and Frictions. Management Science, 69(4):2517\u20132535.\nDi Masi, G. B., Kabanov, Y. M., and Runggaldier, W. J. (1995). Mean-Variance Hedging of Options on\nStocks with Markov Volatilities. Theory of Probability & Its Applications, 39(1):172\u2013182.\nGuedj, I., Li, G., and McCann, C. (2010). Leveraged and Inverse ETFs, Holding Periods, and Investment\nShortfalls. The Journal of Index Investing, 1(3):45\u201357.\nJarrow, R. A. (2010). Understanding the Risk of Leveraged ETFs. Finance Research Letters, 7(3):135\u2013139.\nLeung, T. and Santoli, M. (2016). Leveraged Exchange-Traded Funds: Price Dynamics and Options Valua-\ntion. Springer.\nLu, L., Wang, J., and Zhang, G. (2009). Long Term Performance of Leveraged ETFs. Available at SSRN\n1344133.\nMalkiel, B. G. and Radisich, A. (2001). The Growth of Index Funds and the Pricing of Equity Securities.\nJournal of Portfolio Management, 27(2):9.\nRappoport W, D. E. and Tuzun, T. (2018). Arbitrage and Liquidity: Evidence from a Panel of Exchange\nTraded Funds. Available at SSRN 3281384.\nTang, H. and Xu, X. E. (2013). Solving the Return Deviation Conundrum of Leveraged Exchange-Traded\nFunds. Journal of Financial and Quantitative Analysis, 48(1):309\u2013342.\nTrainor, W. (2012). Volatility and Compounding E\ufb00ects on Beta and Returns. The International Journal\nof Business and Finance Research, 6(4):1\u201311.\nTrainor, W. (2017). Leveraged Exchange-Traded Funds: When Four is Not More. Journal of Investment\nConsulting, 18(1):31\u201340.\nTrainor Jr, W. J. and Baryla Jr, E. A. (2008). Leveraged ETFs: A Risky Double That Doesn\u2019t Multiply by\nTwo. Journal of Financial Planning, 21(5).\nTrainor Jr, W. J. and Carroll, M. G. (2013). Forecasting Holding Periods for Leveraged ETFs using Decay\nThresholds: Theory and Applications. Journal of Financial Studies & Research, 2013:1.\nU.S.\nSecurities\nand\nExchange\nCommission\n(2009).\nLeveraged\nand\nInverse\nETFs:\nSpecialized\nProducts\nwith\nExtra\nRisks\nfor\nBuy-and-Hold\nInvestors.\nInvestor\nAlert.\nAvailable\nat:\nhttp://www.sec.gov/investor/pubs/leveragedetfs-alert.htm.\nA\nTechnical Proofs\nThis appendix collects some technical proofs of the paper.\nA.1\nProofs in Section 2\nProof of Theorem 2.1. We begin by noting that the ETF\u2019s daily return is XETF\nt\n= Xt and LETF\u2019s\ndaily return is to include fees and tracking error: XLETF\nt\n= \u03b2Xt \u2212f + et, where \u03b2 \u2208Z \\ {0} is the\nleverage ratio, f is a daily fee (incorporating expense and transaction costs), and et is a tracking\n28\n\n\n--- Page 29 ---\nerror with |et| \u2264M and E[et] = E[etXs] = E[etes] = 0 for t \u0338= s. Assume also |Xt|, |f| \u2264M, for\nsome M \u226a1. De\ufb01ne:\nAt := \u03b2Xt \u2212f + et,\nBt := Xt.\nHence, the compounding e\ufb00ect is: CEn := RLETF\nn\n\u2212\u03b2RETF\nn\nwhere the cumulative returns over n\ndays are:\nRLETF\nn\n=\nn\nY\nt=1\n(1 + At) \u22121 and RETF\nn\n=\nn\nY\nt=1\n(1 + Bt) \u22121.\nUsing the fact that Qn\nt=1(1+ Zt) = 1+ Pn\nt=1 Zt + P\n1\u2264t<s\u2264n ZtZs + Rn with E[|Rn|] \u2264C\u2032n3M3 for\nsome C\u2032 > 0, it follows that RLETF\nn\nand RETF\nn\nsatis\ufb01es\nRLETF\nn\n=\nn\nX\nt=1\nAt +\nX\n1\u2264t<s\u2264n\nAtAs + R(A)\nn ,\nRETF\nn\n=\nn\nX\nt=1\nBt +\nX\n1\u2264t<s\u2264n\nBtBs + R(B)\nn\n,\nand therefore\nCEn = RLETF\nn\n\u2212\u03b2RETF\nn\n=\nn\nX\nt=1\n(At \u2212\u03b2Bt) +\nX\n1\u2264t<s\u2264n\n(AtAs \u2212\u03b2BtBs) + Rn,\n(8)\nwhere Rn := R(A)\nn\n\u2212\u03b2R(B)\nn\n, and E[|Rn|] \u2264Cn3M3 for some constant C > 0 by triangle inequality.\nNote that the \ufb01rst-order term in Equation (8) can be computed as follows:\nn\nX\nt=1\n(At \u2212\u03b2Bt) =\nn\nX\nt=1\n(\u03b2Xt \u2212f + et \u2212\u03b2Xt) = \u2212nf +\nn\nX\nt=1\net.\nTaking expectations and using E[et] = 0, we obtain: E [Pn\nt=1(At \u2212\u03b2Bt)] = \u2212nf. Next, for t \u0338= s,\nwe expand:\nAtAs = (\u03b2Xt \u2212f + et)(\u03b2Xs \u2212f + es)\n= \u03b22XtXs \u2212\u03b2f(Xt + Xs) + f 2 + \u03b2Xtes + \u03b2Xset \u2212f(et + es) + etes.\nThus, AtAs \u2212\u03b2BtBs = \u03b2(\u03b2 \u22121)XtXs \u2212\u03b2f(Xt + Xs) + f 2 + \u03b2(Xtes + Xset) \u2212f(et + es) + etes.\nTaking the expectation yields\nE[AtAs \u2212\u03b2BtBs] = \u03b2(\u03b2 \u22121)\u03b3|t\u2212s| \u22122\u03b2f\u00b5 + f 2.\nwhere the equality holds by using the facts that E[XtXs] = \u03b3|t\u2212s| from strict stationarity, E[et] =\nE[etXs] = E[etes] = 0 for t \u0338= s, and \u00b5 := E[Xt]. Summing over all 1 \u2264t < s \u2264n, we group terms\n29\n\n\n--- Page 30 ---\nby lag k = s \u2212t, which contributes (n \u2212k) terms per lag. This gives:\nE\n\uf8ee\n\uf8f0\nX\n1\u2264t<s\u2264n\nAtAs \u2212\u03b2BtBs\n\uf8f9\n\uf8fb=\nX\n1\u2264t<s\u2264n\nE[AtAs \u2212\u03b2BtBs] =\nn\u22121\nX\nk=1\n(n \u2212k)\n\u0002\n\u03b2(\u03b2 \u22121)\u03b3k \u22122\u03b2f\u00b5 + f 2\u0003\n.\nPutting everything together:\nE[CEn] = \u2212nf +\nn\u22121\nX\nk=1\n(n \u2212k) [\u03b2(\u03b2 \u22121)\u03b3k \u22122\u03b2f\u00b5] +\n\u0012n\n2\n\u0013\nf 2 + O(n3M3).\nProof of Lemma 3.1. To prove the desired positivity, we proceed with a proof by induction. Firstly,\nfor n = 1, note that E[CE1] = E\nh\nRLETF,\u03b2\n1\n\u2212\u03b2RETF\n1\ni\n= [(1 + \u03b2\u00b5)1 \u22121] \u2212\u03b2[(1 + \u00b5)1 \u22121] = 0. Now,\nassume the statement holds for n = k \u22651. That is, [(1 + \u03b2\u00b5)k \u22121] \u2212\u03b2[(1 + \u00b5)k \u22121] \u22650, which is\nequivalent to (1 + \u03b2\u00b5)k + \u03b2 \u22121 \u2265\u03b2(1 + \u00b5)k. We must show that the statement holds for n = k + 1.\nIndeed, observe that\n(1 + \u03b2\u00b5)k+1 \u22121 =(1 + \u03b2\u00b5)(1 + \u03b2\u00b5)k \u22121\n=(1 + \u03b2\u00b5)[(1 + \u03b2\u00b5)k + \u03b2 \u22121] \u2212\u03b2 \u2212\u03b22\u00b5 + \u03b2\u00b5\n\u2265(1 + \u03b2\u00b5)[\u03b2(1 + \u00b5)k] \u2212\u03b2 \u2212\u03b22\u00b5 + \u03b2\u00b5\nwhere inequality is held by invoking the inductive hypotheses. Hence, it follows that\n(1 + \u03b2\u00b5)k+1 \u22121 \u2265(1 + \u03b2\u00b5)[\u03b2(1 + \u00b5)k] \u2212\u03b2 \u2212\u03b22\u00b5 + \u03b2\u00b5\n=(1 + \u00b5 + (\u03b2 \u22121)\u00b5)[\u03b2(1 + \u00b5)k] \u2212\u03b2 \u2212\u03b22\u00b5 + \u03b2\u00b5\n=\u03b2(1 + \u00b5)k+1 + \u03b2(\u03b2 \u22121)\u00b5(1 + \u00b5)k \u2212\u03b2 \u2212\u03b22\u00b5 + \u03b2\u00b5\n=\u03b2[(1 + \u00b5)k+1 \u22121] + \u03b2(\u03b2 \u22121)\u00b5(1 + \u00b5)k + \u03b2\u00b5(1 \u2212\u03b2)\n=\u03b2[(1 + \u00b5)k+1 \u22121] + \u03b2\u00b5(\u03b2 \u22121)[(1 + \u00b5)k \u22121].\n(9)\nNote that if \u03b2\u00b5(\u03b2\u22121)[(1+\u00b5)k\u22121] \u22650, then we are done. Therefore, to complete the proof, it remains\nto prove \u03b2\u00b5(\u03b2 \u22121)[(1 + \u00b5)k \u22121] \u22650. Set an auxiliary function f(\u03b2, \u00b5) := \u03b2\u00b5(\u03b2 \u22121)[(1 + \u00b5)k \u22121],\nthen, we consider the following four cases:\nCase 1. When \u03b2 > 1 and \u00b5 \u22650, it follows that \u03b2\u00b5 \u22650, \u03b2 \u22121 > 0, and [(1 + \u00b5)k+1 \u22121] \u22650,\nleading to f(\u03b2, \u00b5) \u22650.\nCase 2. When \u03b2 > 1 and \u00b5 < 0, we have \u03b2\u00b5 < 0, \u03b2 \u22121 > 0, and [(1 + \u00b5)k+1 \u22121] < 0, resulting\nin f(\u03b2, \u00b5) > 0.\nCase 3. When \u03b2 \u2264\u22121 and \u00b5 \u22650, it follows that \u03b2\u00b5 \u22640, \u03b2 \u22121 < 0, and [(1 + \u00b5)k+1 \u22121] \u22650,\nwhich implies f(\u03b2, \u00b5) \u22650.\nCase 4. When \u03b2 \u2264\u22121 and \u00b5 < 0, we obtain \u03b2\u00b5 > 0, \u03b2 \u22121 < 0, and [(1 + \u00b5)k+1 \u22121] < 0,\nleading to f(\u03b2, \u00b5) \u22650 again.\nCombined with the four cases above, we conclude that f(\u03b2, \u00b5) \u22650 for all cases. Thus, the given\nstatement holds for any n \u2208N and \u03b2 \u2208Z \\ {0, 1}, which proves that the expected compounding\n30\n\n\n--- Page 31 ---\ne\ufb00ect is always positive.\nA.2\nCompounding E\ufb00ect for Serially Correlated AR(1) Returns\nBelow, we shall work with the AR(1) model for {Xt : t \u22651}. Let \u03b2 \u2208Z \\ {0, 1}. For the two-period\ncase, we have that: (i) If \u03c6 > 0, then E[CE2] > 0 and (ii) If \u03c6 < 0, then E[CE2] < 0. To see this, we\nbegin by noting that\nCE2 = [(1 + \u03b2X1)(1 + \u03b2X2) \u22121] \u2212\u03b2[(1 + X1)(1 + X2) \u22121]\n= \u03b2(\u03b2 \u22121)X1X2.\nTaking the expectation on both sides yields: E[CE2] = \u03b2(\u03b2 \u22121)E[X1X2] = \u03b2(\u03b2 \u22121)\u03c6\n\u03c3\n1\u2212\u03c62 . Note that\n\u03c3 > 0, and for \u03b2 \u2208Z \\ {0, 1}, the product \u03b2(\u03b2 \u22121) > 0. Therefore, to determine the sign of the\nexpected di\ufb00erence, it su\ufb03ces to check the sign of the \u03c6. Speci\ufb01cally, for \u03c6 > 0, then E[CE2] > 0.\nOn the other hand, for \u03c6 < 0, then E[CE2] < 0. The following proof extends this result to the\nmulti-period case.\nProof of Lemma 3.2. Given that \u03b2 \u2208Z \\ {0, 1}, we observe that\nE[CEn] = E\n\" n\nY\ni=1\n(1 + \u03b2Xi) \u22121\n!\n\u2212\u03b2\n n\nY\ni=1\n(1 + Xi) \u22121\n!#\n= E\n\uf8ee\n\uf8f0(\u03b22 \u2212\u03b2)\nX\ni\u0338=j\nXiXj + (\u03b23 \u2212\u03b2)\nX\ni\u0338=j\u0338=k\nXiXjXk + \u00b7 \u00b7 \u00b7 + (\u03b2n \u2212\u03b2)\nX\ni\u0338=j\u0338=\u00b7\u00b7\u00b7\u0338=n\nXiXjXk \u00b7 \u00b7 \u00b7 Xn\n\uf8f9\n\uf8fb\n= E\n\uf8ee\n\uf8f0(\u03b22 \u2212\u03b2)\nX\ni\u0338=j\nXiXj + higher-order terms in Xi\n\uf8f9\n\uf8fb.\nSince |Xi| < m for some m \u2208(0, 1), the \ufb01rst quantity dominates, and the higher-order product\nterm is of smaller order compared to the second-order term for su\ufb03ciently small returns. Therefore,\nwe obtain\nE[CEn] \u2248E\n\uf8ee\n\uf8f0(\u03b22 \u2212\u03b2)\nX\ni\u0338=j\nXiXj\n\uf8f9\n\uf8fb\n= (\u03b22 \u2212\u03b2)\n n\u22121\nX\ni=1\nE[XiXi+1] +\nn\u22122\nX\ni=1\nE[XiXi+2] + \u00b7 \u00b7 \u00b7 +\n1\nX\ni=1\nE[XiXi+(n\u22121)]\n!\n,\n= (\u03b22 \u2212\u03b2)\n\u03c32\n1 \u2212\u03c62\n n\u22121\nX\ni=1\n\u03c6 +\nn\u22122\nX\ni=1\n\u03c62 + \u00b7 \u00b7 \u00b7 +\n1\nX\ni=1\n\u03c6n\u22121\n!\n,\n= (\u03b22 \u2212\u03b2)\n\u03c32\n1 \u2212\u03c62 \u03c6\n\u0002\n(n \u22121) + (n \u22122)\u03c6 + \u00b7 \u00b7 \u00b7 + \u03c6n\u22122\u0003\n\u22770 if \u03c6 \u22770\n(10)\n31\n\n\n--- Page 32 ---\nwhere |\u03c6| < 1.\nNote that since \u03b2 \u2208Z \\ {0, 1}, it follows that (\u03b22 \u2212\u03b2) = \u03b2(\u03b2 \u22121) > 0.\nTo\nsee inequality (10), let Q := (n \u22121) + (n \u22122)\u03c6 + \u00b7 \u00b7 \u00b7 + \u03c6n\u22122.\nThen, for su\ufb03ciently large n,\nE[CEn] = (\u03b22 \u2212\u03b2)\n\u03c32\n1\u2212\u03c62 \u03c6Q, it is readily veri\ufb01ed that \u03c6Q \u2212Q = \u03c6 + \u03c62 + \u00b7 \u00b7 \u00b7 + \u03c6n\u22122 + \u03c6n\u22121 \u2212(n \u22121).\nTherefore, for all |\u03c6| < 1,\nQ = n \u22121 + \u03c6 + \u03c62 + \u00b7 \u00b7 \u00b7 + \u03c6n\u22122 + \u03c6n\u22121\n1 \u2212\u03c6\n= (1 \u22121) + (1 \u2212\u03c6) + (1 \u2212\u03c62) + \u00b7 \u00b7 \u00b7 + (1 \u2212\u03c6n\u22122) + (1 \u2212\u03c6n\u22121)\n1 \u2212\u03c6\n> 0.\nTherefore, E[CEn] \u22770 if \u03c6 \u22770.\nA.3\nCompounding E\ufb00ect for Continuous-Time Model\nProof of Equation (6). Begin by observing that\ndLt\nLt\n= \u03b2 dSt\nSt\n\u2212fdt = (\u03b2\u00b5Zt \u2212f)dt + \u03b2\u03c3ZtdWt.\nGiven that f(Lt) := log Lt, It\u02c6o\u2019s Lemma states:\nd log(Lt) =\n\u0012 1\nLt\nLt(\u03b2\u00b5Zt \u2212f) \u22121\n2\n1\nL2\nt\n(L2\nt\u03b22\u03c32\nZt)\n\u0013\ndt + 1\nLt\nLt\u03b2\u03c3ZtdWt\n=\n\u0012\n(\u03b2\u00b5Zt \u2212f) \u22121\n2\u03b22\u03c32\nZt\n\u0013\ndt + \u03b2\u03c3ZtdWt.\nHence, it has a solution\nLt = L0 exp\n\u0012Z t\n0\n\u0012\n\u03b2\u00b5Zs \u22121\n2\u03b22\u03c32\nZs\n\u0013\nds \u2212ft + \u03b2\nZ t\n0\n\u03c3ZsdWs\n\u0013\nand the proof is complete.\nProof of Theorem 3.1. Note that\nE[CEt] = E[exp(Yt)] \u2212\u03b2E[exp(Xt)] + (\u03b2 \u22121),\nwhere Yt =\nR t\n0\n\u0000\u03b2\u00b5Zs \u22121\n2\u03b22\u03c32\nZs\n\u0001\nds \u2212ft + \u03b2\nR t\n0 \u03c3ZsdWs and Xt =\nR t\n0\n\u0000\u00b5Zs \u22121\n2\u03c32\nZs\n\u0001\nds +\nR t\n0 \u03c3ZsdWs.\nWe approximate the expectation using a regime-mixture approximation based on the occupation\nmeasure: Let \u03c0j(t) := E\nh\n1\nt\nR t\n0 1{Zs=j}ds\ni\nfor each j = 1, . . . , M. This is the expected proportion of\ntime the process spends in regime j over [0, t]. If Zt = j, then\nX(j)\nt\n=\nZ t\n0\n\u0012\n\u00b5j \u22121\n2\u03c32\nj\n\u0013\nds +\nZ t\n0\n\u03c3jdWs\n=\n\u0012\n\u00b5j \u22121\n2\u03c32\nj\n\u0013\nt + \u03c3jWt \u223cN\n\u0012\u0012\n\u00b5j \u22121\n2\u03c32\nj\n\u0013\nt, \u03c32\nj t\n\u0013\n32\n\n\n--- Page 33 ---\nand\nY (j)\nt\n=\nZ t\n0\n\u0012\n\u03b2\u00b5j \u22121\n2\u03b22\u03c32\nj\n\u0013\nds \u2212ft + \u03b2\nZ t\n0\n\u03c3jdWs\n= (\u03b2\u00b5j \u22121\n2\u03b22\u03c32\nj \u2212f)t + \u03b2\u03c3jWt \u223cN\n\u0012\n(\u03b2\u00b5j \u22121\n2\u03b22\u03c32\nj \u2212f)t, \u03b22\u03c32\njt\n\u0013\n.\nHence, using the moment generating function technique, it follows that\nE[exp(Xt(j))] = exp (\u00b5jt) and E[exp(Yt(j))] = exp ((\u03b2\u00b5j \u2212f)t)\nBecause Zt switches between regimes over time, the full Yt is a weighted combination of regime-\nspeci\ufb01c dynamics.\nThe exact expectation E[exp(Xt)] and E[exp(Yt)] can be approximated4 as\nfollows:\nE[exp(Xt)] \u2248\nM\nX\nj=1\n\u03c0j(t) \u00b7 E[exp(X(j)\nt\n)] =\nM\nX\nj=1\n\u03c0j(t) \u00b7 exp (\u00b5jt)\nE[exp(Yt)] \u2248\nM\nX\nj=1\n\u03c0j(t) \u00b7 E[exp(Y (j)\nt\n)] =\nM\nX\nj=1\n\u03c0j(t) \u00b7 exp ((\u03b2\u00b5j \u2212f)t) .\nTherefore, the expected compounding e\ufb00ect is obtained:\nE[CEt] \u2248\nM\nX\nj=1\n\u03c0j(t) [exp ((\u03b2\u00b5j \u2212f)t) \u2212\u03b2 exp (\u00b5jt)] + (\u03b2 \u22121).\nLemma A.1 (Expected Occupation Measure). The expected occupation measure PM\ni=1 \u03c0j(t) = 1\nfor all t \u22650.\nProof of Lemma A.1. For every \ufb01xed sample point \u03c9 \u2208\u2126and any time s, the Markov chain is\nexactly one of the M states. Hence, for all s \u22650 and \u03c9 \u2208\u2126, PM\nj=1 1{Zs(\u03c9)=j} = 1. Hence, with the\n4In principle, one can compute E[exp(Xt)] and E[exp(Yt)] condition on given Zs = j. Then invoke the Feynman-\nKac theorem to solve the resulting PDEs, e.g., see Cvitanic and Zapatero (2004). However, solving the PDEs can be\ncomplex.\n33\n\n\n--- Page 34 ---\naid of the Fubini-Tonelli theorem, we have\nM\nX\nj=1\n\u03c0j(t) =\nM\nX\nj=1\nE\n\u00141\nt\nZ t\n0\n1{Zs=j}ds\n\u0015\n= E\n\uf8ee\n\uf8f0\nM\nX\nj=1\n1\nt\nZ t\n0\n1{Zs=j}ds\n\uf8f9\n\uf8fb\n= E\n\uf8ee\n\uf8f01\nt\nZ t\n0\nM\nX\nj=1\n1{Zs=j}ds\n\uf8f9\n\uf8fb\n= E\n\u00141\nt\nZ t\n0\n1ds\n\u0015\n= E [1] = 1.\nProof of Theorem 3.2. This is an immediate consequence of the approximation:\nE[CEt] \u2248\nX\nj\n\u03c0j(t)\u03a6j(t; \u03b2, f) + (\u03b2 \u22121)\nwhere \u03a6j(t; \u03b2, f) depends on both \u03b2 and \u00b5j. The sign of the convex combination P\nj \u03c0j(t)\u03a6j(t; \u03b2, f)\ndetermines the sign of CE, relative to the o\ufb00set (1 \u2212\u03b2).\nProof of Corollary 3.1. This follows directly from Theorem 3.2. Fix f = 0. Note that for t = 0,\nE[CEt] = 0. Next, for t > 0, Take x := \u00b5jt \u2208R where x \u22650 if \u00b5j > 0 and x \u22640 if \u00b5j < 0, and\nx = 0 if \u00b5 = 0. Hence, it su\ufb03ces to show that h\u03b2(x) := exp (\u03b2x) \u2212\u03b2 exp(x) + (\u03b2 \u22121) \u22650 for all\nx \u2208R. We now consider two cases:\nCase 1. For \u03b2 > 1, set the weights \u03bb1 := 1\n\u03b2, \u03bb2 := 1 \u22121\n\u03b2 then \u03bb1 + \u03bb2 = 1 and \u03bb1, \u03bb2 > 0. Since\nthe exponential function exp(\u00b7) is convex, applying the Jensen\u2019s inequality yields \u03bb1e\u03b2x + \u03bb2e0 \u2265\ne\u03bb1\u03b2x+\u03bb2\u00b70 = ex. Multiplying \u03b2 we obtain e\u03b2x \u2212\u03b2ex + (\u03b2 \u22121) \u22650 for all x \u2208R, which shows that\nE[CEt] \u22650 for all t \u22650 when \u03b2 > 1.\nCase 2. For \u03b2 < 0, Take \u03b2 := \u2212k with k \u2208N. Then, we obtain hk(x) := e\u2212kx + kex \u2212(k + 1).\nNote that the \ufb01rst derivative: h\u2032\nk(x) = \u2212ke\u2212kx + kex, which vanishes at x = 0. Moreover, its\nsecond-order derivative: h\u2032\u2032\nk(x) = k2e\u2212kx + kex > 0. Hence, hk is strictly convex and x = 0 is the\nunique global minimizer of hk. Value at the minimum is hk(0) = 1 + k \u2212(k + 1) = 0. Since the\nminimum value is 0, we have hk(x) \u22650 for every x \u2208R. Hence, it also implies that E[CEt] \u22650 for\nall t.\nLastly, we consider the degenerate case \u00b5 = 0. It implies that E[CEt] = e0 \u2212\u03b2e0 + (\u03b2 \u22121) = 0,\nwhich is desired. Therefore, in all cases, E[CEt] \u22650 for all t \u22650 and equality holds when t = 0, or\nwhen \u00b5 = 0.\n34\n",
    "pages": 34
  }
]